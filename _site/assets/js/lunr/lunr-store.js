var store = [{
        "title": "Introduction - End-to-End Machine Learning for Real Estate Price Prediction",
        "excerpt":"Machine learning is an extremely powerful tool, applicable to an astounding breadth of use cases. Today, almost any question imaginable can be the starting point for a machine learning project.      “What products should we recommend to customers to complete their order?”   “What will stock prices be tomorrow?”   “How much should this house cost? How can we better estimate prices?”   This last question was especially interesting to us in the context of Spanish real estate. Spain is a diverse country, with tourist-haven islands and coastal towns, aging mountain villages, big cities, and everything in between. This makes it a very interesting case study and challenging prediction problem.   Starting with this question, “What is that house worth? How can we better estimate its price?”, the first step is to gather the data needed to answer it and develop a modeling plan. These are the first steps in the well known CRISP-DM process for data science — business understanding, and data understanding.      Once we have the data, we can assess which data preparation and machine learning methods will help us answer this question. The articles in this series dive deep into each step of this process, including data preparation, modeling, and iteration on these steps based on evaluations of the models in order to find the best possible model for predicting Spanish real estate prices.   Getting the data   Real estate is a topic with a wealth of information online — but often not in an easily-accessible public format. It would be very difficult if not impossible to find up-to-date information about real estate listings freely available on the web in a format that’s easy to work with from a data science perspective, like a CSV. Instead, data like this is often stored in a format that’s more human-readable and less machine-readable, like flashy advertisements on a website.      How can the data hidden in these advertisements be used to help us estimate real estate prices? What other data influences prices? Is all of this data included in the advertisements, or do we need to find additional data on the web?   We start by showing how we acquired the initial real estate data using web scraping in article 2, Webscraping Real Estate Market Data.   Gleaning information from the data Once we have scraped this information from the web, the next step is to transform this human-readable data into a machine-readable format. We start this process by removing data that we don’t want to use in our prediction model — this helps streamline the data processing steps which come later.   Removing unwanted data or data that is not applicable to modeling is shown in article 3, Outlier Detection. After these datapoints are removed, features are extracted from the data. Features are individual categories of information that each datapoint contains. These features together tell a story about the datapoint and are used for prediction the target variable. For example, features for predicting real estate prices include categories like number of bedrooms, number of bathrooms, location of the property, and indicators like whether the property has a garage. Article 4 in the series, Feature Creation, shows how these features were extracted from the raw data scraped from the real estate website.   As part of the initial analysis, unsupervised clustering methods were used to further understand the data. Clustering methods work by trying to identify previously unseen patterns in the data. Article 5, Clustering, shows the outcome of this unsupervised modeling and how the clusters identified can be used as a further feature for supervised modeling.   Initial modeling   Once the initial features were extracted and clustering was completed, we moved on to supervised modeling. All supervised methods shown are regression methods, as the target variable Price is a continuous variable. We began by training a simple multiple linear regression model as a baseline, then moved on to tree-based algorithms including Random Forest and XG-Boost. Article 6, Predicting Real Estate Prices, shows the results from this initial modeling.  Model improvement   Our initial models included only features from the advertisements that were already in numeric format , like square meters or number of bathrooms, or those that could be easily calculated with additional data found on the web, like distance away from a major metropolitan city. This meant that these models did not see any information nested in the advertisement text. The advertisement text often includes more detailed information about the location of the property, its condition, and additional amenities.   The next article in our series, NLP Part 1: Text Exploration, shows initial analysis of the text feature of the data, and how the text data was prepared for modeling. Article 8, NLP Part 2: Modeling with Text Features shows how these text features were vectorized using a TF-IDF vectorizer and presents the results from including this text feature vector in the model.   Conclusion   The final article in our series shows how the models which incorporate NLP features compare to the original models. We also present further next steps to take, such as deploying the model to make predictions on unseen data.  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/Introduction-End-to-End-Machine-Learning-for-Real-Estate-Price-Prediction/",
        "teaser": null
      },{
        "title": "Webscraping Real Estate Market Data",
        "excerpt":"Using BeautifulSoup and Python to scrape real estate data from the web   At the beginning of every data science project the central question is: where do we get the data from? As is the case for many projects, the web provides us with tons of data which can be used- if one knows how to obtain it. This first post of our Real Estate Data Science Project series shows how to web scrape using the example of the Spanish real estate market.   This article proceeds as follows:     Finding the right scraping strategy for the project   Inspecting the HTML code and initial steps   Scraping properties from each province   Problems caused by website irregularities   Conclusion   01 Finding the right scraping strategy for the project   Web scraping can be tiresome. That is because web scraping truly depends on how clean the html code underlying the website is. Every time there is an irregularity, the scraping will run into problems, resulting in the need to run the entire process again. Later in this blogpost several of these irregularities will be shown with their respective workarounds. Pragmatic solutions are critical. Web scrapping is truly the “dirty work” of data science. On the plus side, having this ability to create data out of nothing is essential and the basis for many data science projects. It also opens up a wealth of project options that otherwise would not be available if one had to rely solely on published data.   Given the oftentimes frustrating nature of web scraping, it is essential to automate as many steps in the scraping process as possible. As an example: the Real Estate observations which are scraped for this project are divided into different areas in Spain. That means in order to find and group every property for each area it is first necessary to find all the URLs associated with that individual area. This strategy of finding the hierarchical nature of the data on the website is visualized below. Here, two explanatory provinces from the website are shown, namely Alicante and Murcia. Clicking on these pictures on the website then leads to the available properties in each of those regions. The graphic below shows two properties for each.      The strategy of the scraping then starts with first finding all the links to each province and second to scrape all the relevant property information for each region. With this plan and understanding of the website structure in mind, we can proceed to looking at the website’s underlying HTML code.   02 Inspecting the HTML code and initial steps  Opening the underlying HTML code of a website (Crtl+Shift+I) for the first time might be a scary experience. It is truly overwhelming how much information is stored behind the façade of a pretty looking website. The good news is that most of this information is not relevant- it’s all about finding the right point where to start. Our strategy outlined above tells us that the first thing we should find are the links to all provinces. Starting on the home page of the website, the first thing we do is to right click on (for example) the picture of “Alicante Province”.      What is seen on the right side is the HTML code of the site. When we right clicked on the picture of Alicante, the HTML code of this picture gets highlighted in blue. We suspect that the link of the estate of Alicante are not too far from where the picture is embedded. Let us take a closer look at the HTML code:      As in the picture before that, we can see the highlighted (in grey this time) HTML code of where the picture is embedded. Also visible right above we find the URL of the properties for Alicante province. This information now has to be extracted. We want to repeat this process for all provinces on this website. To do so, it is important to see how the link is stored within the HTML code. Namely, it says:   &lt;a class=”location-tile__link” href=”https://www.XXXX.com/en/alicante-province-property-for-sale-0l3”&gt;      # Packages needed for scraping and extracting import requests from bs4 import BeautifulSoup import re # The website link url = \"https://www.XXXX.com/\" # Making the request look like a normal browser headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'}  # Requesting the sites content page = requests.get(url,headers=headers) soup = BeautifulSoup(page.content, \"html.parser\") # Getting the relevant links links_html = soup.find_all('a', class_='locations-list__link') # Turning the soup into a list of strings links_v1 = [str(x) for x in links_html] # Extracting the relevant links for all regions pattern = 'href=\"(.*)\"&gt;'     links_v2 = [re.search(pattern, x).group(1) for x in links_v1]   What is happening here is that HTML of the URL specified is requested and saved. The header command disguises our request to make it look like a normal browser request of the website. Then the entirety of the HTML code is saved in the soup variable. Now the relevant part of how the links are stored within the HTML code kicks in. The first argument of the find_all command takes the tag name. The tag name is the first letter in the HTML code before the start of the object class. For this class, it is the letter a in the beginning of the code. The second part specifies what kind of class we are looking for here. This information was also already available to us within the HTML snippet. Afterwards we change the format into a string for easier usage later. Finally, we remove problematic substrings like “href=” in order to obtain the raw links. The output of that code looks like this:      These links now open up the possibility to scrape properties from over 107 different provinces in Spain. One can only imagine how long that would have taken if done manually.   03 Scraping properties from each province  The next step after obtaining all provinces is to scrape the individual estate data from each province individually and loop over all provinces individually. The first step, as always, is to inspect the website.      The picture above shows an example of what kind of information a property is providing. We are interested in extracting almost every detail provided. We want know for example how expensive the property is, how many bedrooms it has, number of bathrooms, square meters of living space has and whether it has a pool. All this information must be available in the HTML code if it is written on the website. After some deeper digging through the website’s code, we find following:      We can see that the information about the property price is nicely saved under the class card-property__price. The information about the bedroom and bathrooms is a bit trickier to find. Both of them are saved under the header “card-property__feature-item”. That does not make it very easy to distinguish bed and bathrooms. What is helping us here is the little icon the website is providing next to the number of each room. Namely, the little icon placed next to the number shows up with a distinct name in the HTML code, which reveals whether it is a bed or bathroom.   Bedroom: “kicon el-ml-sm el-mr-sm kicon bed”   Bathroom: “kicon el-ml-sm el-mr-sm kicon-bath”   The next question would then be how to best extract all this information in a neat and concise way. This problem is a bit more difficult than the extraction of links done earlier since the information is hidden further down within the HTML code. Furthermore, the information is not saved as the type “class”. This makes the find_all command a bit more difficult to apply. The workaround is to save the entirety of the HTML code as a string and use the regular expression package to scan the text. In order to see how that would look in code, the following snippet shows how the bathroom information is extracted. The variable “relevant_list” represents the HTML string of an estate.   # Specifying the pattern which incorporates the prices pattern = '\"&gt;(.*?)&lt;/span&gt;&lt;span&gt;&lt;i class=\"kicon el-ml-sm el-mr-sm kicon-bath\"&gt;'     # Extracting the price information bathroom_v1 = [re.search(pattern, x).group(1) for x in relevant_list] # Taking the last part of each string since that contains the number bathroom_v2 = [int(x.split(\"&gt;\")[-1]) for x in bathroom_v1]   To further explain the code shown in this snippet: the idea is to extract the number of bathrooms by finding the number in between a certain string combination. As we can see in the HTML, the integer of how many bathrooms an estate has is specified after an “&gt;” sign and directly in front of the   &lt;/span&gt;&lt;span&gt;&lt;i class=”kicon el-ml-sm el-mr-sm kicon-bath”&gt;   part. Once the bedroom information is located and extracted from the regular expressions search operator, it is saved in a list. This very same procedure is also applied to the number of bedrooms, the square meters of the property and whether it has access to a pool. Additionally, the small excerpt of the description text which every ad provides is extracted in order to apply Natural Language Processing algorithms to extract further features.   04 Problem caused by website irregularities   As with every web scraping project, the biggest cause of code adjustments and the need to re-run the code is the occurrence of irregularities. Glancing at a page of listings, an example can be quickly found      This property provides information about square meters, where it is located, and even has some nice advertisement text, but does not provide any information about bedrooms or bathrooms. This property is not useful as a datapoint since it lacks the information we would like to feed into our model. The extraction code provided earlier would run into a problem and break when it comes across this property. It would try to look for the occurrence of the bed and bathroom strings, which do not appear. This will result in an error and a necessary adjustment of the code. This problem is solved in the following way: Every property ad is first checked for the existence of all required strings. If all required strings do not exist, that property is eliminated and no further scraping actions are taken on the property. To do this, after each property’s full HTML code is separately stored, and advertisement text is separated from the other information. This text is then run through a test function which checks for the existence of substrings like “bed”, “bath” and “m²” within the HTML code. This works since these keywords only show up if the little icons denoting “bed”, “bath” or “m²” are also available. In the property shown above, these keywords would not exist since the little icons are not present. This property would then be eliminated during this test step.   05 Conclusion  Web scraping is a powerful tool and does not require very complicated coding. It does however require patience and creativity to work around irregularities. In the end our code was able to scrape 42,367 estates from the entire website. This should be enough not only for interesting analysis of the market, but also to train machine learning algorithm  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/Webscraping-Real-Estate-Market-Data/",
        "teaser": null
      },{
        "title": "Outlier detection in Real Estate Data",
        "excerpt":"Preparing the data for feature extraction and modeling In the prior post, we outlined how we scraped real estate data for 42,367 properties in Spain. Before jumping directly to any modeling exercises, it is important to look at the variables at hand. Data inspection is essential in order to see whether the set goal is actually achievable with the given data, and how we can improve the data quality. This section is structured as follows:     Assessing the data   Why are outliers problematic?   Identifying outliers   Defining the cutoff level   Conclusion   01 Assessing the data  In order to get a first feeling for the data, it is always wise to plot the variables. Below we see boxplots of the four main variables. The variables are the number of bedrooms, number of bathrooms, number of square meters and the price of the property.      It is clearly visible that all variables are heavily skewed to the right. That means that most of the data is centered at the left side of the distribution, with fewer observations on the right side. This occurs because of two reasons: the first is that all four variables are capped at zero on the lower side. This leaves less room for outliers on that end of the distribution. Secondly, even though the website is supposed to show only valid houses and estates, there are still some properties which are not valid and skew the distribution drastically to the right, like large commercial properties such as hotels and restaurant spaces.   We can see from the graph above that there are apparently several observations with more than 50,000 square meters or over 300 bathrooms. These are definitely not features one would expect to see in an ordinary house - the validity of these observations should be questioned.   It would be easy to simply drop any observation with a insensibly high or low amount of a certain variable, but there are major problems with doing so. The first problem is how to define what is a too high or low amount. Choosing an arbitrary amount is not scientific and hard to defend in front of the project owner. The following section outlines other more scientific methods for defining what makes an outlier.   02 Why are outliers problematic  In regression estimation problems, outliers can influence the algorithm massively and distort the results and predictive power of the model. Consider a multiple linear regression: each beta coefficient tries to quantify a linear relationship between the respective feature and the dependent variable. Introducing an extremely high or low observation strongly influences this linear relationship and distorts it. That is because of the loss function the algorithm tries to minimize, which in the case of multiple linear regression it is the mean squared error (MSE). In order to minimize the loss, the algorithm tries to fit better to the outlier given the enormously large error the outlier produces.   This problem also holds for other models such as Boosting and Bagging models. A common misconception is that bagging models, such as Random Forest, are insensitive to outliers because of their non-parametric nature. This is not true, especially not when you have more than one outlier. The reason for that is, again, the loss function. Like in multiple linear regression, the bagging model overfits to the outliers and finds an overall worse performing result compared to a model which does not contain outliers at all.   The problem of outlier(s) with boosting models is even greater. Boosting model such as AdaBoost and Gradient Boosting are iterative models, meaning that in contrast to bagging models, the algorithm fits a first model, sees where it performs relatively badly, and will puts more emphasis on those observations in the later iterations. Outlier(s) make this task much harder - the model then builds itself primarily on a few non-sensical outlier observations.   02.2 Why dropping outliers is problematic  Generally it should be said the simply dropping outliers to improve the model at hand should not be taken lightly. Simply dropping observations to improve the model at hand is a form of data manipulation and is not justifiable scientifically or statistically.   Removing outliers could potentially lead to a biased sample. Furthermore, it could also make the results of the predicting algorithm completely meaningless as the results would not apply to any prospectively definable population. Looking in the advertisement texts of properties with more than 300 bathrooms or 50,000 square meters reveals that these observations are often commercial land properties or even multiple houses bundled together. Given the nature of this project, namely building a prediction model for house prices in Spain, it is valid to drop observations which do not fall in the category of a residential property.   03 Identifying outliers  When it comes to identifying outliers, there are many methods which can be useful. This post uses three of the most common ones, namely the percentile method, the interquartile range method and the empirical rule. It is important to stress that there is no holy grail in outlier detection and different models will result in different conclusions. In order to make a well-rounded decision and not to rely too much on any method, we made the decision to classify an observation as an outlier only if all three methods classify it as an outlier. In the following sections all methods are explained and the implementation of the method in code is shown.   03.1 Percentile based Method  The percentile method cuts off a predefined percentage amount from the top and the bottom of a distribution (if both sides are desired). In order not to cut off too much of the observation, a cutoff value of 1% is commonly used. That means that the top and bottom 1% of the a variable is labelled as an outlier.   def pct_method(data, level):     # Upper and lower limits by percentiles     upper = np.percentile(data, 100 - level)     lower = np.percentile(data, level)     # Returning the upper and lower limits     return [lower, upper]   03.2 Interquartile Range Method  The interquartile range approach first calculates the interquartile range (IQR) of the data. The IQR is defined as the difference between the 75 and 25 percentile of a variable. This IQR is then multiplied with 1.5. Any data that is then further away than the 75 percentile plus 1.5IQR or 25 percentile minus 1.5IQR is classified as an outlier.   def iqr_method(data):     # Calculating the IQR     perc_75 = np.percentile(data, 75)     perc_25 = np.percentile(data, 25)     iqr_range = perc_75 - perc_25     # Obtaining the lower and upper bound     iqr_upper = perc_75 + 1.5 * iqr_range     iqr_lower = perc_25 - 1.5 * iqr_range     # Returning the upper and lower limits     return [iqr_lower, iqr_upper]   03.3 Standard Deviation Method (Empirical Rule)  The standard deviation approach arises from the so called empirical rule which states that given a normally distributed variable, approximately 99.7% of the data is within three standard deviations. This approach then classifies any observation which is more than three standard deviation below or above the mean as an outlier. It is essential to note though that this approach only works if the data is approximately Gaussian.   def std_method(data):     # Creating three standard deviations away boundaries     std = np.std(data)     upper_3std = np.mean(data) + 3 * std     lower_3std = np.mean(data) - 3 * std     # Returning the upper and lower limits     return [lower_3std, upper_3std]   03.4 Combination of all three methods  As mentioned earlier, dropping outliers should only be done in extreme cases and when the observation are clearly misrepresenting the project task. For that reason we only classify an observation as an outlier if it satisfies all three conditions introduced above. Combining all three approaches is done in the following way:   def outlier_bool(data, level=1, continuous=False, log=False):     # Taking logs is specified     if log is True:         data = np.log(data + 1)     # Obtaining the ranges     pct_range = pct_method(data, level)     iqr_range = iqr_method(data)     std_range = std_method(data) if continuous is False:         # Setting the lower limit fixed for discrete variables         low_limit = np.min(data)         high_limit = np.max([pct_range[1],                              iqr_range[1],                              std_range[1]]) elif continuous is True:         low_limit = np.min([pct_range[0],                             iqr_range[0],                             std_range[0]])         high_limit = np.max([pct_range[1],                              iqr_range[1],                              std_range[1]]) # Restrict the data with the minimum and maximum     outlier = data.between(low_limit, high_limit) # Return boolean     return outlier   Two options are important to note within this function. The first is that for variables which are continuous, we also take a look at observations with obscurely low levels. This is done since several properties report a number of square meters of 1. These observations are spam ads, which clearly do not belong in the data. For discrete data (number of bedrooms or bathrooms) we only use the minimum value of the data series.   The second important argument is the use of logs. Given the high skewness of our variables, using the outlier detection methods has several issues. Firstly, the empirical rule does not work since it requires an approximate Gaussian distribution. Secondly, given that the goal is to remove as few outliers as possible, squishing extreme values closer to the rest of the data gives them a better chance not to be identified as an outlier in the first place.   Given the reasons outlined in the paragraph above, all variables are log-transformed before the outlier detection methods are applied.   04 The cutoff levels  This section gives an indication as to how many observations were thrown out and where the cutoff variable was set for all four variables. The table below shows the cutoffs for each variable as well as there respective minimum and maximum.      All cutoff level seem sensible. The cutoffs allow easily for all kinds of properties, but also allow for multi-million mansions priced up to €6,300,000. In order to have a better visual understanding of how exactly the outliers have been cut, the graph below should shed some light. The first row of the graph below shows the log values all four variables, before removing any observations (unrestricted version). The red area then shows what our above defined, three methods using, outlier detection model regarded as an outlier. The second row then shows the distribution of the log values after the outliers have been removed (restricted version). The third row then shows the non-transformed data unrestricted and with a red indication shown where the cutoff happened. The last row then shows the restricted version of the raw values.      For all variables we can see a clear reduction in skewness and something which come closer to a Gaussian compared to the unrestricted model. Looking at the summary statistics below, we can also see that the skewness is still there but much less rightly skewed than before. Going forward, there might still be a case of applying logs on the variables in order to dampen the higher values within the price and square meter variable.      05 Conclusion  At the end of this post, it is important to remind ourselves where we came from and why removing outliers is an important step before moving on to modeling and prediction. After scrapping around 40k properties, we took a first look into the data. We found that the data is heavily affected by some extreme observations that do not represent residential properties. We then outlined potential problems of keeping and dropping outliers and decided to only remove observations that represent truly obscure cases. The next step is to try to make sense of the dataset we have now that outliers have been removed. More specifically, the next post will apply some clustering in order to separate estates by their variable characteristics.  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/Outlier-detection-in-Real-Estate-Data/",
        "teaser": null
      },{
        "title": "Clustering Real Estate Data",
        "excerpt":"Using unsupervised learning techniques to create features for supervised price prediction.   01 What is clustering and what can it be useful for  Clustering has many applications. Most people know it as an unsupervised learning technique. Here, we use clustering to find similarities in observations of real estate listings and allocate similar observations into buckets, but clustering can also be used for other use cases like customer segmentation.   Whenever a new property comes on the market, the question of how it should be priced naturally arises. One good approximation for that is to see how similar properties are priced. The goal here is to define what exactly makes one property similar to another. Clustering can be helpful here to identify which larger category of properties a given property belongs, and which features influence belonging in this category. One can then use the average from this category to get an indication of the price.   Another use case for clustering is to use the cluster information as an additional feature within a supervised prediction model. The reason why that could be a helpful is that a cluster variable provides condensed information to the model. A tree model, for example, has an easier time splitting observations based on one variable instead of splitting based on many variables individually.   To explain that in more depth, let us assume a Random Forest model, which is a special form of bagging. It is special in the sense that it does not consider all features for every tree, but chooses random features for every tree. If a tree chooses the features number of bathrooms and bedrooms but not the number of square meters, it has an incomplete picture. Even though the Random Forest model tries to average out this incompleteness through a majority vote of trees, the tree would have an easier time to have variable which condenses much of the basic information of the property already. That would lead to fewer misclassifications of properties which show an unexpected value for one of their features.   A more elaborate post on the usefulness of using a cluster variable within a supervised predction model is shown here.   02 Clustering Method - K Means  The clustering method used in this example is K Means. The reasons for choosing that clustering method compared to more complicated methods are:      Simplicity - K Means, compared to many other clustering methods, is relatively easy to understand and to implement. Given that clustering is used as a simple additional feature, the use case does not require an overly complex model.   Adaptive approach - The algorithm is known to easily adapt to new observations. This means that newly added data can easily classified with this algorithm   Guaranteed convergence - by trying to minimize the total SSE as an objective function over a number of iterations.   Before jumping into any results it is important to elaborate a bit more on how the K Means algorithm works. The first step, and at the same time one of the most difficult steps within this approach, is to set how many clusters we want our data to be split into. This is challenging - how are we supposed to know how many clusters the data needs? We will elaborate a bit later on how to make an educated guess on that number. For now, we will simply call that mysterious number of clusters k.   We start by taking k randomly selected observations from our dataset. In our case, we choose k randomly chosen properties. These are our initial centroids. A centroid a fancy name for the mean of a cluster. Given that this is our first observation within each cluster, it therefore also represents the mean. Afterwards, we assign all other properties left (N-k) to exactly one of the k groups. This is done by calculating the difference between the features of a property to all possible centroids. Having more than one feature requires us to use the euclidean distance for that. The euclidean distance is the square root of the sum of all squared features of a property.   After we have assigned all properties to one cluster, we re-calculate the means (centroids) of each of the k groups. This is done by calculating the simple average of all observations for each cluster. This calculation marks the end of the first iteration.   The next iteration starts again by allocating all properties to one of the k centroids again, using the euclidean distance of each property. It could now be that some observations which were assigned to a certain group change over to a different of the k groups. If that happens, that means that the model has not yet converged and needs more iterations.   After many iterations, no observation will change its group assignment anymore. When that happens, the algorithm is done and said to have converged. This also means that the summed variation of all k groups has reached a minimum. K Means is heavily dependent on which initial values were randomly chosen at the start. In order to test the robustness of the result, it is advised to apply algorithms such as k-means seeding. These methods test different initial values and assess the change in results.   The entirety of the explanation above can also be summarized in the pseudo code below:      02.1 Choosing K  As stressed before, one disadvantage of clustering is the necessity of choosing the number of clusters at the start. This task seems truly difficult since it somewhat requires us to have an initial idea about the data. For cases where we have up to three variables, this problem can be solved by simply plotting the observations and see visually which number of clusters could be sensible.   There are multiple problems with that approach, however. The main problem is probably that “eyeballing” the number of clusters to use based on a plot is not very scientific and very subjective. A more sophisticated approach is the silhouette score, which is explained in the next section.   On a high level, what the silhouette score does is to assess whether an observation fits nicely to a certain cluster and badly to a neighboring cluster. This is done through the comparison of two distance measurements. The first is the measurement of how far an observation within a certain cluster is away from the other observations within the cluster. This is done by calculating the average euclidean distance of one observation i to all other observations within the same cluster.      The second measurement is to see how well an observation in a certain cluster fits to all other observations within a so called “neighboring cluster”. A neighboring cluster is the cluster which, on average, is closest to a another cluster. The assessment is done by calculating the average distance of a certain observation to all other observations of a neighboring cluster.      The last step is then to compare these measurements. The silhouette score is defined for a certain observation as the difference between the average distance to a neighboring cluster and the average distance within its cluster. This number is standardized by dividing the difference by the maximum of a(i) and b(i).      Through this standardization the silhouette score will be between -1 and 1. In order to assess how well the clustering fits not only one observation, but the entirety, the silhouette score is calculated for all observations and then averaged. In our example the average silhouette score for all observations has the following shape:      From the graph above, we can see that the best silhouette score is achieved by having three clusters. This result is interpreted as follows: Having three clusters, on average, the mean euclidean distance between an observation and a neighboring cluster is greater than the mean euclidean distance between an observation and other observations within the same cluster.  02.2 Code Implementation   When implementing K Means, it is important to start by figuring out which value k should take using the silhouette score.   # Number of maximum clusters tried  max_cluster = 5 # Initialise dictionary  sil_graph = {} # Looping starting with 2 clusters and then for cluster in range(2, max_cluster + 1):         # Calculate the kmeans with clusters     kmeans = KMeans(n_clusters=cluster, random_state=0).fit(data)     # Calculate the kmeans labels     sil_graph[cluster] = silhouette_score(data_v3, kmeans.labels_)   As seen above, when implementing the silhouette score, we have to state the number of clusters from the start. For our example we tried all cluster levels up to 5. It is important to allow for a relatively high amount of potential clusters. Otherwise, it could happen that the calculation will be only able to find a local minimum.   The second step is then to implement the K Means algorithm with the optimal value of clusters, namely three. This is relatively easy and shown below.   # Choosing decided cluster level cluster_level = 3 # Initialising cluster level kmeans = KMeans(n_clusters=cluster_level, random_state=0).fit(data) # Assign cluster labels cluster_label = kmeans.labels_   It should be said that before feeding the observations in any of the clustering algorithms, it is advised to first standardize the data. This ensures comparable scaling of the variables.  03 Clustering Results  After deciding for three clusters, it would be interesting to see how the data was split. As discussed before, plotting the variables and highlighting the cluster gets difficult whenever more than three variables are at play.   One workaround is the usage of Principal Component Analysis (PCA). What PCA does is to compress the information of many variables in a selected amount of fewer variables. The graphic below shows exactly that - here, we have projected all the feature variables onto a 2-dimensional plane for plotting.   First, the variables #of bathrooms, # of bedrooms, # of squaremeters, longitude and latitude are used to build exactly two principal components. Afterwards these two newly created variables are plotted. The color of each point shows its assigned cluster group.      At first glance, the clusters don’t look very intuitive. It seems that we have two islands of observations, one larger than the other.   Given that we are in possession of the location data of the properties, it is also interesting to see how the clustering is spread through the country. One very interesting observation here is that the grey-blue dots are mostly solely in Gran Canaria, whereas all other observations are in Peninsular Spain. This finding explains the two “islands” seen in the original PCA graph.      The other mystery that remains now is what causes the two cluster groups within Peninsular Spain. This can be explained with the other variables. The 3-D plot below shows the other 3 features and shows that the cluster group one shows more extreme values for all variables compared to cluster group two.   04 Conclusion  In this post, we explained what clustering can be used for, how the relatively simple method of k-means works, and how to determine k. Furthermore, we showed example code of how to implement the clustering, and the results of clustering our real estate features. We further showed methods of how to visualize the results of clustering.   In the upcoming posts we will use the cluster group as a feature in our prediction model. So stay tuned to read about the performance of this variable!  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/Clustering-Real-Estate-Data/",
        "teaser": null
      },{
        "title": "Feature Creation for Real Estate Prices",
        "excerpt":"Feature Creation for Real Estate Price Prediction This post elaborates on feature engineering for Spanish real estate price prediction.   This post is structured as follows.     Importance of feature engineering   Proximity of larger cities   Hyped cities   Temperature Data   Conclusion   01 Importance of feature engineering  Within Machine Learning, the big hype is normally centered around fancy prediction models and opportunities for incremental improvements through hyperparameter tuning. Equally important, if not even more important, are the features which are fed into the model. Feature creation and selection are the true heroes of Machine Learning- and often times responsible for much higher gains in performance than hyperparameter tuning. Finding relevant and well-defined input variables can boost the accuracy of any model significantly.   Creativity and data availability are the “only “ limits when it comes to feature creation. Deciding which features to implement depends on the particular prediction task. Our task is to build a prediction model for real estate prices. The question is then: “what factors influence are most likely to influence the price of real estate, and how can we help our model to better understand these?” We ended up with three implementable ideas.   The first one is the proximity of a property to a larger city. Living closer to a denser populated city comes with the perks of being able to take advantage of everything a city has to offer and should therefore also impact the price of an property.   The second variable is an indication of whether the property’s location is one of the hotspots within Spain. This feature tries to capture the superior attractiveness and lifestyle of these cities.   The last variable is the climate in the property’s area. Spain is espcially attractive as a retirement destination because of its Mediterranean climate and endless sunshine. The variation of climate within the country is also likely to explain some variation within price.   This post elaborates on all these variables and shows how they were implemented. For each variable, we start by motivating the theoretical significance of the variable. We then show the methodology and implementation of the code.  02 Proximity to larger cities  02.1 Motivation  The idea for this variable came when thinking about small apartments in large cities like London or New York. The price tag of small apartments in New York is mind boggling compared to what you get. You end up paying multiple thousands per month for a small bedroom and an even smaller bathroom. The amount of money paid in rent (or purchase) of such an apartment can buy significantly more property somewhere else.   The reason why these prices are justifiable in New York or London is the location in the midst of a major global city. This characteristic of a property, namely being in a reasonable range of such a city (and how many such cities are close by) is what we try to capture for our Spanish real estate data.  02.2 Methodology  Two different kinds of feature variables are created. The first one is a count of how many cities are in a certain radius of a property. The second one is how close the nearest city with more than 100,000 habitats is. The latter should give an indication how rural the estate is and corrects for a potentially incorrectly chosen radius when constructing the first variable.   Furthermore, we distinguish between the kind of city. Living close to New York may be better (or worse) than living close to a 50,000-inhabitant city. Regardless of the effect on price of these different cities, meaning regardless of whether one has a stronger or weaker effect, we assume that the effect is different in magnitude. In order to allow for a different effect on price, we distinguish between different city sizes. Our definitions are in line with the commonly applied settlement hierarchy. Hence we define:      This variable creation requires knowing the city names, as well as the location of every city with more than 100,000 inhabitants. This information is taken from here. The data looks like this:      According to our source, we find two cities with more than one million inhabitants, ten cities between 300k and 1m habitats and 76 cities with a population between 100k and 300k. In order to keep the type of city groups to a minimum (medium, large and metropolis) we assigned Madrid to the metropolis definition as well (even though it has 200k more habitats than the formal definition of a Metropolis city).   The next step now is to count how many of these cities shown above are within a certain radius of each property. To do that we need two things: a radius definition, and a function to calculate the distance between the two locations.   We use a radius of 30 kilometers euclidean distance (the way the crow flies). This number seems sensible given that distance on the road would then be probably around 50 kilometers, which represents approximately a one-hour drive with a car. As this radius was chosen arbitratily, we tested several different radius lengths in order to conduct sensitivity checks. We found only a negligible variation in the results with these checks.   The code for calculating distances between two locations given their longitude and latitude information is the following:   def dist_bool(list1, list2, radius): # Separation of the longitude and latitude data     lat1, lon1 = list1[0], list1[1]     lat2, lon2 = list2[0], list2[1] # Earth radius in km     R = 6371 #conversion to radians     d_lat = np.radians(float(lat2)-float(lat1))     d_lon = np.radians(float(lon2)-float(lon1))     r_lat1 = np.radians(float(lat1))     r_lat2 = np.radians(float(lat2)) #haversine formula     a = np.sin(d_lat/2.) **2 + \\         np.cos(r_lat1) * np.cos(r_lat2) * np.sin(d_lon/2.)**2 # Calculate distance between two points     haversine = 2 * R * np.arcsin(np.sqrt(a)) # Checking whether it is still within the radius     within = radius &gt;= haversine # Returning the results     return within   02.3 Results   The summary statistics of these two features are visible below. We see that 8% of all properties are within a 30 kilometer range of a metropolis. That is sensible given the high amount of smaller cities that are essentially suburbs of either Madrid or Barcelona. We find furthermore that the average amount of medium cities within a 30 kilometres radius for all properties is around two on average.   One should be cautious with the interpretation of that result, given that one observation had around 26 medium sized cities within its radius. This is likely to be the case of cities which are located within the Madrid Provence, where all smaller suburbs around Madrid are counted as an independent city. This fact can alter the statistic.      The second variable is the measure of how many kilometres away (euclidean distance) from the property is the closest city with more than 100k habitats.   The summary statistics as well as the distribution plot below show us that a considerable amount of estates within our data are well connected, with more than half of our observations being less than 50 kilometers away from a larger city.    03 Hyped cities  03.1 Motivation   Real Estate is an interesting mixture of art and science. Nobody can truly explain why one area of the city is so much more popular than another by just looking at a map. Often times certain districts or even entire cities are hyped. This subjectivity in attractiveness is difficult to encapsulate in a measurement. One approximation would be to see what well known magazines report as hyped cities is districts, as we’ve done here.   03.2 Methodology   In order to understand what cities are currently the best places to live, we use this website which ranks as one of the best expatriate publications online. We again use webscraping to obtain all the locations which, according to the website, are the best places to live. The code below shows how exactly this is done.  # The link where to find all the real estate url = \"https://expatra.com/guides/spain/best-places-to-live-in-spain/\" # Requesting the sites content page = requests.get(url) soup = BeautifulSoup(page.content, \"html.parser\") # Getting the table with all places in spain table_html = soup.find_all('li', class_='ez-toc-page-1') # Getting all the cities cities = [x.text for x in table_html] cities = cities[1:-1] # Changing all city names to lower case cities = [x.lower() for x in cities]   03.3 Results   After scraping we have a list of cities. As the last step we then create a dummy variable which indicates whether a property is located in any of these hyped cities.   04 Temperature data  04.1 Motivation   One of the main reason to obtain a Spanish property for many foreigners is probably the weather. Spain, given its location, is one of the warmest places within Europe. It is therefore likely to be of high interest for a buyer how often it rains, what the average temperature is and potentially how windy it is.   Furthermore, given that the air pressure is higher for areas closer to sea level (explanation below), this variable may kill two birds with one stone, serving also as a proxy for distance to the sea. Including this variable also assumes that a buyer is more likely to pay a premium to live closer to the sea.      04.2 Methodology  Since it was not possible to find good average climate data for each city in Spain, we had to change strategy. Instead, we found climate data for different weather stations in Spain, namely information of 114 weather stations scattered around the country. Using their longitude and latitude information we plotted the different weather stations below in a scatter plot. The graph shows nicely that we have weather stations all around Spain, even with several on in Gran Canaria.      The task is now to calculate the average climate information and then find the weather stations closest to every property. This is done by using the distance calculator shown earlier in this post.   The question arises how many years should be used to average the climate data. The accepted convention to calculate the average climate is 30 years, according to scientific sources. Our data starts as early as 2012. We then use 30 years of observations from 2012 backwards. Below we can see the results of these calculations for some example weather stations.      05 Conclusion  Feature creation is one of the most important steps when building prediction models. Feature engineering also often proves much more important for model performance than hyperparameter tuning.   However, feature engineering also has plenty of drawbacks. From our perspective, there are three main challenges that arise during the process of feature creation.   It begins with having an idea what kind of feature could be potentially useful. Next and even more critical is the availability of data. In our example we could not find climate data for every city in Spain and therefore had to use weather stations instead. This alternative source is nevertheless better than not including any weather data and represents a good compromise.   The last difficulty is data handling. Oftentimes scraped data or csv imports come in inconvenient formats and always have to be cleaned and prepared before being useful in modeling.   The potential unrewarding moment comes when the model shows insignificant importance of the feature you engineered. Unfortunately, this problem is not possible to mitigate beforehand - it’s hard to predict how important a feature will be for a model.   In the next post, we feed the features created here and the original features from the real estate advertisements into several prediction models to assess the importance of all features in predicting price.  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/Feature-Creation-for-Real-Estate-Prices/",
        "teaser": null
      },{
        "title": "Predicting Real Estate Prices",
        "excerpt":"Machine Learning for predicting Spanish real estate prices.   This post is discusses machine learning model selection and performance assessment for prediction Spanish real estate prices. More specifically, in this post we compare different models based on their prediction performance on our real estate data. This post also elaborates on the workings of the Gradient Boosting model.   This post then concludes with an explanation of the best model’s performance, and the hyperparameters that led to the best model.   This post proceeds as follows:     Feature Pre-Processing   Model selection and specification   Model Results   Conclusion   01 Feature Pre-Processing  Before proceeding with any kind of prediction model, it is important to spend some time with pre-processing the features. This helps the model get as much information as possible from the data. In earlier posts we explored and explained all of our basic features and more advanced features.   While extracting these features, we saw that there were quite a few outliers which could be potentially problematic for the model. This is important to note because the decision tree models used here, in contrast to what is commonly believed, are not immune to the effects of outliers. After thorough inspection of the data and exclusion of the observations deemed outliers, the number of observations was reduced from 40,733 to 40,243 - a drop of roughly 500 observations.   The features used for training these prediction models are the following:     No. of Bedrooms   No. of Bathrooms   Dummy variable for whether property has a pool   No. of Square Meters   Longitude of the City   Latitude of the City   Cluster Grouping variable   How many Metropolis cities within 30km radius   How many Large cities within 30km radius   How many Medium cities within 30km radius -Dummy for whether cities is listed as “quality” city from a leading tourist website   How far away a city with more than 100k habitats is (in km)   Average temperature over the last 30 years   Average total precipitation over the last 30 years   Average wind speed over the last 30 years   Average air pressure of the last 30 years In order to get a feeling for these variables, we take a deeper look into the continuous variables of the features listed above.      Looking at the plots, we see that many of them (especially the dependent variable: Price) exhibits a large amount of skewness. In order to work around this unfavorable distribution, we apply logs on all continuous variables. The goal of using logs is to “squish” more extreme values closer to the rest of the data. This step within the pre-processing is favored by nearly all statistical model in case of very high or low skewness.   The figure below shows nicely the effect of the natural logarithm. Especially the Price variable (which exhibited the most extreme skewness before) shows a significantly improved shape. Other variables like the average air pressure may still show a skewed distribution, but the relative difference in the more extreme values is not as daunting as before applying logs.   For the upcoming prediction models, we are proceeding with the log version of the continuous variables.      02 Model selection and specification   For the task of predicting real estate prices, we rely on tree based models. We also implemented a multiple linear regression, but only as a baseline. The main reasons for applying tree-based models for this tasks are the following:     Ability to capture non-linear effects. This is important since it is fair to assume that the marginal effects of another bath-/bedroom within a property are not constant. Consider, for example, the benefit to a family of going from one bedroom to two bedrooms compared with the added benefit of going from nine to ten bedrooms.   Ability to handle sparse data. This benefit is especially noticeable when the natural language processing features are added later on, since word-vectorization results in a sparse feature matrix.   Tree-based models, and especially Gradient Boosting are famous for their superior prediction power. This is nicely described and explained here.   02.1 Inner Workings of Gradient Boosting  Since Gradient Boosting is our main model within this project (its superior performance is shown later), we here briefly walk through how this model works.   We begin with discussing what the Gradient Boosting model takes as an input. As with every boosting model, Gradient Boosting needs (next to data of course), a differentiable loss function. The necessity of this will be outlined later on.   Boosting is an iterative process in contrast to bagging (the category under which the Random Forest falls). That means that we start with an initial “naive” model (also called a weak learner) and then iteratively increase performance by adding further weak learners.   In the beginning, the model makes a prediction on the entire dataset. This is done by minimizing the loss function. In case of a regression problem (in contrast to a clustering problem) the loss function is most commonly the mean squared error (MSE). It is important to stress that we get only one prediction for every single observation. As expected, this first prediction will likely not do a good job. The magic happens in the next steps. In contrast to Adaptive Boosting, which in the following steps focuses on the observations the model predicted relatively badly, Gradient Boosting is rather interested in the residuals of the first step. Namely, it takes the residuals from the first step and fits a tree model on them.   The tree model algorithm then splits the residuals into different leafs. The last leaf of a tree model is called its terminal leaves. The Gradient Boosting algorithm then forms a prediction for all terminal leaves. This is done by, again, minimizing the loss function. This leaves us with a prediction of the residual (also called pseudo residuals in this application) for every terminal leaf.   The last step is to go back to our initial prediction made in the first step. This time we make a more educated guess about our prediction of the actual value. As a baseline for our new prediction we use the initial “naive” prediction we estimated at the very beginning for all observations. We then add our terminal leaf-specific residual prediction to it.   Lastly, since we do not want to overstate our result of the residual prediction we add to the baseline, we multiply that residual prediction by a factor called the learning rate. This learning rate, which is hyper-parameter we can tune, allows us to tell the model how much emphasis it should put on the residual prediction we did.  0.2.1.1 The Learning rate   When adjusting the learning rate, it is important to be aware of the trade-off that comes along with it. A learning rate that is too high leads to jumpy or scattered predictions can cause the algorithm to be unable to find the actual minimum of the loss function. On the other hand, if the learning rate is too low, the model is not adjusting enough and has a hard time to jump away from the “naive” first guess. This could lead to the model getting stuck within a local minimum and not being able to find the desired global minimum.  0.2.1.2 Number of trees   Another important hyperparameter which interacts with the learning rate is the number of trees fitted. How often the algorithm of the Gradient Boosting model should run is also a hyperparameter we can set for the model. The higher number of trees, the more time the algorithm has to improve its prediction. When the learning rate is fairly low, we need to give the model enough iterations to make improve its initial estimate. Conversely, when we set the learning rate very high, less trees are needed.   The following mathematical explanation summarizes how the Gradient Boosting model works. Here we have n observations, built M trees with J terminal leaves. Gamma denotes the prediction of the residual and F the prediction of the actual observation (not the residual). y denotes the true value of the observation. R describes the terminal region of of a tree.      02.2 Code implementation   Having understood the theory behind Gradient Boosting, we will now take a look into the implementation of the algorithm. The library used in this example is scikit-learn. As discussed above, two important hyperparameters of the boosting model are the learning rate and the number of trees (also called estimators). Since these hyperparameters are critical for the overall model performance, it is wise to test out several values for them. This is done through the GridSearchCV command shown below. The results of the GridSearchCV are elaborated on in the next section.   The differentiable loss function used is negative mean squared error, which is the most commonly used loss function for the regression application of Gradient Boosting.  def gb_score(X, y):     # Model assignment     model = GradientBoostingRegressor     # Perform Grid-Search     gsc = GridSearchCV(         estimator=model(),         param_grid={             'learning_rate': [0.01, 0.1, 0.2, 0.3],             'n_estimators': [100, 300, 500, 1000],         },         cv=5,         scoring='neg_mean_squared_error',         verbose=0,         n_jobs=-1)     # Finding the best parameter     grid_result = gsc.fit(X, y)     best_params = grid_result.best_params_     # Random forest regressor with specification     gb = model(learning_rate=best_params[\"learning_rate\"],                n_estimators=best_params[\"n_estimators\"],                random_state=False,                verbose=False)     # Apply cross validiation on the model     gb.fit(X, y)     score = cv_score(gb, X, y, 5)     # Return information     return score, best_params, gb   After finding the optimal hyperparameters using GridSearchCV, we use these hyperparameter values to initialize the final model. In order to assess model performance, and to compare different algorithms to each other, we use the fitted model in 5-fold cross validation.   Next, we take the square root of the negative values of the loss-function. This is done in order to get the so-called root mean squared error (RMSE) which is easier to interpret than the MSE, which is in squared units. The reason we take the negative of the loss function values is to remove the negative sign. This step is necessary in order to apply the square root, given that the negative mean squared error results in a negative loss.   def cv_score(model, X, y, groups):     # Perform the cross validation     scores = cross_val_score(model, X, y,                              cv=groups,                              scoring='neg_mean_squared_error')     # Taking the expe     # of the numbers we are getting     corrected_score = [np.sqrt(-x) for x in scores]     return corrected_score   02.3 Hyperparameter tuning   As mentioned in the prior section, hyperparameter tuning represents an essential part of the model choosing process. For this process GridSearchCV is applied, which tries several values for each pre-defined hyperparameter. This is done through Cross-Validation in order to prevent overfitting. The table below shows which hyper-parameters are chosen to be tested and which values ended up chosen, given superior model performance.    As seen above, Multiple Linear Regression only tests whether to use an intercept. This is done because there are simply no other hyper-parameters to tune for this model.   The Random Forest ends up choosing relatively few (50), but deep (depth of 6) trees. Especially the number of trees is in stark contrast to the number of trees chosen by the Gradient Boosting model, which chose 1,000 trees. Next to that, we also find the learning rate to be relatively high, with a value of 0.3.   It has to be said that performance of Gradient Boosting machines are generally worse for smaller learning rates. Furthermore, fitting more trees allows the model to make more granular predictions and allows the algorithm to fit the data better. Hence, even though both hyper-parameters seem large, they are likely well be justified.   03 Model results  03.1 Model selection   After all three model are initialized with the best-fitting hyper-parameters, their performance is evaluated. This is done through the Cross-Validation scoring code shown in section 02.1.      It is clearly visible that the benchmark of the Multiple Linear Regression performed the worst across of all 5 validation sets. The Random Forest performs better than the benchmark, but still nowhere close to the performance of the Gradient Boosting algorithm. Given the out-performance of the Gradient Boosting model, we will continue our analysis with the hyperparameter-tuned Gradient Boosting model.   03.2 Feature Importance   The Gradient Boosting model is next applied to the test data, which was separated at the beginning of the model selection through the following code:  X_train, X_test, y_train, y_test = train_test_split(final_x, y,                                                     test_size=0.2,                                                     random_state=28)   We set a random state and kept it the same for reproducability across different models. Now let’s take a look at the variable importance to assess which of the created features were most helpful to the model in predicting Real Estate price.      From the chart above we can see that the most important variables are those which came already with the scraping - namely the basic features of how many bathrooms the estate has and how many square meters. Furthermore, the location (longitude and latitude information) of the city of the location also plays an important role in explaining the price.   Interestingly, the cluster variable turns out to be the second most important variable. This confirms our initial belief that condensing the information of several variables to one number in the form of a cluster gives the model an easier time in allocating a property to its correct price group.   The results of the variable importance chart above should be interpreted carefully. The finding that the number of bathrooms turns out to be so much more important than the number of bedrooms, for example, could be simply explainable by the fact that these two variables are highly correlated with one another. In this case, the model is not able to separate the explanatory power of the two variables. Looking at the correlation matrix below, our believe of the positive correlation between number of bedrooms and bathrooms is confirmed - they have a correlation of 0.8.    It is important to remember that our goal was never to pin down the causal effect of propery prices, but to build a model which results in a low prediction error. The difference between these two goals is important. When interested in quantifying the effect of certain features on the dependent variable (e.g. important in policy making), it is essential to also assess the multicollinearity of the independent variables. Given our more straightforward task of building a low prediction error model the econometrical correctness of our features, is of lesser importance.   03.3 Model Assessment for Price Quintiles   Since the dependent variable in our model (Price of the property) was log-transformed in the beginning for better distributional behavior, we have to take the exponential of the predicted log-price values. Afterwards we use the mean absolute percentage error (MAPE) in order to assess the model’s performance. The reason for choosing the MAPE is the large absolute value difference between the price quintiles. For example, if our MAE (Mean Absolute Error) was €100,000, this would be over 100% MAPE for a house costing €50,000, but only a 10% MAPE for houses costing €1 million.  # Getting the predictions gb_predictions = gb_model.predict(X_test) # Bringing them back to normal scale and calculation of MAPE scale_pred = np.exp(gb_predictions) scale_y = np.exp(y_test) mape = (abs(scale_pred - scale_y)/scale_y) * 100   In order to see whether the model predicted low- and high-priced houses equally well, we split the predicted data into ten price buckets, using the actual price of the property. That means that we calculate separate MAPEs for each price quintile.   The graph below offers us interesting insights into the model performance. Namely, it shows that the model performance is relatively consistent for all price quintiles except the for the first quintile. This quintile contains the least expensive ten percent of houses.      One potential reason that the model under-performs on the inexpensive houses could be the amount of variation of the features within the different price quintiles. The chart below tries to shed some more light on this. The graph on the left shows the accumulated standard deviation of all 16 features of the model. The right side shows the variation of the dependent variable, namely the log-price.   The graph shows nicely one potential reason for the struggle of the model to explain the price variation for the cheapest quintile. Namely, the amount of variation is the lowest, as visible in the right graph. Additional to that, we find that the amount of variation in the dependent variable for that price group is relatively high. This combination is dooming for any prediction model, facing a high amount of variation in the variable it wants to explain, but nothing to explain it with.      One might rightly ask why we chose MAPE as an accuracy assessment. An alternative could be, for example the root mean squared error (RMSE). The graph below sheds some light why the MAPE seems like the better fit.   On the left side of the chart below, we see that the RMSE of the lowest price group might seem superior when compared to the RMSE of higher price groups. That superior performance disappears when considering the average price of the quintile.   The reason for the weakness of RMSE as a performance assessment for this prediction task is its absolute nature. For property prices, it makes a difference whether the predicted price is off by €50,000 when the property costs multiple million or only €100,000.   Given that we are more interested in the relative error and not in the absolute one, the mean absolute percentage error seems like the superior accuracy assessment.    04 Conclusion   In the next post we show several examples where the model under-performs and explain why that is the case. Furthermore, we apply natural language processing to use text features next to the quantitative features to explain the price of properties.  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/Predicting-Real-Estate-Prices/",
        "teaser": null
      },{
        "title": "NLP with Real Estate Advertisements - Part 1",
        "excerpt":"Analyzing and exploring real estate advertisement descriptions using NLP pre-processing and visualization techniques.   Pre-processing and exploring the text data   Each real estate listing scraped contains a text component, the ad description. The descriptions are actually previews of the full descriptions, scraped from the listing page.      This description text helps to explain what the numbers alone cannot.   Take for example the following two advertisements, both for houses in the Murcia region of Spain:   House 1: 2 Bedrooms, 2 Bathrooms, Pool, 97m2. Price: €99,800.   House 2: 2 Bedrooms, 2 Bathrooms, Pool, 101m2. Price: €170,000.   How is it possible that House 2 is almost 70% more expensive than House 1? Surely the additional 4 square meters of living space cannot explain that difference. Price differences of such a magnitude with identical feature values make it very difficult for the model to learn.   Looking at the description of each house explains the price difference: House 2 is located directly on the beach. This is important information that will help our model learn to better predict these prices.      After scraping the description preview for all real estate listings, we were left with text samples that looked like this:     “Amazing villa with incredible views in a very attractive area!\\n\\nThe view takes you down over the valley, you’ll be able to see Fuengirola and the Mediterranean sea. The house is located on the way from Fuengirola going up to Mijas. \\nIt i…”   ‘This stunning Villa is located on the edge of Almunecar on Mount El Montanes.\\n\\xa0The location is absolutely stunning with panoramic views of the water / beach, city, mountains. Yes you can look at Salobrena, Motril and Torrenueva.\\nThe 2-st…’   ‘A rare opportunity - a brand new modern villa in a sought after area of lower torreblanca near the beach!\\n\\nVilla Alexandra is a new modern villa centrally located close to the beach in Torreblanca, Fuengirola. The villa is recently compl…’   ‘Price Reduction ! Price Reduction ! Now 495.000 euro\\n\\nThis stunning wooden country house sits on an elevated plot overlooking the village of Alhaurín el Grande with views down the valley and to the Mijas Mountains. The property benefit…’   Our goal is to use the information from this text to help our model predict real estate prices.  Preparing the data for NLP Analysis and Modeling   Before we can extract insights from the text data, we have to prepare the data- getting it into a form that algorithms can ingest and understand.   As we go through these steps, it’s important to keep our goal in mind: what do we want our model to understand from the text? In our case, our goal was to use the description of the real estate advertisement to predict price. At every step of the NLP process, we asked ourselves: will this help our model understand which words have the biggest impact on the price of a house?   The steps taken for analyzing the text data are as follows:      Tokenize the data   Lemmatize the data   Get n-grams   Visualize   Repeat   All NLP models and algorithms require the text data to be prepared - usually in a very specific format - before it can be ingested by the model.   Here, we define some NLP terms we’ll be referencing a lot in this section: tokens, documents, and the corpus.  01 Tokenization   Tokenization involves splitting text into its smallest components, called tokens. Tokens are generally words, though they can also be numbers or punctuation marks. The ‘\\n’ symbol appearing commonly in the descriptions represents a line break and would also be classified as a token. Tokens are the building blocks that, when combined, give text its meaning.   A document is the entire text for each record, or observation, in our dataset. Each advertisement represents one observation, so each advertisement’s description text is a unique document.   The corpus is the collection of all documents in our dataset.   Nearly every modern NLP library today contains a tokenizer. We used the spaCy package to tokenize the text. In this step, we also removed any tokens representing punctuation or known English stop words.  02 Lemmatization   Once we have the text split into its component tokens, the next step is to make sure these tokens contain as much information as possible for the model to learn from. There are 2 common ways to do this: lemmatization, and stemming. Both lemmatization and stemming are used to reduce words to their root. The idea behind using both of these methods is that the root of the word contains most of the word’s meaning, and that the model learns associations better when explicitly told that words with the same root are essentially the same.      We used lemmatization because it has the advantage of always returning a word that belongs to the language. This makes it easier to read and understand than stem words, especially when the roots are used in visualizations. We also were not concerned about lemmatization taking longer and being more computationally intensive, as our corpus is not extremely large. Lemmatizing the entire corpus took about 10 minutes on a local machine.  03 Creating n-grams   After stemming and lemmatizing the words, we can extract n-grams. N-grams are token groups of n words that commonly appear together in a document or corpus. A unigram is just one single word, a bigram contains 2-word groupings, and so on. N-grams help us quickly identify the most important words and terms in our corpus.   We wanted to see which uni-, bi- and tri-grams were most important in the entire corpus. This is also a great way to quickly visualize our text data and assess how well we’re preparing our description text for our goal of helping the model predict price.  from nltk.util import ngrams def get_n_grams(processed_docs):     \"\"\"     Getting uni- bi-, and tri-grams from text.     By default creates grams for entire corpus of text.     Returns dictionary with {gram_name: gram}     \"\"\"     # initializing list for tokens from entire corpus- all docs     total_doc = []     for doc in processed_docs:         total_doc.extend(doc)     # extracting n-grams     unigrams = ngrams(total_doc, 1)     bigrams = ngrams(total_doc, 2)     trigrams = ngrams(total_doc, 3)     # getting dictionary of all n-grams for corpus     gram_dict = {         \"Unigram\": unigrams,         \"Bigram\": bigrams,         \"Trigram\":trigrams}     return gram_dict   04 Visualizations   Once we extracted our n-grams, we visualized which words were the most important. We looked at these visualizations through a critical lens and asked, “are these words helping our model learn to estimate price? How can they better support our model’s understanding?”   Below are the initial n-gram plots, plotted with words from the entire corpus of advertisement descriptions.        The most common uni-, bi- and tri-grams are “apartment”, “living room” and “bedroom 2 bathroom”. These are all typical real estate-focused words, and many of them are likely not helping our model’s prediction of price. For example, the most common trigram “bedroom 2 bathroom” is likely arriving from all X bedroom 2 bathroom listings. This information is already captured by our explicit bedroom and bathroom features, so it’s not giving our model any new information and should be excluded.   Other terms like “living room” should also be excluded- it’s likely that virtually all homes listed have a living room, and it’s unlikely that the inclusion of this word in a listing has much impact on the target variable of price.   05 Repeat  After looking at our initial visualisations, we decided to add the following steps in our text pre-processing pipeline:   01 Removing additional real estate-specific stop words, like those listed here:  real_estate_stopwords = [     \"area\",     \"province\",     \"location\",     \"plot\",     \"hectare\",     \"m²\",     \"m2\",     \"sq\",     \"sale\",     \"square\",     \"meter\",     \"bedroom\",     \"bathroom\",     \"room\",     \"living\",     \"kitchen\",     \"hallway\",     \"corridor\",     \"dining\",     \"pool\",     \"apartment\",     \"flat\"     ]   For words like “bathroom” and “pool”, it was easy to justify their removal with the argument that they’re already explicitly included in the numeric features. However, with other words like “apartment”, “flat”, and “house”, it wasn’t so easy to know if we should remove them. It’s easy to imagine that whether the property is a flat or a house could have an impact on the price.   02 Remove all numeric tokens   We removed numeric tokens to avoid adding redundant information, and to assure we weren’t accidentally including our dependent variable (price) in our features.   After adding these steps to our pre-processing pipeline, the n-grams became much more interesting:      The bigrams and trigrams are no longer dominated by words like bedroom and bathroom. They now contain price-relevant terms like “sea view”, “enjoy leisure time”, and “golf course”.   Further Visualizations: Word Clouds   Word clouds are a great way to further understand text data. Our next step was to use word clouds to visualize two subsets of our corpus: the most and least expensive 5% of all observations. Our hypothesis is that the words included in these word clouds should be different: words used to describe inexpensive houses might include things like “fixer-upper” or “cozy”, whereas we might expect to see words like “dream home” or “extravagent” used to describe the most expensive homes.   We started by visualizing the word cloud for the entire corpus of text. This is the same data used to create the n-gram visuals above.      Next we created a wordcloud for the cheapest 5% of listings, which worked out to be listings under €75,000. Since our dataset contains about 32,000 listings, the top and bottom 5% each contain roughly 1,570 listings.      The words “villiage” and “town” appear much more commonly in the inexpensive houses- which makes sense as real estate in rural areas generally costs less than in cities. There are also many words about contacting the real estate agent or requesting information. This could signal that these cheaper homes are priced to move and the seller and real estate agent are motivated to sell.   For the word cloud of the most expensive 5% of properties, the cutoff price was €1.7 million.      Here we can clearly see a stark difference in the most common words. Sea views are more common, as are adjectives like “luxury”, “exclusive” and “high quality”.   Now that we have our text prepared and are confident that there is a difference in the description tokens of inexpensive and expensive properties, we can turn to applying our text data to the model in the form of a TF-IDF feature vector.  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/NLP-with-Real-Estate-Advertisements-Part-1/",
        "teaser": null
      },{
        "title": "NLP with Real Estate Advertisements - Part 2",
        "excerpt":"Applying our NLP feature vector to the Gradient Boosting model.   Recall from Part 1 of analyzing real estate advertisement descriptions that we prepared our data for modeling by splitting it into tokens and removing common real estate stop words. These initial steps made it easier to visualize our text data, but before we can use the text data in models, we need to put our words in some kind of numeric format. This is because ML models can only take numeric data as inputs.   In this article, we show how we prepared the data for use in machine learning models using a TF-IDF vectorizer, and how these features impact the model.   This article proceeds as follows:     TF-IDF vectorization of text features   Inclusion of TF-IDF features in XG-Boost model   PCA for dimensionality reduction of text features   Conclusion   01 TF-IDF Vectorization of Text Features   TF-IDF stands for Text Frequency-Inverse Document Frequency. It is a ratio of how often a word appears in a given text, compared to how often that word appears in all texts.      To understand how TF-IDF vectorization works, we’ll look at a simplified example. In our example, we have 2 short real estate advertisement descriptions where stopwords have been removed.      Note that Advertisement 1 contains 6 words, whereas Advertisement 2 only contains 5 words. Both advertisements contain the term “for sale”, though these words are in a different position in each of the advertisements.   To calculate the TF-IDF score for each advertisement, we first need to calculate the inverse document frequency, IDF, for each word. The first step in calculating IDF is to divide the total number of documents, N, by the number of documents containing the given word. Then, this inverse fraction is logarithmically scaled.      Note that only the words “for” and “sale” have a lower IDF score, since they are the only words that appear in both documents. All other words only appear in one document each, so they each receive the same score.      Next, for each document, the term frequency (Tf)is calculated. This is simply a count of how often each term appears in the document. Since each advertisement only has 5 or 6 words and each word only appears once, the term frequency is never higher than 1 for each document.      With term frequency for each document, a matrix multiplication is done with the term frequencies and inverse document frequencies to arrive at the final TF-IDF vector for each document.      TF-IDF scores for each advertisementNote that the scores for the words that appear in Advertisement 2 receive a are always a bit higher than the scores for the words in Advertisement 1. Since Advertisement 2 contains fewer words than Advertisement 1, each word is counted as relatively more important.   To better understand how TF-IDF vectorizes our Spanish real estate data set, we’ll look at the same example we used in Part 1 of analyzing subsets of “cheap” and “expensive” homes in our data. Recall from Part 1 that we defined “cheap” homes as the most inexpensive 5% of our data, those under €75,000, and the “expensive” homes as the most expensive 5% of our data, those above €1.7 million.   The TF-IDF scores for the cheapest and most expensive properties are shown below. The TF-IDF scores shown are the sum of all scores for the word for each advertisement that was included in the “cheap” or “expensive” category.   cheap_property_TF_IDF_scores = [('town', 122.68691198513302),  ('village', 85.8624409730794),  ('like', 84.06423081796622),  ('need', 72.47179535414357),  ('terrace', 71.42720081950334),  ('large', 68.99777093615201),  ('townhouse', 68.9844994563653),  ('family', 63.417517090288484),  ('space', 62.07432797305012),  ('build', 60.04214108962623),  ('distribute', 58.4163261755536),  ('street', 58.24476489238489),  ('situate', 57.41386787457465),  ('close', 54.99513021566562),  ('child', 53.1243500223459),  ('good', 50.895927008310515),  ('ideal', 50.66644469452883),  ('size', 47.013810045109445)]  expensive_propertytfidf_list = [('villa', 217.80253394946453),  ('view', 147.775603675447),  ('sea', 124.69304169060362),  ('build', 105.68834130627678),  ('luxury', 98.60403119678404),  ('exclusive', 93.06584872644288),  ('modern', 85.62951731804527),  ('beautiful', 85.62351304555642),  ('design', 74.60755941148277),  ('offer', 70.22014987779879),  ('minute', 67.10389304885832),  ('beach', 64.12585728939085),  ('unique', 63.49425807437234),  ('spectacular', 61.895439206382186),  ('high', 58.92975340566454),  ('town', 58.88803267728475),  ('large', 57.488549315155126),  ('stunning', 55.85328236142857),  ('quality', 53.57454240044263),  ('style', 51.88205837353272)]   These words are many of the same words that were included in the “cheap” and “expensive” wordclouds.   The TF-IDF vectorizer has a few hyperparameters that, when adjusted, change the vector they create. Perhaps the most important of these hyperparameters are min_df and max_df. Min_df defines the minimum number of documents in which a word must appear in order for it to be counted. Setting this value to 0.05, for example, means that words which appear in only 5% of the documents, or less, are not included. In the context of real estate listings, this would likely exclude words like a particular street name or seldom-used adjective that only occur in one advertisement and can prevent overfitting. Max_df, on the other hand, defines the maximum number of documents in which a word can appear. This prevents words which appear in almost every listing from being included in the feature vector. Terms like “for sale” would likely be excluded with this metric.   Below is a list of some of the words which were excluded with the min_df=0.02 and max_df=.90 with our real estate dataset. This means we excluded words that don’t exist in at least 2% of all property advertisements, as well as words that exist in more than 90% of all advertisements.  selected_exluded_words = ['kennel', 'ciencias', 'mayores', 'castiilo', 'montroy', 'worthy', 'furniture', 'ricardo', 'fend', 'españa', 'iron', 'rotas', 'sans', 'alike', 'portals', 'dividable', 'majestically', 'ladder', 'communicate',  'orientation',  'grass', 'visited', 'identify', 'setting', 'café', 'specimen', 'dorm', 'unsurpassed', 'later', 'tarred', 'oil']   Limiting the NLP features considered in this way decreased the dimensionality of our TF-IDF feature matrix from 13,233 columns to 158 columns, meaning 158 terms were then used to train the model. This drastically decreases the dimensionality of the NLP feature vector, as well as decreasing potential noise.  02 Inclusion of the NLP features in the XG-Boost model   These 158 additional features were then fed in as additional training features to the XG-Boost model. The model’s hyperparameters were also tuned using GridSearchCV.   The improvements in performance were quite surprising. The best MAPE score achieved on the first, and hardest to predict, quintile of data using the baseline features was a 46.74 % error. Including the 158-feature TF-IDF matrix, this error was cut nearly in half to 27.01%.         We further investigated the impact of the additional NLP features by looking at observations where the NLP features led to especially large model perfomance gains.   We identified almost 50 properties where the prediction improvement using NLP was more than 100%. Of these, nearly all were properties where the “logs-only” model had predicted much too high.      The word “opportunity” seems to be one that really helps the model learn that a property should be valued lower. The inclusion of the words “town”, “village” and “rural” also fits with our understanding of words associated with inexpensive properties found in Part 1.   The inclusion of NLP features improved the overall model performance drastically. However, there were some individual observations where including the NLP features increased the absolute percentage error for those properties. Upon closer investigation, many of these contained no description. This then makes sense that the model trained on a feature set which includes NLP features did a worse job predicting these observations with no description. The model now relies more on the NLP features and less on the original features. So when there are no NLP features, the model does a worse job of predicting since it is putting less weight on the original features.   03 Dimensionality reduction of text features using PCA   It’s clear that the inclusion of NLP features greatly improved model performance. However, the number of features it adds is quite large - the NLP feature vector adds 158 additional features.   One of the most common methods for reducing dimensionality in input features is Principal Component Analysis, or PCA. PCA works by projecting the features onto a smaller (lower-dimension) vector space.   We started with mapping the 158-NLP feature matrix onto an 8-feature PCA feature space.      The MAPE in each price quintile is lower (better) than using the logged features alone, but significantly higher than when using all 158 NLP features.   Conclusion  Adding the full 158 NLP features greatly improved model performance. Using only 8 PCA principal components still improves model performance over the original model, but not nearly as much as including all 158 features.   The decision to include all 158 features or only the 8 PCA features depends on what is needed from the model. In this case, it is likely that the gains in performance outweigh the slightly longer prediction time caused by including all NLP features.   In the next article, we summarize the work done so far and look forward to potential next steps.  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/NLP-with-Real-Estate-Advertisements-Part-2/",
        "teaser": null
      },{
        "title": "Conclusion - Predicting Real Estate Prices with Features Scraped from the Web",
        "excerpt":"What we learned predicting real estate prices with data scraped from the web.   We started this data science project with a seemingly simple question — “What is the price of this property? How can we improve our price estimations?”   The past articles in this series have shown the work needed to answer this question. After gathering data from real estate web advertisements using web scraping, a significant amount of time was spent cleaning the data and extracting the features needed for modeling.   We tested multiple variations of regression models, including Random Forests, Adaptive Boosting, and XG-Boost. XG-Boost was the clear winner and the algorithm used for all later modeling. All MAPE values shown were calculated with the holdout test data.   We started with an initial baseline XG-Boost model using the features that were easiest to extract from the scraped website advertisements. This model had roughly a 24% MAPE on price quintiles 9–10, but a MAPE of over 50% on the lowest priced houses.      We showed that this much larger error in the first quintile was likely caused by the lower variance in the feature data, yet relatively higher variance in the target variable, for this quintile. We also showed how taking the log value of many of the continuous numeric features and re-training the XG-Boost improved the predictions in the lowest quintile by about 7%, though it did not significantly change the errors in the other quintiles.      After these improvements were made, we shifted our focus to extracting more features from the advertisement’s description text. We used a TF-IDF vectorizer to extract features from the text and added these features to the logged features. This significantly improved the performance of the model in all quintiles, but most significantly in the lowest price quintile, where MAPE improved by about 19%.      Finally, we tried reducing the dimensionality of the NLP feature vector using principal component analysis, PCA. The model trained with logged features and the 8-feature PCA representation of the text features still outperforms the model trained only with logged features, but underperforms compared to the model with all 158 text features.      Final model selection   Many models using different combinations of features were trained for this project — it is helpful to compare them side by side.   Below is a table showing the MAPE for each price quintile for all models discussed above.      Here, the almost 50% improvement between the original baseline model (No logs) and the final model (Log Values + All Word Vectors) can be clearly seen. Overall MAPE is less than 20%, compared to our initial overall MAPE which was nearly 40%.   To put this into perspective- in the lowest quintile, a house may cost €50,000. Our original model would, on average, predict this house with a 50% absolute error, meaning it would predict either €25,000 or €75,000 for this house. The final model, with MAPE of 27%, in contrast, would predict either €36,500 or €63,500 — both much more reasonable and closer to our real target value.   For the top quintile, the results are even more exciting, as the model predicted throughout model configuration better for the more expensive estates. An estate in the highest quintile of our data cost around €1,500,000. The final model MAPE for the highest quintile is 15%. Hence, the model would on average predict something between €1,275,000 and €1,725,000, which is not a bad prediction at all.   With our final model, and the data understanding we’ve gained throughout this project process, we can now answer our original question of which factors are most influential in predicting the price of Spanish real estate, and how we can improve our estimation.   The features bedroom and bathroom were highly correlated, so when both were included in the model, bathroom was much more important. Square meter of the flat was also very important, followed by distance to various sized cities. Improving the features by taking the log of most continuous variables further improved predictive power, and including descriptions that gave insight into the condition of the property and further amenities greatly improved the model.   Caveats   On the last note, it is important to stress the nature of real estate pricing, which is more an art than an actual science. Just because a house is on the market for a certain price, does not mean that this price is in any way correct. Most house prices are subjectively chosen by real estate agents and/or home owners. It is also very likely that different agents would assign quite different prices for the same estate. Any type of prediction model would have problems to capture this kind of sentiment in the target variable.   Another problem is the selection bias we encounter by scraping the data from the web. Imagine the following: A proud home owner decides to sell her house. After talking to multiple real estate agents she thinks that she truly knows the worth of her house when putting a price tag on it. After she left the computer, her kid puts his little bungalow, which is located in the front yard of the house, on the web for the very same price, just as he saw his mom do. Since the actual house was very reasonably priced, it got sold soon after being put on the market. After it got sold, the ad was also taken down from the web. The little bungalow, on the other hand, will be on the web forever, waiting to mess with the data of some data scientists.   This illustrative example underlines two points: First, the data we used to train our model can, even after initial outlier detection, still be very biased and full of the aforementioned completely unreasonably priced bungalows. Secondly, the final prediction could also well be the true value of the estate. A real estate agent does not run any model to come up with a price but rather uses his/her gut-feeling.   Learnings &amp; Next Steps   Overall, we are very happy with our final model’s performance, especially on the more difficult to predict lowest quintile. The inclusion of NLP features and logging high-variance continuous variables drastically improved our model’s performance.   That being said, there are still additional features and methods we would like to try that we think could further improve the model’s performance.   One additional feature we would like to add would be to include a more explicit representation of location — one that more directly impacts price. We realized especially after plotting the latitude and longitude data and seeing how the properties in the Canary Islands skewed the latitude and longitude data. This feature could be a ranking of each town or city based on average price in the city. For example, a more expensive city like Barcelona would receive a high ranking whereas a small town in the mountains would recieve a low ranking. Adding such a feature would also not increase dimensionality of the data.   Concerning the process of the project, we were happy with how it proceeded. We trained baseline models very quickly and used these results to inform practical improvements. We visualized often and at every stage tried to further increase our understanding of the problem and factors influencing the model’s performance. We pivoted quickly if it looked like a new feature or approach wasn’t working and always kept the main question in view.   The next step for us is to deploy the model so that it can predict on new data. Model deployment can take a variety of forms. One option would be an API, for example in a simple cloud-based web service. A further step would be to build an interactive front end to this API. Further next steps involve taking the knowledge gained in this project to future data science projects, and incorporating feedback from this project and further reflection.   Get in touch   If you’re interested in this project, have a question for the authors, or would like to discuss the methodology, get in touch at data4help.contact@gmail.com.  ","categories": ["Real Estate","Python"],
        "tags": [],
        "url": "/real%20estate/python/Conclusion-Predicting-Real-Estate-Prices-with-Features-Scraped-from-the-Web/",
        "teaser": null
      },{
        "title": "And the Oscar goes to...",
        "excerpt":"As an economist it is pretty frustrating to hear the news anchor saying that the cast of the new James Bond movies did it again. “The highest gross revenue ever…” Of course, we are more than convinced that Daniel Craig gave his everything, but the inflation rate helps as well.   Surprisingly many of monetary statistics which can be found online do not account for the phenomena of inflation. This leads to the dominance of movies produced since the 2000s, as can be seen for example here. In order to give a bit clearer view on monetary success of the different movie genres, actors and directors, this blog-post scraped data on over 300k movies from ImdB and analysed the monetary performance, while adjusting for inflation.   The blog-post is structured as follows: We start with an explanation of which deflator was used and how it was applied. Afterwards we look into the highest inflation-adjusted gross revenue by genre, stars in the movies, directors and movie titles. We conclude the post with showing the development of inflation-adjusted revenue over the years.  01 Inflation Factor  01.1 Basic understanding   There are two main terms when talking about monetary units, namely the nominal and real value of money. Nominal monetary units are the prices as they are reported at the time. When we go to the cinema today and ask for a ticket, we get told the nominal price of the ticket. If we would like to compare how much we paid for that ticket to the ticket our grandparents bought back in the day, the nominal amount they paid is not of big interest to us. It does not convey any comparable information without knowing what the price level was at that time. That is where the real value comes in - in the inflation-adjusted version of the nominal value.   In order to go from the nominal value to the real value, we need an adjustment factor for inflation. For this purpose, a price index is used. A price index gives an insight on how expensive the world is in a given time period. This is approximated by noting how expensive an extensive basket of products is for every time period. For better understanding, one could imagine going to the grocery store once every year and buying the exact same items. Afterwards, one notes what was paid for the total amount. Since the basket contains many items, it represents some sort of a diversified sample. Some things got more expensive, some things got cheaper. Overall it gives an indication what an average person has to pay to live at a certain point in time. Over the years one would observe price changes which are then said to be, on average, due to increase or decrease of the value of money. If things got more expensive, we are talking about inflation. If things got cheaper, we talk about deflation.   In reality, nobody goes to the grocery store to buy that basket. This is done on a big scale, including many more items than only the ones you can buy at a grocery store, like for example energy, transportation and water. Furthermore, effects like the substitution and quality bias also have to be considered.   The correct measures to be taken are quite complex and also not in the scope of this article and are not needed to know for the further understanding of this article.   01.2 Deflator used   Nowadays price indexes are made tailored for certain industries, in order to have a better understanding how the much cheaper/ expensive certain industries are on their own. In our case, where we are interested in comparing gross revenue of movies over the years, we used a price index for all goods instead of using one tailored to the entertainment industry. This is done to ensure a more extensive time series. Given that many movies go quite far back in time, not having a deflator for those years would make it impossible to calculate the inflation-adjusted gross revenue for those movies. We would then have had to drop all of these older movies. Since a longer time series was more important to us then a perfectly matching deflator. The time series is taken from here and plotted below.      As seen above, the price index rises monotonically over the years. This means that the average price of living went up over the years, meaning we experienced inflation over the years.   Lastly we briefly elaborate of how to go from a nominal value over to a real value. As can been seen from the chart above, the base year, to which all future years are going to be compared to, is 1960. For that starting year, we see the value of 100 was assigned. This number gains meaning when comparing to the number 300 in approximately 1980. It means that the prices are 300% as high as they were in 1960. In order now to compare prices, we take the prices of 1980 and divide them by 300%, or 3. This methodology is then applied to all movie’s gross revenue.   02 Revenue figures by…   After introducing the workings of inflation and how we adjust for its effects, we now look into the more interesting results of inflation-adjusted gross revenue. This is done by splitting the results into the different variables we average by, namely Genre, Actor/Actresses (Stars) and Movie-Directors.   Before diving into the numbers, it is to be said that all number rely on the information that was provided on the ImdB website on the beginning of May 2020. That means that it could happen that newly released movies in 2020 do not have any information about the gross revenue or that some movies in especially very early years do not have any information listed regarding their gross revenue.   02.1 …Genre   Below we can see the average inflation-adjusted gross revenue of the different genres. From today’s viewpoint, it was probably to be expected to find Adventure and Action genres on top of the list. Surprisingly, the mode (category which appeared the most often), Drama did not make it onto the top ten list. This could be potentially due to the fact that basically everything could be labelled as a Drama movie, since it does not require many special effects like for example in Adventure or Sci-Fi movies. Many of these Drama-titled movies then apparently were a financial disappointment, dragging down the average of this category.      02.2 … Stars   Before talking about the graphs below, some methodology of how the numbers are calculated is necessary. When encountering a movie with multiple actors (most movies), the gross revenue the movie received is then assigned to each actor with the full amount. This means that if Tom Hanks and Tom Cruise acted in a movie that brought in 100 Euro of gross revenue, the data is split in a way that both actors got assigned 100 Euro for that movie. Afterwards, the average gross revenue over all movies is calculated for each actor.   It is important to stress that this number is not the same as what the actor got paid shooting that movie. The numbers below serve as an indication to show how high the average gross revenue of a movie is when certain actors have a role in the movie.   In order to adjust for one-hit wonders, we implemented the rule that only actors which starred in at least three movies were taken into consideration.   Below we can see the surprising results of that averaging:      Not many people would have thought that Taylor Lautner would rank as number one on that list. Rightfully so. This statistic is a bit problematic in that it favors certain actors more than others. This theory can be explained by transferring the idea to a casino.   When asking which gambler earned on average the most money in the casino, we will not end up finding the people who go there every single day for thirty years. We will find the person who maybe went to the casino twice and won, due to luck, a considerable amount. This is because, being in the industry for a long time allows for a inevitable bad financially performing movie. Many movies of Tom Hanks at the beginning of his career for example, were not the biggest success story, even though his later movies were.   Taylor Lautner starred in exactly four movies in our dataset. He probably acted in more, but there are four movies under the ImdB link given above, for which Taylor Lautner was listed as an actor. These movies are:      The question may arise why Robert Pattinson or Kristan Stewart are not listed in the average top ten list, even though their co-star Taylor Lautner is. The answer is the number of movies shoot for the former two actors. Robert P. and Kristan S. both starred in many more movies than only these four and therefore have a lower average. This example shows how this graph above gives a biased view.   If, on the other hand, we would like to see which actor/actress ranks highest when using cumulative gross revenue over all movies they starred in, we find a more expect-able result, with Tom Hanks as number one.      02.3 … Directors   The same problem we encountered with the actors/actresses above rears its head again when ranking movie directors by average revenue. Looking at the graph below, one might wonder why the household name Steven Spielberg only ranks so low on this results table.      The answer is again the lucky observations which did not encounter a flop during their career, but also did not shoot as many movies as some of the others. When looking again at the cumulative statistics, it becomes clear who rules this business.      The cumulative inflation-adjusted gross revenue for Steven Spielberg is now twice as high as the one for George Lucas.   02.4 …by movie   Maybe the most interesting graph of this post is the one below. Here we show the inflation-adjusted gross revenue of all movies since 1960. The winner is Star Wars IV - A New Hope released in 1977. Even though Avengers: Endgame got a lot of publicity lately given its massive gross revenue, it’s still smaller than One Hundred and One Dalmatians. We are aware that the ranking shown below differs somewhat to other rankings shown on the web, even to ones which adjust for inflation. This could have many reasons, ranging from different sources for the gross revenue over to using a different price index.      03 Development by Genre   Lastly it would be interesting to see how the inflation-adjusted gross revenue changed over the years. For this purpose two graphs are shown below, one for all years in the data and another one which has a focus on the 2000s.   03.1 Overall   In order for the statistic not to be completely distorted by stark outliers, only years are considered which have more than one movie in a certain year.   Especially interesting is the performance of the Family genre, which spiked around the 1960s and continued strong until this day. From a financial perspective the strong performance of family movies is not surprising, targeting a market which pure action movies cannot take.   It is also visible that the inflation-adjusted gross revenue average was considerably higher in the early years of the data compared to the more recent years. That fact can also explained by the fact that there are considerably more movies produced in the last twenty years in contrast to the years between 1960–1980. The increase of movies produced is, according to the data, not completely aligned with people going to watch these movies. This fact leads to a lower average of gross revenue nowadays.    To prove this theory outlined above is proven below. The two graphs below show the cumulative inflation-adjusted revenue over the years as well as the number of movies in every year since 1960.      It is clearly visible that nowadays there are massively more movies produced and the industry as a whole got consistently larger over the years. On average, though, each movie had a smaller gross revenue given the sheer amount of movies and competition.    The increasing number of feature films made in a year is not the only thing impacting declining revenue per film. The number of other entertainment options available to people have also increased substantially, with streaming services today, and movie rentals available from about the 1980s onwards. These factors all likely influence the much higher adjusted revenue per movie for movies made in the 1960s and ’70s.   03.2 Focus on 2000s   As a last step, we thought it would be interesting to zoom in to the average inflation-adjusted gross revenue since 2000. When comparing the average inflation-adjusted gross revenue in last years, we find a very volatile line for the Sci-Fi and Adventure category. This is likely do be due to the blockbuster releases in these years.   We would like to end up on the nice note that the graph below shows that the exploding-cars-showing Action genre is still behind the more thoughtful and happier Family genre.      04 Next up   The next blog post looks more into the movie description and makes an attempt to predict the movie category using only the description text of a movie.  ","categories": ["Data Journalism","Python"],
        "tags": [],
        "url": "/data%20journalism/python/And-the-Oscar-goes-to-/",
        "teaser": null
      },{
        "title": "Multi-label Genre Predictor with Interactive Dashboard",
        "excerpt":"An intuitive explanation of how to deal with a multi-label classification problem using ImdB movie-data.   This blogpost walks through the entire process of building a genre prediction model, using movie descriptions as the sole feature.   This post is structured as follows: We start by giving some background information about the origin of the data and show an example of what information can be extracted from ImdB. Afterwards, pre-processing of the movie descriptions is explained in detail, including an elaborated section on the workings of tfidf-vectorization.   Next, we explain how we dealt with movies which are labelled with more than one genre and motivate our modeling choice. Lastly, we present our results in a custom-build web application designed for this project. The app can be accessed here.   Data Origin   As shown in the last blog post, the movie data was scraped from the ImdB website. We scraped approximately 200k movies with their genre as well as with their descriptions.   Below we see an example of what information is extracted for each movie. The scraped information is indicated with a red square and consist out of the movie genres as well as the movie descriptions.    Data Processing   Before we can feed any model the movie descriptions, some pre-processing is necessary. Specifically, we must transform the current movie description into some form of numeric representation, in order for the model to use that information. In total we apply four pre-processing steps, outlined below:     Tokenizing   Removing stopwords   Stemming   Tf-idf vectorization   All four bullet points contain some sort of buzzwords. In the following we will go over each of them and explain what they mean.   Tokenizing describes the process of breaking down the text into its smallest components, called tokens. Tokens are generally words, but can also be numbers of punctuation. Combined, tokens represents the building blocks which give text its meaning. Stop-words are a list of words which are removed from the text, since they carry no information. Stop-words refer to the most common words in a language, or used in a particular setting. Specifically because they are so commonly used everywhere, they do not tend to care any significant information and can therefore be removed in order to allow the model to focus on the more important tokens. Stemming is one of two methods (the other is called lemmatization) to reduce a word to its so called root. The roots of words contain the majority of the meaning. That procedure is best explained by looking at an example. Consider the following two made up sentences:   …he was in quite some trouble… …she was troubled by what she witnessed…   Reading the two sentences, a human being can tell that even though the two words trouble and troubled are not the same word, they carry a similar meaning. A machine, on the other hand, does not know that these two words should be treated as similar or even the same, since they are spelled differently and thus represent two different tokens.   One way to help the machine is to break down each word into its root. Using the NLTK Python package for stemming, this break-down would mean the following for our example:   trouble → troubl troubled → troubl   The example above shows two things. First, both words are identical after the stemming. That is great news for the model, since now it will know that the meaning is the same. The second observation is that the resulting word is not really a word anymore. Exactly here lies the difference between stemming and lemmatization. Stemming does not require that the resulting string carry any meaning for a human reader, whereas lemmatizing always returns a word that actually exists in the language.   Pre-Processing Example   At this point it would make sense to take a look what all pre-processing steps up until now have done to our text. Below we see what our pre-processing has done to the movie description of our example description of The Godfather (1972).      Raw: The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son. Processed: age patriarch organ crime dynasti transfer control hi clandestin empir hi reluct son   The result is hardly readable anymore. Especially the stemming took its toll on readability, reducing every word to its word-stem.   Term frequency-inverse document frequency   The last step on our pre-processing list if tf-idf vectorization. Since this method represents an important step and is not as straight forward to explain as the other pre-processing steps, we will explain that approach in a bit more detail.   The main motivation for the tfidf- vectorization is the translation of the string format of the movie description into a numeric representation of this text. This is necessary given that the prediction-model needs a numeric input.   One way to transform a combination of strings into a numeric vector is a methodology called term frequency-inverse document frequency, or tf-idf for short. It represents a ratio of how often a string appears in a given document, compared to how often that word appears in all documents.      To understand how tfidf vectorization works, we’ll look at a simplified example from the real estate world. In our example, we have 2 short real estate advertisement descriptions where stop-words have been removed.      Note that Advertisement 1 contains 6 words, whereas Advertisement 2 only contains 5 words. Both advertisements contain the term “for sale”, though these words are in a different position in each of the advertisements.   To calculate the tf-idf score for each advertisement, we first need to calculate the inverse document frequency, idf, for each word. The first step in calculating idf is to divide the total number of documents, N, by the number of documents containing the given word. Then, this inverse fraction is logarithmically scaled.      Note that only the words “for” and “sale” have a lower IDF score, since they are the only words that appear in both documents. All other words only appear in one document each, so they each receive the same score.      Next, for each document, the term frequency (tf) is calculated. This is simply a count of how often each term appears in the document. Since each advertisement only has 5 or 6 words and each word only appears once, the term frequency is never higher than 1 for each document.      With term frequency for each document, a matrix multiplication is done with the term frequencies and inverse document frequencies to arrive at the final tf-idf vector for each document.      Note that the scores for the words that appear in Advertisement 2 receive a are always a bit higher than the scores for the words in Advertisement 1. Since Advertisement 2 contains fewer words than Advertisement 1, each word is counted as relatively more important.   Target variable processing   After we covered the pre-processing of the movie-descriptions, we focus now the target variable - the movie-genre.   Multi-Label   One of the most difficult questions of this project is how to deal with movies which are labelled with more than one genre. In the example of The Godfather, for instance, we see two genres, namely [Crime, Drama].      The immediate thought one could have is to label that movie as what it is, namely a [Crime, Drama] movie observation. This solution might seem appealing, given its ease of implementation, but it comes with a drawback which is illustrated below.   Considering the following three potentially possible movie-genres (a movie can have at maximum have three genre labels):   [Crime, Drama]   [Crime]   [Crime, Drama, Thriller]   Since assigning movies a genre is much less a science but rather an art, it is very difficult to say why a movie is labeled [Crime, Drama, Thriller] and not [Crime, Drama]. This potentially subjective source of labeling makes it very difficult for any Machine Learning model to learn the difference between the three examples given above.   Furthermore, we would need quite a high quantity of observation if we would like to ensure that every single potential combination of three/two movie-genres has sufficient data to train the model.   Splitting Multi-Label into Multi-Single-Label   A different solutions that seems more appealing is to split a movie observation with n-genres into n-observation, one observation for each genre. Below we find a visual explanation how this genre-splitting procedure would work:      This is then done to every movie which has more than one genre. After applying this to all observations in our dataset we end up with the following frequency distribution of genres.      Barchart to illustrate how many single-label movie-genres we have. The cutoff is set at 2500 observationsSince this approach requires it to train a prediction model for each movie-genre individually we have to ensure a minimum number of observation for a genre in order to ensure we have enough training observations. Therefore only movie-genres with more than 2500 observations are considered, which is represented by the red cutoff line in the graph.   In the next step we talk about how we were able to help the model learn from the movie-genres.   Genre Intensity  Splitting the movie in the way done above made us think about whether it is fair that a movie which is labelled as [Horror, Comedy, Thriller] carries the same weight for the prediction model for the comedy genre as the movie which is solely labeled as a [Comedy]. These two movies are probably very different pieces of art, and their descriptions are also likely to be quite different. It is reasonable to assume that the model has an easier time learning what a movie description for a [Comedy] movie looks like when seeing a “pure” [Comedy]-labeled movie description. For that reason we apply a weighting mechanism.   Namely, what is done is that a movie which is only labelled as one genre is duplicated three times, since three is the maximum amount of genres a movie can be labelled under on the ImdB website.   A movie which has three labels is not duplicated at all after the split, carrying therefore less importance for the model in contrast to a one-genre movie. A two genre-labeled movie is then duplicated once (appears 2 times) to also indicate a higher importance to the model.  Visual Example - Target Variable   A complete example of how that entire process for multi-genre problem looks like is shown below:      Model - Logistic Regression   At this point we have translated every movie description into a concise numeric vector with exactly one genre-label. When turning to the modeling side of the project, the question arises how to deal with having unbalanced multiple target variables. One solution, which is also implemented for this problem, is to train one model for each genre.   Genre Prediction   Below we see how well the different algorithms are able to predict the different genres. We can see that overall, the model struggles more to predict genres like [Drama] or [Comedy], whereas it is quite good at prediction other genres like [War] or [Horror].   The likely reason for this is the inflation of movies labeled Drama or Comedy. If you think about it, nearly every movie is a Drama in one way or another. The bar chart shown earlier also showed that the mode as well as the second most-often movie-genre is [Drama] and [Comedy]. This makes it very difficult for the model to learn these often used movie-genres.      Web App - Interactive Dashboard   In order to present the final result of the model, we built an interactive Dashboard. Here, one can insert any real or made-up description of a plot and let the model calculate which genre it is the most likely to be.   Below we find a screenshot showing how the app interface looks. On the left side one can insert the plot and press submit. The algorithm then calculates a probability for each genre individually. All probabilities together are then normalized to sum up to one and plotted on the right side.   The genre with the highest probability is then colorfully highlighted and the shown with its individual probability on the left side.   The app can be accessed here.     ","categories": ["Data Journalism","Python","Dash"],
        "tags": [],
        "url": "/data%20journalism/python/dash/Multilabel-Genre-Predictor-with-Interactive-Dashboard/",
        "teaser": null
      },{
        "title": "Richter's Predictor- Data Challenge from DrivenData",
        "excerpt":"Scoring in the top one percent in the Richter’s Predictor: Modeling Earthquake Damage on DrivenData.   Next to Kaggle there are many other websites which host highly relevant and competitive data science competitions. DrivenData is one of these websites. The main difference between the renowned Kaggle and DrivenData is probably the topics of the challenges. Wheras Kaggle hosts more commercially driven competitions, DrivenData focuses more on philanthropic topics.   We, data4help, took part in one of their competitions and scored out of around 3000 competitors in the top one percent. This blogpost explains our approach to the problem and our key learnings.  01 Introduction - Problem Description   The project we chose is called Richter’s Predictor: Modeling Earthquake Damage. As the name suggests, the project involves predicting earthquake damages, specifically damage from the Gorkha earthquake which occurred in April 2015 and killed over 9,000 people. It represents the worst natural disaster to strike Nepal since the 1934 Nepal-Bihar earthquake.    Our task in this project to forecast how badly an individual house is damaged, given the information about its location, secondary usage, and the materials used to build the house in the first place. The damage grade of each house is stated as an integer variable between one and three.   02 How to tackle the project - Plan of attack   The key to success in a Kaggle/ DrivenData challenge, just like in a data challenge for a job application, is a solid plan of attack. It is important that this plan is drafted as early as possible, since otherwise the project is likely to become headless and unstructured. This is especially problematic for data challenges for a job application, which generally serve to gauge whether a candidate can draft a solid strategy of the problem and execute it in short amount of time.   Therefore, one of the first things to do is to get a pen and paper and sketch out the problem. Afterwards, the toolkit for the prediction should be evaluated. That means we should investigate what kind of training data we have to solve the problem. A thorough analysis of the features is key for a high performance.   Do we have any missing values in the data? Do we have categorical variables and if so, what level of cardinality to we face? How sparse are the binary variables? Are the float/integer variables highly skewed? How is the location of a house defined? All these questions came up when we went through the data for the first time. It is important that all aspects are noted somewhere at this stage in order to prepare a structured approach.   After noting all the initial questions we have, the next step is to lay out a plan and define the order in which the problem is to be evaluated and solved. It is worth noting here that it is not expected to have a perfect solution for all the problems we can think off right at the beginning, but rather to consider potential problem areas that could arise.  03 Preliminaries &amp; Base model   One of the first steps in any data challenge should be to train a benchmark model. This model should be as simple as possible and only minor feature engineering should be required. The importance of that model is that it gives us an indication of where our journey starts and what a sensible result is.   Given that DrivenData already set a benchmark using a Random Forest model, we will also use that model as a baseline. Before the data can be fed into the model, we have to take care of all categorical variables in the data, through the handy get_dummies command from Pandas. Secondly, we remove the variable building_id which is a randomly assigned variable for each building and hence does not carry any meaning.   train_values.drop(columns=[\"building_id\"], inplace=True) dummy_train = pd.get_dummies(train_values) y_df = train_labels.loc[:, [\"damage_grade\"]] X_df = dummy_train model = model_dict[\"lgt\"] baseline_model = calc_score(model, X_df, y_df)   From the model_dict we then import the basic random forest model. With just these couple of lines of code, we have a baseline model and baseline accuracy of 71.21%. This is now our number to beat!   In the next sections, we show the steps taken to try to improve on this baseline.   04 Skewness of the integer variables   As one of the first steps in feature engineering for improving on this baseline, we will further investigate all float and integer variables of the dataset. To make all the numeric variables easier to access, we stored the names of all variables of each kind in a dictionary called variable_dict.   In order to better understand the variables, we plot all integer variables using the package matplotlib:  int_variables = variable_dict[\"int_variables\"] int_var_df = dummy_train.loc[:, int_variables] fig, axs = plt.subplots(1, 5, figsize=(60, 10)) for number, ax in enumerate(axs.flat):     sns.kdeplot(int_var_df.iloc[:, number], bw=1.5, ax=ax,                 shade=True, cbar=\"GnBu_d\")     ax.tick_params(axis=\"both\", which=\"major\", labelsize=30)     ax.legend(fontsize=30, loc=\"upper right\") path = (r\"{}\\int.png\".format(output_path)) fig.savefig(path, bbox_inches=\"tight\")      As we can see from the graph above, all the plots exhibit an excessive rightward skew. That means that there are a few observations for each variable which are much higher than the rest of the data. Another way to describe this phenomena would be to say that the mean of the distribution is higher than the median.   As a refresher, skewness describes the symmetry of a distribution. A normal distribution has, as a reference, a skewness of zero, given its perfect symmetry. A high (or low) skewness results from having a few obscurely high (or low) observation in the data, which we sometimes also call outliers. The problem with outliers is manifold, but the most important problem for us is that it hurts the performance of nearly every prediction model, since it interferes with the loss function of the model.   One effective measure to dampen the massive disparity between the observations is to apply the natural logarithm. This is allowed since the logarithmic function represents a strictly monotonic transformation, meaning that the order of the data is not changed when log is applied.   Before being able to apply that measure, we have to deal with the zero values (the natural logarithm of zero is not defined). We do that by simply adding one to every observation before applying the logarithm. Lastly we standardize all variables to further improve our model performance.  # Applying the logs and create new sensible column names logged_train = dummy_train.loc[:, int_variables]\\     .apply(lambda x: np.log(x+1)) log_names = [\"log_{}\".format(x) for x in int_variables] stand_logs = StandardScaler().fit_transform(logged_train) stand_logs_df = pd.DataFrame(stand_logs, columns=log_names)  for log_col, int_col in zip(stand_logs_df, int_variables):     dummy_train.loc[:, log_col] = stand_logs_df.loc[:, log_col]     dummy_train.drop(columns=int_col, inplace=True) # Plot the newly created plot log variables fig, axs = plt.subplots(1, 5, figsize=(60, 10)) for number, ax in enumerate(axs.flat):     sns.kdeplot(logged_train.iloc[:, number], bw=1.5, ax=ax,                 shade=True, cbar=\"GnBu_d\")     ax.tick_params(axis=\"both\", which=\"major\", labelsize=30)     ax.legend(fontsize=30, loc=\"upper right\") path = (r\"{}\\logs_int.png\".format(output_path)) fig.savefig(path, bbox_inches='tight')  The graph below shows the result of these operations. All distributions look much less skewed and do not exhibit the unwanted obscurely high values which we had before.      Before moving on, it is important for us to validate that our step taken had a positive effect on the overall performance of the model. We do that by quickly running the new data in our baseline random forest model. Our accuracy is now 73.14, which represents a slight improvement from our baseline model!   Our performance has increased. That tells us that we took a step in the right direction.  05 Geo Variables - Empirical Bayes Mean Encoding   Arguably the most important set of variables within this challenge is the information on where the house is located. This makes sense intuitively: if the house is located closer to the epicenter, than we would also expect a higher damage grade.   The set of location variables provided within this challenge are threefold. Namely, we get three different geo-identifier with different kind of granularity. For simplicity, we tended to regard the three different identifier as describing a town, district and street (see below).      These geo-identifiers in their initial state are given in a simple numeric format, as can be seen below.      These integer do not prove to by very useful since even though in a numeric format, they do not exhibit any correlation with the target (see graphic below). Meaning that a higher number of the identifier is not associated with higher or lower damage. This fact makes it difficult for the model to learn from this variable.      In order to create a more meaningful variable for the model to learn from these variables, we apply a powerful tool, oftentimes used in data science challenges, called encoding. Encoding is normally used when transforming categorical variables into a numeric format. On first glance we might think that this does not apply to our case, since the geo-identifier is given as a numeric variable. However this understanding of encoders is shortsighted, since whether something represents a categorical variable does not depend on the format, but on the interpretation of the variable. Hence, the variable could gain greatly in importance when undergoing a transformation!   There are a dozen different encoding methods, which are nicely summarized in this blogpost. The most promising method for our case would be something called target encoding. Target encoding replaces the categorical feature with the average target variable of this group.   Unfortunately, it is not that easy. This method may work fine for the first geo-identifier (town), but has some serious drawbacks for the more granular second and third geo-identifier (district and street). The reason is that there are multiple districts and streets which only occur in a very small frequency. In these cases, mean target variable of a group with a small sample size is not representative for the target distribution of the group as a whole and would therefore suffer from high variance as well as high bias. This problem is quite common when dealing with categorical variables with a high cardinality.   One workaround for this problem is a mixture between Empirical Bayes and the shrinkage methodology, motivated by paper [1]. Here, the mean of a subgroup is the weighted average of the mean target variable of the subgroup and the mean of the prior.      In our example that would mean that the encoded value for a certain street is the weighted average between the mean target variable of the observations of this street and the mean of the district this street is in. (one varaiable level higher). This method shrinks the importance of the potentially few observations for one street and takes the bigger picture into account, thereby reducing the overfitting problem shown before when we had only a couple of observations for a given street.   The question may now arise how we are determining the weighting factor lambda. Using the methodology of the paper in [1], lambda is defined as:      Where m is defined as the ratio of the variance within the group (street) divided by the variance of the main group (district). That formula makes intuitive sense when we consider a street with a few observations which differ massively in their damage grade. The mean damage grade of this street would therefore suffer from high bias and variance (high sigma). If this street is in a low variance district (low tau), it would be sensible to drag the mean of the street into the direction of the district. This is essentially what the m coefficient captures.      It is worth mentioning that the overall model performance in-sample will drop when applying the Empirical Bayes-shrinkage method compared to using a normal target encoder. This is not surprising since we were dealing with an overfitted model before.   Lastly, we run our model again in order to see whether our actions improved the overall model performance. The resulting F1 score of 76.01% tells us that our changes results in an overall improvement.   06 Feature selection   At this point, it is fair to ask ourselves whether we need all the variables we currently use in our prediction model. If possible, we would like to work with as few features as possible (parsimonious property) without losing out too much in our scoring variable.   One benefit of working with tree models is ability to display feature importance. This metrics indicates how important each feature is for our prediction making. The following code and graph displays the variables nicely.  fimportance = main_rmc_model[\"model\"].feature_importances_ fimportance_df = pd.DataFrame() fimportance_df.loc[:, \"f_imp\"] = fimportance fimportance_df.loc[:, \"col\"] = dummy_train.columns fimportance_df.sort_values(by=\"f_imp\", ascending=False, inplace=True) fig, ax = plt.subplots(1, 1, figsize=(12, 24)) ax = sns.barplot(x=\"f_imp\", y=\"col\",                 data=fimportance_df,                 palette=\"GnBu_d\") ax.tick_params(axis=\"both\", which=\"major\", labelsize=20) ax.set_xlabel(\"Feature Importance in %\", fontsize=24) ax.set_ylabel(\"Features\", fontsize=24) path = (r\"{}\\feature_importance.png\".format(output_path)) fig.savefig(path, bbox_inches='tight')      As we can see from the graph above, the most important variables to predict the damage grade of a house is the average damage grade of the different geo-locations. This makes sense, since the level of destruction of one house is likely to be correlated with the average damage of the houses around.   06.1 Low importance of binary variables  The feature importance also shows that nearly all binary variables have a low feature importance, meaning they are providing the model with little to no predictive information. In order to understand that better we take a look into the average of all binary variables, which is a number between zero and one.   binary_variables = variable_dict[\"binary_variables\"] mean_binary = pd.DataFrame(dummy_train.loc[:, binary_variables].mean()) mean_binary.loc[:, \"type\"] = mean_binary.index fig, ax = plt.subplots(1, 1, figsize=(12, 12)) ax = sns.barplot(x=\"type\", y=0,                 data=mean_binary,                 palette=\"GnBu_d\") ax.tick_params(axis=\"both\", which=\"major\", labelsize=16) ax.set_xticklabels(mean_binary.loc[:, \"type\"], rotation=90) path = (r\"{}\\binaries_mean.png\".format(output_path)) fig.savefig(path, bbox_inches='tight')      As can be seen above, nearly all variables have a mean below ten percent. That implies that most rows are equal to zero, a phenomenon we normally describe as sparsity. Furthermore, it is visible that the binary variables with an average above ten percent have also a higher feature importance within our prediction model.   This finding is in line with the fact that tree models, and especially bagging models like the currently used Random Forest, do not work well with sparse data. Furthermore, it can be said that a binary variable which is nearly always zero (e.g. has_secondary_usage_school), simply does not carry that much meaning given the low correlation with the target.   Using cross-validation, we find that keeping features which have an importance of minimum 0.01%, leaves us with the same F1 score compared to using all features. This leaves us with 53 variables in total. This number, relative to the amount of rows we have (260k) seems reasonable and therefore appropriate for the task.   07 Imbalance of damage grades  One of our key learnings in this challenge was how to handle the massive imbalance of the target variable. Namely, not to touch it at all!   When looking at the chart below, we can see that the first damage grade does not nearly appear as often as the second damage grade. It may be tempting now to apply some over- or undersampling to the data in order to better balance the data and to show the model an equal amount of each damage grade. The main problem with this approach is that the test data comes from the same (imbalanced) distribution as the training data, meaning that improving the accuracy score for the lowest damage grade, through sampling methods, comes with the costs of a lower accuracy of the highest occurring, and therefore more important damage grade two.      07.1 Performance &amp; Concluding Remarks   Following all steps of this blogpost (with a few minor tweaks and hyperparameter tuning) led us to place 34 out of 2861 competitors.      We are overall quite happy with the placement, given the amount of work we put in. This challenge touched on many different aspects and taught us a lot. Data Science Challenges are a perfect learning opportunity since they are very close to real life problems.   We are looking forward to our next one!  References  [1] Micci-Barreca, Daniele. (2001). A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.. SIGKDD Explorations. 3. 27–32. 10.1145/507533.507538.  ","categories": ["Data Challenge","Python"],
        "tags": [],
        "url": "/data%20challenge/python/Richter's-Predictor-Data-Challenge-from-DrivenData/",
        "teaser": null
      },{
        "title": "Using Python to do Your Homework",
        "excerpt":"Applying OpenCV and Tesseract to do your math-homework   The possibilities to use Python are almost endless - repetitive tasks especially can be solved easily using Python. Here we show how Python can be used to automatically answer problems on a math worksheet.   First we take a look at the math questions:      Nothing too difficult, but the amount of questions could make it very tiring to solve and fill in every single one. Instead, let us try it in Python!   We start by importing the relevant packages. In fact, we need exactly three packages. The first package enables us to read the questions, meaning it transforms image to text. The package we are talking about is called pytesseract. It is important to note that a bit more work than only pip install … is required to get it running. Here is a link to a good tutorial regarding this problem.   The second package is necessary for finding where exactly the solution should be written. This means we have to tell the machine that the answer of every equation should be written in the black squares next to the equation. In order to find and identify these squares, OpenCV is needed.   Last but not least we import a package which is able to handle strings or regular expression operations, called “re” for short.  import pytesseract as tess path = (r\"C:\\Users\\PaulM\\AppData\\Local\\Tesseract-OCR\\tesseract.exe\") tess.pytesseract.tesseract_cmd = path import cv2 import re   01 Reading the questions   We start by importing the picture and apply the image_to_string function from tesseract   png = \"{}\\questions.png\".format(raw_path) text = tess.image_to_string(png)  Looking at the results below, it seems at first glance that everything worked succesfully. Our result is one big string where each equation is delimited by a line break, which is denoted as a \\n symbol.    However, a bit more cleaning is still necessary before doing the calculations. First we have to remove all spaces between numbers and then figure out which part of this string represent actual questions. This is done with the following three lines of code:   text.replace(\" \", \"\") pattern = re.compile(\"[0-9]+x[0-9]+\") equations = [x for x in parsed_text if bool(re.match(pattern, x))]      The last line of the code above is filtering the long string shown above and only extracts a certain string pattern. Specifically, it extracts an undefined amount of numbers (denoted as [0–9]+) then the letter x and then again an undefined amount of numbers. The result of that code is a list which contains all equations.   The last step is probably the easiest, namely to calculate the solutions of all the equations. For this we build a small function, which is then used within a list comprehension to solve the equations.   def multiplication (equation):     split_equation = equation.split(\"x\")     num1 = int(split_equation[0])     num2 = int(split_equation[1])     return str(num1 * num2)  solutions = [multiplication(x) for x in equations]      The result of this function is the solutions of all questions. In total we end up with a list with the length of 40, which is the exact number of questions on the sheet.   02 Processing the image   The next step is now to fill in the solutions back on the questionnaire. This sounds easier then it actually is. To fill in the answers on the sheet, we have to find a specific location on the png where we want the solution to be written. In our example, we would like to find the coordinates of the black answer box which is next to every equation.   We start by reading in the image using OpenCV. Next, we transform the picture into a gray-scale format. This is done in order to compress information. Since we would like to identify a certain shape on an image, colors are not important to us and we can move from a tensor to a matrix.   raw_img = cv2.imread(png) img = cv2.imread(png, cv2.IMREAD_GRAYSCALE)   Let’s take a look on how the output of the gray-scale looks like. The Gif below nicely shows that we now have a large matrix containing all the pixels of the picture. We can see that most of the picture is covered with white pixels (white encodes to the integer 255). Furthermore, we can even read the equation, the equal sign, as well as the answer box ny looking at where the pixel number and color changes.      The pixels representing the answer box are of particular interest to us, since we would like the answer to be placed within it. Before continuing to identify the box, some pre-processing is necessary - namely to enhance the contrast between the box and the white background for better identification of the shapes, a process called thresholding. An example which exemplifies the need to do that, is shown below. On the left side we have the number three shown from the initial image, whereas on the right side we have the same number after applying the thresholding.   As can be seen below, a written number is not entirely black. Especially on the sides the strength of the ink fades out. In order to make it easier for the computer to identify clear shapes, like a square for example, we turn every pixel below a certain threshold black and the rest white.      img = cv2.imread(png, cv2.IMREAD_GRAYSCALE) _, threshold = cv2.threshold(img, 170, 255, cv2.THRESH_BINARY)   The code above shows how this step was implemented. The first line reads in the png we imported at the beginning and directly transforms it into a gray-scaled picture. The second line then applies the thresholding to the gray-scaled image. This is done by specifying the image, the threshold value (in our case 170, which is obtained by trial and error), the maximum value (in our case we would like the pixels to turn white if the exceed the threshold), and the way OpenCV should apply the thresholding. Binary thresholding means that there will be a clear cut - every pixel with a value below the thresholding will be set to zero, every pixel above the threshold will be set to the maximum value, in our case 255.   The next step is then to identify the squares within our image. This is done by the handy function called findContours   contours, _ = cv2.findContours(threshold, cv2.RETR_TREE,                                cv2.CHAIN_APPROX_SIMPLE)   We see that the function takes three inputs (it thas more arguments than that, but these three are relevant for our problem). The first input represents our thresholded image. The second input is not of great importance to us, since it states which kind of hierarchy should be used when storing the contours. The third output defines how a shape should be saved.   The image below shows this last point visually. Even though we have two white squares, there are two different in which we could save the relevant information needed to replicate these squares: we could either save every single pixel, as it is done in the left picture, or we save only the corner coordinates. Needless to say, the right one would use significantly less memory. Exactly this method is specified when the cv2.CHAIN_APPROX_SIMPLE in the command above was called.      03 Inserting the solutions   After storing all the information of all kind of shapes from the picture, we would like to restrict the shapes we are looking for to the squares. As outlined above, we stored the information of every shape by storing the coordinates of the corner points of every contour. Since we are interested in squares, only contours which have exactly four corner points are relevant for our problem.   rectangles = [x for x in contours[1:] if (len(x)==4)]      One not very intuitive feature of the OpenCV function findCountours is that it detects contours from right to left and from bottom to top. This created a bit of a problem, given that we our solutions are stored in a different way, namely from top to bottom and left to right. In order to align these two lists, we alter the rectangles list we created in the code above through the following code   right_side = list(reversed(rectangles[0::2])) left_side = list(reversed(rectangles[1::2])) sorted_list = left_side + right_side   Now the solutions as well as the rectangle information are both in same order. Last but not least we then have to write the solutions into the rectangle. This is done by extracting the bottom left x and y coordinate, which is shown in the image below as the red circle.      The actual writing of the solution for each question is done by a function called putText. The input of the function are relatively straight forward. Namely, we insert the image as well as some coordinates and a font.   font = cv2.FONT_HERSHEY_COMPLEX for i, j in zip(solutions, sorted_list):     x = j[1][0][0]     y = j[1][0][1]     cv2.putText(img, i, (x, y), font, 0.7, (0)) cv2.imshow(\"Threshold\", img) cv2.waitKey(0) cv2.destroyAllWindows()   Finally we then can display our results, which look very promising. It might be that solving these question by hand instead of using Python could have been quicker, but it would have been considerably less fun!      ","categories": ["Image Recognition","Python"],
        "tags": [],
        "url": "/image%20recognition/python/Using-Python-to-do-Your-Homework/",
        "teaser": null
      },{
        "title": "Signal Processing - Engine Sound Detection",
        "excerpt":"Detecting the engine-brand from a Formula 1 car by its sound using Neural Networks   Formula 1 is not everybody’s cup of tea. Whereas some celebrate risky takeover maneuvers, others only see dangerous driving in spaceship-looking vehicles. One particularly divisive aspect of the sport is the incredibly loud and distinct noise made by the cars. Even though the FIA banned the infamously loud V12 in the year 2000, Formula 1 cars are not known to go by unnoticed.   For many opponents of the sport all engines sound the same. But do they really? This project tries to predict the engine-brand using sound data.   Overview   This blogpost elaborates on how to build a sound-recognition model from scratch using a Neural Network. Given that we need to process audio data before the Neural Network can be trained on it, several sound concepts and terminology are introduced to give the reader a better understanding.   The post starts by elaborating on where from and how the data was collected. Afterwards, we cover some audio-related terminology before diving into the practical implementation of signal processing in Python.   After the bulk-processing of the audio data is conducted, we focus our attention onto the implementation of the neural network using tensorflow. As the last step we then implement our prediction onto some test-data and create a montage video, which is also used as the title image of this blogpost.   Data Collection   In order to build a model that predicts which sound belongs to which engine, we need labelled sound-data. Labelling means we know which engine-brand produced which sound.   Obtaining Formula 1 content proved to be relatively easy through their official Youtube Channel. Here we find a compilation of videos labelled Pole Laps. Before each race in Formula 1, as in every other motorsport, every driver undergoes a procedure called Qualifying where each driver tries to complete the track in the shortest possible time. The person who wins this competition gets rewarded with the Pole Position. This term, which originates from horse racing, describes the most favorable position a driver can think off for the actual race - namely starting from the very front.   Given that the lap which sets the quickest time, the so called Pole Lap, is interesting to watch for many, the official Formula 1 channel uploaded an on-board video taken from the care from this lap for every race. Below we find a compilation of short snippets from these videos.   We use these qualifying lap videos as they contain only one car, making the sounds data pure. In the actual race where many cars are driving, it would be much more difficult to identify and separate out the individual sounds of each engine.      Next to be very entertaining to watch, these videos are particularly useful for our task. That is because the data is labelled (we know which engine-brand is driving in which video) and the data is relatively free from any noise.   Furthermore, in the racing year 2019 two engine-brands were taking turns in winning the Qualifying - Ferrari and Mercedes. To be exact, out of 21 Qualifying, Mercedes and Ferrari won 10 and 9 of these, respectively.   Since we are solely interested in the sound of these videos we extract the sound information from these 19 videos and save them in a wav-format.      Sound Data - First Steps   The processing of sound data might be a bit unfamiliar for some, therefore the following section explains the fundamentals of sound data.   We’ll start by answering the first fundamental question - what is sound? Sound could be described as simply the vibration of an object. The vibrations then cause the oscillation of air molecules. We can hear this sound as soon as these vibrations hit our ear.   Sound is a continuous variable, meaning that we have an infinite amount of information, or data points, per second. This actually represents a bit of a problem when trying to store that information - we have to turn this continuous signal into a discrete one. That process is called Analog to digital conversion (ADC).   ADC does two things: first, it samples the sound information, and second, it quantifies the information at each sampling point. The sampling frequency is measured in hertz and describes how often we would like to store sound information per second. The more samples we take within a second, the smaller the overall loss of information. That does not mean though that a higher sampling rate is always better though. A classical CD has around 22,050 hertz. The reason for this seemingly arbitrary number is the hearing range for humans, which is around 20,000 hertz. Hence, for humans the sampling rate of a CD is more than sufficient to appreciate the sound.   After knowing how often we would like to sample per second we should talk about what exactly we store. Here the term quantization comes into play. Quantization assigns a value to each sound signal we extract. Given that sound information has a continuous value and we have to store that information in a discrete way for a computer to store it. For example, if the signal has a value of 2.234253243… (meaning a never ending number), we will round to e.g. 2.23. This, of course, leads to some sort of a rounding error. How high this error is depends on the resolution of the sound. This resolution is measured in bits (also referred to as bit depth). A CD has a bit depth of 16, which is sufficiently high for the human ear.   Wave-plots and related Terminology  After covering how sound-information is stored, it is now time to talk about how the actual processing of sound-information works. We start by introducing the probably most common sound-visualization - the wave-plot.      A wave-plot displays the sound-signal as a function of time. Technically put, a wave-plot shows the deviation of the zero-line in air pressure. A wave-form, even though looking relatively simple, carries a lot of further relevant information like frequency, amplitude, intensity and timbre of the sound. These attributes shape how exactly the wave-plot looks.   These additional attributes in the wave-plot are extremely important for understanding sound, so we’ll look at them more closely and explain their meaning. Frequency is the inverse of the amount of time that elapses between two peaks of a soundwave, called periods. The graph below illustrates that concept, where f denotes the frequency and T the time of a period. Overall, a higher frequency leads to a higher-pitched sound, whereas a lower frequency leads to a lower/ deeper sound. Unlike the graphic below, most sounds in real-life are not that nice and easy. Most of the time we face a so called aperiodic sound, which consists out of many different frequencies laying on top of each other.      Amplitude, on the other hand, quantifies the perturbation of the air pressure. A higher perturbation signifies more energy being transmitted through the soundwave. This results in a louder sound. A low amplitude results then, consequently, in a more quite sound.      Intensity is formally measured in sound power per unit area. Given the incredibly large range of sounds that a human ear is able to hear, this variable uses a logarithmic scale. Furthermore, it is important to know that this variable is measured in decibels. Below we find a table which gives us some sort of indication about the magnitude of decibels. Our Formula 1 car has an intensity of 140 decibels and is therefore relatively close to the threshold of permanent hearing damage.      The last concept of importance is called timbre. This variable is probably the most abstract concept we encountered so far. It is normally described as the color of the sound. It describes the difference in sound holding frequency, intensity and duration constant. The importance of the variable becomes clear when trying to distinguish a trumpet and a violin which play the exact same sound. It is exactly this variable which will play a crucial role within our task of identifying the engine-brand.   After covering the most important concepts of sound, it is now time to take a look at our data by building wave-plots. We start by building one sound-wave for each engine-brand, meaning one for Ferrari and one for Mercedes.   We start by importing all the relevant packages and defining the paths on our computer. Furthermore, we build a dictionary which specifies the location of the two audio files and the color we would like to draw them in.   # Packages import librosa, librosa.display import numpy as np import matplotlib.pyplot as plt import os # Paths main_path = r\"/Users/paulmora/Documents/projects/formula\" raw_path = r\"{}/00 Raw\".format(main_path) code_path = r\"{}/01 Code\".format(main_path) data_path = r\"{}/02 Data\".format(main_path) output_path = r\"{}/03 Output\".format(main_path)  # Specifying plotting information dict_examples = {     \"ferrari\": {         \"file\": r\"{}/ferrari/{}\".format(raw_path, \"f_austria.wav\"),         \"color\": \"red\"     },     \"mercedes\": {         \"file\": r\"{}/mercedes/{}\".format(raw_path, \"m_australia.wav\"),         \"color\": \"silver\"     } }   When importing the sound file we have to specify the sample rate. As discussed in the beginning of the blog-post, a Sample Rate of 22,050 hertz is more than sufficient.   The go-to sound processing package in Python is called librosa. This package has a handy command for calculating and plotting the wave-plot.   fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20, 10)) SR = 22_050 axs = axs.ravel() for num, team in enumerate(dict_examples):     signal, sr = librosa.load(dict_examples[team][\"file\"], sr=SR)     dict_examples[team][\"signal\"] = signal     librosa.display.waveplot(signal, sr=sr, ax=axs[num],                              color=dict_examples[team][\"color\"])     axs[num].set_title(team, {\"fontsize\":18})     axs[num].tick_params(axis=\"both\", labelsize=16)     axs[num].set_ylabel(\"Amplitude\", fontsize=18)     axs[num].set_xlabel(\"Time\", fontsize=18) fig.savefig(\"{}/waveplot.png\".format(output_path),             bbox_inches=\"tight\")      Listening to one of the videos, one quickly notices that the engine is as its loudest when driven full-speed. This, probably not very noble finding, explains the aperiodic behavior of the wave-plots above. It is likely that every systematical temporary drop in amplitude is due to a time when the car slowed down a.k.a when facing a corner.   Looking at the two plots above it looks like the Ferrari engine is significantly louder than the Mercedes engine. It would be important to check whether that is a systematic finding, or whether a specific one. For that reason we plotted the wave-plots for all races.      The plot above shows us that identifying which engine-brand a sound belongs is not as easy as just looking for the higher amplitude/ louder sound. The plot tells us that whether a sound is relatively loud or quite is somewhat random and not specific to an individual engine.   The pondering question now is what we can learn from the wave-plots we saw before. Even though a wave-plot captures a lot of information, most of it is not entirely visible to us at that point. As the next step we will take a deeper look into the different frequencies of our sound. This is done by using a so-called Fourier Transform.   Fourier Transform  A Fourier Transform describes the method of decomposing complex sounds into a sum of sine waves, oscillating at different frequencies. To understand what that means, it is important to acknowledge that every complex sound (e.g. not a simple sine wave) consists out of a sum of many smaller sounds.      The Fourier Transform is particularly interested in the amplitude of each frequency. That is because the amplitude tells us the magnitude by how much a specific frequency contributes to the overall complex sound.   A good comparison to understand that context better would be the creation of a cooking sauce. A sauce has a certain taste which is driven by its different ingredients. In the beginning it might seem very difficult to tell which and by how much each spice is contributing to the overall taste. A Fourier Transform tells us exactly that. It splits up the sauce into the different components and tells us how much of which spice was used to create the overall sauce and taste.   Coming back to our Formula 1 car-sound example. When applying a Fourier Transform on our sound information from before, we end up with the following two, so called, power-spectrum. The code below shows how to create the power-spectrums for our problem:   fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20, 10)) axs = axs.ravel() for num, team in enumerate(dict_examples):     # Calculating the fourier transform     signal = dict_examples[team][\"signal\"]     fft = np.fft.fft(signal)     magnitude = np.abs(fft)     frequency = np.linspace(0, sr, len(magnitude))     left_frequency = frequency[:int(len(frequency)/2)]     left_magnitude = magnitude[:int(len(frequency)/2)] # Plotting results     axs[num].plot(left_frequency, left_magnitude)     axs[num].set_title(team, {\"fontsize\":18})     axs[num].tick_params(axis=\"both\", labelsize=16)     axs[num].set_ylabel(\"Magnitude\", fontsize=18)     axs[num].set_xlabel(\"Frequency\", fontsize=18)     axs[num].plot(left_frequency, left_magnitude,                   color=dict_examples[team][\"color\"]) fig.savefig(\"{}/powerspectrum.png\".format(output_path),             bbox_inches=\"tight\")      The plots above show us the Magnitude as a function of Frequency. We can see that the main sound for a F1 car is to be found in the lower levels of frequency.   A power-spectrum has one central weakness. Namely, the loss of the time domain. A power-spectrum shows us which frequency drives the overall sound, for the entire soundtrack. That static behavior, of course, a problem since music is a time series and changes over time are crucial.   The solution to that problem is something called a Short Time Fourier Transform which is explained in the next section.   Short Time Fourier Transform   In order to fight the static behavior of the power-spectrum, a dynamic version is created, namely a Short Time Fourier Transform (STFT). A STFT computes several Fourier Transforms at different intervals. This has the benefit that the time information is persevered without losing any of the benefits of a regular Fourier Transform.   To apply a Short Fourier Transform in Python we need to specify several variables, namely the n_fft and the hop_length. The n_fft denotes the number of samples for number of samples we use to calculate each Fourier Transform. It can be regarded as the size of the moving window over time. The hop_length, on the other hand, denotes how much we shift to the right after conducting one Fourier Transform. The common values for these variables are 2048 and 512 samples, respectively.   A STFT results in something called a spectogram, which is an important concept within the realm of signal processing, as it compromises information about time (x-axis), frequency (y-axis) and magnitude of sound (color). The following code shows how to implement such a STFT for our example.   n_fft = 2048  # Window for single fourier transform hop_length = 512  # Amount for shifting to the right fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True,                         figsize=(20, 10)) axs = axs.ravel() for num, team in enumerate(dict_examples):     signal = dict_examples[team][\"signal\"]     stft = librosa.core.stft(signal, hop_length=hop_length,                               n_fft=n_fft)     spectogram = np.abs(stft)     log_spectogram = librosa.amplitude_to_db(spectogram)     plot = librosa.display.specshow(log_spectogram, sr=sr,                                     hop_length=hop_length,                                      ax=axs[num])     axs[num].tick_params(axis=\"both\", labelsize=16)     axs[num].set_title(team, {\"fontsize\":18})     axs[num].set_ylabel(\"Frequency\", fontsize=18)     axs[num].set_xlabel(\"Time\", fontsize=18) cb = fig.colorbar(plot) cb.ax.tick_params(labelsize=16) fig.savefig(r\"{}/short_fourier.png\".format(output_path),             bbox_inches=\"tight\")      Looking at the chart above, the benefit of the spectogram over the power-spectrum becomes apparent. Whereas in the power-spectrum we were only able to tell that the sound-piece contains strong low-frequency sounds, the spectogram can also tell us now when in time the frequencies occur.   For example it becomes apparent that the Mercedes Engine exhibits overall somewhat more intense higher frequencies compared to the Ferrari engine. That could be potentially helpful for the model to depict the differences.   The information we gathered so far provided us with a lot of insights. However, so far we do not have anything to feed into a prediction model. The next step will take care of that problem. For that we introduce the concept of MFCCs.   MFCCs   Arguably even more important than the spectogram is the concept of the Mel Frequency Cepstral Coefficients (MFCCs). The reason for their importance is that the resulting coefficients are going to be the input values for our Deep Learning model. Therefore, it is important to gain some intuition what these factors are and what they represent.   The usefulness of MFCCs lies in their ability to capture timbral and textural aspects of sound. Timbre, as explained earlier in this blogpost, captures the color of the sound. Meaning the difference of sound which is not due to frequency, pitch or even amplitude. Next to that, MFCCs are, in contrast to spectrograms, able to approximate the human auditory system.   When calculating MFCCs we get a vector of coefficients for each time frame. The number of coefficients, as well as the time frame has to be specified in the beginning. Normally, the number of coefficients for sound-classification lies between 13–40, where we chose 13.   The code below shows how to implement the extraction of these MFCCs for our Formula 1 example.   fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20, 10)) axs = axs.ravel() for num, team in enumerate(dict_examples):     signal = dict_examples[team][\"signal\"] MFCCs = librosa.feature.mfcc(signal, n_fft=n_fft, hop_length=hop_length,                                  n_mfcc=13)     plot = librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length,                                     ax=axs[num])     axs[num].tick_params(axis=\"both\", labelsize=16)     axs[num].set_title(team, {\"fontsize\":18})     axs[num].set_ylabel(\"Time\", fontsize=18)     axs[num].set_xlabel(\"Frequency\", fontsize=18) cb = fig.colorbar(plot) cb.ax.tick_params(labelsize=16) fig.savefig(r\"{}/mfccs.png\".format(output_path),             bbox_inches=\"tight\")      The plots above visualize the extracted MFCCs from the both sound-pieces. These extracted sound-pieces are now ready to be fed into a neural network. Before doing that though, we have to bulk-process all tracks instead of only these two. The following section elaborates on the implementation of this process.   Data Processing in bulk   When completing the final processing of all sound pieces to be fed into the neural network, it is important to notice that we do not have to go through all steps outlined above. Since the input features for our Deep Learning model is going to be a list of MFCCs, we can directly jump to this step. In addition to the necessary packages from before, we have to import several other packages for the following steps.   # Bulk processing packages from pydub import AudioSegment from pydub.utils import make_chunks import math import re from tqdm import tqdm import json import copy   Before being able to process every race-track we have in our data, we should step back and ask ourselves a quite fundamental question in the realm of Neural Networks. Do we have enough data? Currently we have 19 sound-pieces, ten tracks with a mercedes engine and nine tracks with a ferrari engine. It is pretty clear to say that 19 observations are not going to be sufficient for training purposes.   There is luckily one work-around. Namely, it is possible to chop up every audio-track into one-second long snippets of sounds. The implications of that are tremendous. An audio-piece which has a length of around e.g. 80 seconds would then result into 80 pieces. Doing that for all tracks in our dataset, results in around 1000 audio snippets for Ferrari as well as for Mercedes. The following code shows how we split the longer audio-pieces into one-second-snippets.   raw_files = {     \"ferrari\": r\"{}/ferrari\".format(raw_path),     \"mercedes\": r\"{}/mercedes\".format(raw_path) } for team in [\"ferrari\", \"mercedes\", \"montage\"]:     wav_files = os.listdir(\"{}/{}\".format(raw_path, team))     for file in wav_files:         if not file.startswith(\".\"):             file_name = \"{}/{}/{}\".format(raw_path, team, file)             myaudio = AudioSegment.from_file(file_name, \"wav\")             chunk_length_ms = 1000             chunks = make_chunks(myaudio, chunk_length_ms)             for i, chunk in enumerate(chunks):                 padding = 3 - len(str(i))                 number = padding*\"0\" + str(i)                 chunk_name = \"{}_{}\".format(re.split(\".wav\", file)[0], number)                 chunk.export(\"{}/{}/{}.wav\".format(data_path, team,                                                     chunk_name), format=\"wav\")   The resulting files of this code can be seen below. Here we can see the first twelve one-second-snippets of a Ferrari sound. From the name we can also infer where the sound was recorded: on the austrian race-track.      Next up is now the mass-extraction of the MFCCs for each of the different one-second-snippets. In order to avoid data leakage, we put aside two audio-pieces for both engines to have a clean test-dataset for later performance assessment.   Furthermore, for visual purposes we build a montage video which compromises several different snippets in order to better present our model performance. The following code shows the bulk processing for all three audio segments.   data = {      \"train\": {\"mfcc\": [], \"labels\": [], \"category\": []},      \"test\": {\"mfcc\": [], \"labels\": [], \"category\": []},      \"montage\": {\"mfcc\": []} } test_tracks = [\"f_austria\", \"m_australia\"] SAMPLE_RATE = 22050 n_mfcc = 13 n_fft = 2048 hop_length = 512 expected_num_mfcc_vectors = math.ceil(SAMPLE_RATE / hop_length) for i, (dirpath, dirnames, filenames) in enumerate(os.walk(data_path)): # ensure that we are not at the root level     if dirpath is not data_path: # save the team information         dirpath_components = dirpath.split(\"/\")         label = dirpath_components[-1] # looping over the wav files         for f in tqdm(filenames):             if not f.startswith(\".\"):                 file_path = os.path.join(dirpath, f)                 signal, sr = librosa.load(file_path, sr=SAMPLE_RATE) # extract the mfcc from the sound snippet                 mfcc = librosa.feature.mfcc(signal, sr=sr,                                              n_fft=n_fft,                                             n_mfcc=n_mfcc,                                             hop_length=hop_length)                 mfcc = mfcc.T.tolist()                 # to ensure that all snippets have the same length                 if len(mfcc) == expected_num_mfcc_vectors: if any([track in f for track in test_tracks]):                         data[\"test\"][\"mfcc\"].append(mfcc)                         data[\"test\"][\"labels\"].append(i-1)                         data[\"test\"][\"category\"].append(label)                     elif (\"montage\" in f):                         print(f)                         data[\"montage\"][\"mfcc\"].append(mfcc)                     else:                         data[\"train\"][\"mfcc\"].append(mfcc)                         data[\"train\"][\"labels\"].append(i-1)                         data[\"train\"][\"category\"].append(label) # saving json with the results with open(\"{}/processed_data.json\".format(data_path), \"w\") as fp:     json.dump(data, fp, indent=4)   Lets take a look on the processed data. Here we can see from left to right the captured MFCCs, the label which indicates whether it is a Ferrari or a Mercedes, and the category which leaves no doubt what is meant with a label of zero.      The MFCC values we see on the left side on the picture above are extracted from one Fourier Transform. As mentioned before, we use a frame size of 2048 to conduct the FFT, afterwards we shift 512 samples to the right and conduct the next FFT. Given that we have 22050 samples per second, we end up with 44 Fourier Transforms, and 44 vectors containing 13 extracted coefficients each. The image below shows an excerpt of the resulting matrix.      Neural Network   Finally it is time now to build our Neural Network. We start by loading in the data we saved earlier, and split test and training data using sklearn train_test_split command. Here we assign 20% to the test data before turning to the model architecture.   As can be seen from the image above, every one-second audio snippet consists of a matrix. Since we cannot really feed in a matrix in its current form into a neural network we define our first layer as a flattening layer. This is not anything other than bringing the data into a 1-D format. Given that we have 13 coefficients and 44 extraction cycles, our input layer will have a length of 13 x 44 = 572   Afterwards, we define three hidden layers with 512, 256 and 64 neurons, respectively. We also specify a 30% dropout layer in order to prevent overfitting. Since the output is binary we specify 1 as the output dimensionality. When compiling the network we use the compiler Adam and a learning rate of 0.0001. Furthermore, given that we have a binary problem (an engine either belongs to Ferrari or Mercedes) we use binary_crossentropy as our loss function.   # Importing packages from sklearn.model_selection import train_test_split import tensorflow.keras as keras from sklearn.metrics import accuracy_score # load and convert data with open(\"{}/processed_data.json\".format(data_path), \"r\") as fp:     data = json.load(fp) inputs = np.array(data[\"train\"][\"mfcc\"]) targets = np.array(data[\"train\"][\"labels\"]) # turn data into train and testset (inputs_train, inputs_test,  target_train, target_test) = train_test_split(inputs, targets,                                                test_size=0.2) # build the network architecture model = keras.Sequential([     # input layer     keras.layers.Flatten(input_shape=(inputs.shape[1],                                       inputs.shape[2])), # 1st hidden layer     keras.layers.Dense(512, activation=\"relu\"),     keras.layers.Dropout(0.3), # 2nd hidden layer     keras.layers.Dense(256, activation=\"relu\"),     keras.layers.Dropout(0.3), # 3rd hidden layer     keras.layers.Dense(64, activation=\"relu\"),     keras.layers.Dropout(0.3), # output layer     keras.layers.Dense(1, activation=\"sigmoid\") ]) # compiling the network optimizer = keras.optimizers.Adam(learning_rate=0.000_1) model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",               metrics=[\"accuracy\"]) model.summary() # train the network history = model.fit(inputs_train, target_train,                     validation_data=(inputs_test, target_test),                     epochs=100,                     batch_size=32)   After storing our training history it is now time to look at our results over all epochs. The following code graphs the accuracy as well as the error over the epochs.   fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10)) # create accuracy subplot axs[0].plot(history.history[\"accuracy\"], label=\"train_accuracy\") axs[0].plot(history.history[\"val_accuracy\"], label=\"test_accuracy\") axs[0].set_ylabel(\"Accuracy\", fontsize=18) axs[0].legend(loc=\"lower right\", prop={\"size\": 16}) axs[0].set_title(\"Accuracy evaluation\", fontsize=20) axs[0].tick_params(axis=\"both\", labelsize=16) # create error subplot axs[1].plot(history.history[\"loss\"], label=\"train error\") axs[1].plot(history.history[\"val_loss\"], label=\"test error\") axs[1].set_ylabel(\"Error\", fontsize=18) axs[1].legend(loc=\"upper right\", prop={\"size\": 16}) axs[1].set_title(\"Error evaluation\", fontsize=20) axs[1].tick_params(axis=\"both\", labelsize=16) fig.savefig(\"{}/accuracy_error.png\".format(output_path),             bbox_inches=\"tight\") plt.show()      The result is pleasantly surprising. The model learns very quickly, and is nicely able to tell which sound belongs to which engine. To really test how well our models work, we have to see how they perform on data they haven’t seen before. We now apply the model on the two sound-pieces which we not used in the building of the neural network.   test_inputs = np.array(data[\"test\"][\"mfcc\"]) test_targets = np.array(data[\"test\"][\"labels\"]) predictions = model.predict_classes(test_inputs) acc = accuracy_score(test_targets, predictions)   We assess the model’s accuracy with the code above. The accuracy is 98.79%, on the never-before-seen data, making it a very accurate predictor of engine sound.   Montage Video   Lastly, as touched on earlier in this post, we build a small montage video which combines the sound-prediction with a visual to make the results come alive. For that we first predict the engine-brand using our priorly trained neural network. Afterwards, we store the results in a list, and replace the numeric label with a string.   montage_inputs = np.array(data[\"montage\"][\"mfcc\"]) predictions = model.predict_classes(montage_inputs) list_pred = [x.tolist()[0] for x in predictions] engine_prediction = [\"Mercedes\" if x == 1 else \"Ferrari\" for x in list_pred]   Using the wonderful package moviepy we import the video and print the position of video (measured in seconds) and our prediction in the middle of the screen. Voila!   import moviepy.editor as mp montage = mp.VideoFileClip(\"{}/video/montage.mp4\".format(raw_path)) clip_list = [] for second, engine in tqdm(enumerate(engine_prediction)):     engine_text = (mp.TextClip(         \"Prediction at Position {}:\\n{}\".format(second, engine),         fontsize=70, color='green',         bg_color=\"black\")         .set_duration(1)         .set_start(0)) clip_list.append(engine_text) final_clip = (mp.concatenate(clip_list, method=\"compose\")               .set_position((\"center\", \"center\"))) montage = mp.CompositeVideoClip([montage, final_clip]) montage.write_videofile(     \"{}/video/montage_with_text.mp4\".format(raw_path),     fps=24, codec=\"mpeg4\")      Sources [1] https://www.youtube.com/watch?v=kmuKQ2JQK30&amp;list=PLfoNZDHitwjUA9aqbPGKw1l4SIz2bACi_ [2] https://www.youtube.com/channel/UCZPFjMe1uRSirmSpznqvJfQ   ","categories": ["Signal Processing","Python"],
        "tags": [],
        "url": "/signal%20processing/python/Signal-Processing-Engine-Sound-Detection/",
        "teaser": null
      },{
        "title": "Empirical Evidence to common Stock Portfolio Questions",
        "excerpt":"An intuitive and replicable explanation to some of the most important questions of portfolio building   At first glance investing seems like rocket science to many people. The sheer amount of possible assets to invest in and the amount of fancy sounding finance terms might seem intimidating. This blog-post elaborates on three of the most asked questions when building a portfolio. This is done by starting with a theoretical explanation and, more importantly, an empirical validation using Python.   We start by loading all the relevant packages and putting the directories for our project in place.   # Packages import pandas as pd import numpy as np import os import matplotlib.pyplot as plt from tqdm import tqdm # Paths main_path = r\"/Users/paulmora/Documents/projects/markowitz\" raw_path = r\"{}/00 Raw\".format(main_path) code_path = r\"{}/01 Code\".format(main_path) data_path = r\"{}/02 Data\".format(main_path) output_path = r\"{}/03 Output\".format(main_path)   Loading asset returns   In order to make the financial theory come alive, we need to create financial return time-series data. In order to work with real financial returns, we use the yahoo finance API. This is done by using the pandas webreader. This powerful package can, among others, download historical price data for different stock tickers. A stock ticker is the abbreviation of a company name used at the stock exchange. For example, the ticker-name of Apple is AAPL. An extensive list of companies and their stock tickers can be found here.   After downloading the aforementioned excel, we upload them into Python with the following lines.   # Loading the tickers ticker_excel = pd.read_excel(\"{}/ticker.xlsx\".format(raw_path)) ticker_list = list(ticker_excel.loc[3:, \"Yahoo Stock Tickers\"])  Now it is time to build a function with which we can create financial returns for a certain number of assets. As the input parameters we select the number of return time-series we would like and for how many years back from today we would like them. The following function allows us exactly that.   def financial_dataframe_creation(ticker_list,                                  number_of_stocks,                                  starting_year):     \"\"\"     This function imports finanial price data from yahoo from a     pre-defined list of stock-tickers. Parameters     ----------     ticker_list : list         list with stock-tickers used by yahoo finance     number_of_stocks : int         number of stocks desired in the final dataframe     starting_year : TYPE         year from which we would like to extract financial data from Returns     -------     returns : dataframe         this dataframe contains daily returns for as many         stocks as were specified in the beginning \"\"\" df_prices = pd.DataFrame()     i = 0     while len(df_prices.columns) != number_of_stocks:         ticker = ticker_list[i]         try:             tmp = web.DataReader(ticker, \"yahoo\",                                  \"1/1/{}\".format(starting_year),                                  dt.date.today())             df_prices.loc[:, ticker] = tmp['Adj Close']         except KeyError:             pass         except RemoteDataError:             pass         i += 1     returns = np.log(df_prices / df_prices.shift(1)) * 252 return returns   Before tackling any question we go for a test-run of the function to see whether it returns the desired output. The following code:   min_year_restriction = 2010 number_of_stocks = 5 returns = financial_dataframe_creation(ticker_list,                                        number_of_stocks,                                        min_year_restriction)   results in this output:      We can see that the different assets are now displayed as columns names and we find one annualized return information for every day of our time series. With the knowledge that our function works, we can start to answer the questions.   “The more stocks in the portfolio the lower the overall risk”   Theory   This statement might seem counter-intuitive at first glance. Having more stocks in your portfolio might seem that there are more things that could go wrong. The reason that this is not the case is related to a concept called diversification.   Imagine that you only have one stock in your portfolio. In that scenario your portfolio is predominately exposed to two kind of risks: Market risks and firm-specific risks.   The firm-specific risks are specific to one individual firm. These risks range from a sudden death of the CEO (assumed to be negative news) up to unexpectedly finding oil on the company’s grounds (assumed to be positive news). Note that if an investor is only carrying one stock in their portfolio, they are heavily exposed to these company-specific risks.   Market risks compromises risk that affect macroeconomic factors like interest rate changes, inflation, unemployment, and so on. If for example the global economy enters a recession and overall fewer goods are sold and bought, the one individual firm in the investor’s portfolio is also going to be affected by it.   When now investing not in one, but in multiple stocks, the company specific risk reduces quite massively given its small contribution to the overall portfolio performance. The market risks on the other hand, given its wide-reaching and all-affecting implications, has an impact on the total portfolio performance nevertheless. Since market risks affect all stocks, they are also referred to as systematic risk. This kind of risk does not reduces when we increase the number of stocks within our portfolio.   Empirical Evidence   In order to validate the financial theory outlined above we create multiple portfolios with an increasing amount of stocks in each portfolio and re-calculate the risk of each.   We start by initializing time series data for 500 stocks, starting in the beginning of 2016 up until today. This is done using the following code   min_year_restriction = 2016 number_of_stocks = 500 returns = financial_dataframe_creation(ticker_list,                                        number_of_stocks,                                        min_year_restriction)   Now we would like to show how the overall portfolio risk reduces when we include more assets in our portfolio. For that we calculate and store the portfolio risk of a given portfolio of the first 25 assets. Afterwards we add further 25 assets to that portfolio and re-calculate the risk again and so on. This is done through the following code:   step_size = 25 risk_list = [] num_stocks_list = [] for num_stocks in tqdm(range(1, number_of_stocks+2, step_size)):     portfolio = returns.iloc[:, :num_stocks].copy()     num_of_companies = len(portfolio.columns)     cov_matrix = portfolio.cov()     sum_covariances = cov_matrix.sum().sum()     weighting = num_of_companies**2     portfolio_variance = sum_covariances / weighting     risk = np.sqrt(portfolio_variance)     risk_list.append(risk)     num_stocks_list.append(num_of_companies)   When plotting the results we get the following picture:      Our hypothesized reduction in overall portfolio risk with an increasing amount of stocks shows up nicely. The black horizontal line drawn in the graph furthermore depicts the level of risk of a portfolio which cannot be further reduced even when including more stocks into the portfolio. That is because all stocks are going to be affected by the volatility of the market as a whole. This minimum threshold of risk is also called undiversifiable risk, whereas we call the risk which is possible to reduce the diversifiable risk.   “The important risk kind of an asset is its covariance with other assets and not its own volatility”   Theory   The answer to the question of how to calculate the risk of an equity portfolio is easier than one might initially think. It is actually only the sum of the portfolio covariance matrix. The covariance matrix shows each asset’s variance on the diagonal and the covariance with each other asset in the upper and lower diagonal matrix. As an example let us consider the covariance matrix below. The variance are denoted by a light-green shaded background whereas the covariance have a light-red shaded background. It is important to note that the upper diagonal matrix and lower diagonal matrix are identical. That is because the covariance of A and B is equal to the covariance of B and A.      It is important to know how the number of variances and covariance scale with the size of the portfolio. A portfolio with n-assets for example has exactly n variances and n²-n covariances. It is easy to see that with a higher number of n the covariances outnumber the variances significantly.   Empirical Evidence   To check whether that is also empirically the case, we implement the following code.   step_size = 10 maximum_number_of_stocks = 100 list_num_stocks, sum_variances, sum_covariances = [], [], [] for num_stocks in range(1, maximum_number_of_stocks+1, step_size):     portfolio = returns.iloc[:, :num_stocks]     covariance_matrix = portfolio.cov()     list_num_stocks.append(num_stocks)     variance_returns = np.nansum(np.diag(covariance_matrix))     sum_variances.append(variance_returns)     sum_covariances.append(covariance_matrix.sum().sum()                            - variance_returns)      The plot above shows us exactly the hypothesized outcome. The over sum of covariances represents a multifold of the sum of variances of the portfolio. This finding entails interesting implications for the selection process of an asset. The asset’s own variance carries little to no importance for the risk of a well-diversified portfolio.   “A longer investment horizon reduces the portfolio risk”   Theory  The investment horizon describes how an investor plans to hold the portfolio. These ranges can span from merely a few days up to multiple years. When deciding for a longer investment horizon an investor typically allocates most of their assets to a higher risk area, like equities. This is because the larger price swings, equities exhibit in the short term, have no impact on the long-term goals of the investor.   Empirical Evidence   The following code shows how we can empirically test this theory.   number_of_stocks = 100 weighting = 1/(number_of_stocks ** 2) years = np.arange(2000, 2020, 2) risk_list = [] for year in tqdm(years):     time_returns = financial_dataframe_creation(ticker_list,                                                 number_of_stocks,                                                 year)     portfolio_var = weighting * time_returns.cov().sum().sum()     portfolio_std = np.sqrt(portfolio_var)     risk_list.append(portfolio_std)   The code above first takes 100 arbitrary stocks using our function which we declared in the beginning of this post. Afterwards the portfolio variance for an equally weighted portfolio is calculated for different time-spans. The result is the following plot.      Interestingly, even though we can see an overall trend to a smaller risk level over time, the relationship is not perfectly negative. This is because we find a local minimum after around eight years. This finding is likely to be explained by the financial crisis 2008. Given that we started to track our portfolio performance in the year 2000, after eight years we find our portfolio in one of the most financially unstable times in modern history. After a couple of turbulent years, we find the portfolio risk to decrease again, which is quite an affirmative sign in the direction of our hypothesis that a longer time horizon reduces our portfolio risk.   ","categories": ["Portfolio","Finance","Python"],
        "tags": [],
        "url": "/portfolio/finance/python/Empirical-Evidence-to-common-Stock-Portfolio-Questions/",
        "teaser": null
      },{
        "title": "Markowitz Portfolio Theory - An empirical explanation",
        "excerpt":"Building and simulating different stock portfolio and assessing risk and return with Python and R Shiny Link to App   Building a portfolio is easier said than done. The sheer amount of combination possibilities is overwhelming. Furthermore, it is oftentimes not completely clear what a certain stock combination will do to the portfolio return and risk. This blog-post clarifies how to determine the portfolio risk and return by showing a two-asset example before moving on to the general n-asset example.   Before starting, we declare the needed packages in Python as well as the directories we are going to need. Furthermore we import the a function which creates financial return data by inputting the   # Packages import os import numpy as np import pandas as pd import matplotlib.pyplot as plt from tqdm import tqdm import quandl import datetime as dt import scipy.optimize as solver # Paths main_path = r\"/Users/user/Documents/projects/markowitz\" raw_path = r\"{}/00 Raw\".format(main_path) code_path = r\"{}/01 Code\".format(main_path) data_path = r\"{}/02 Data\".format(main_path) output_path = r\"{}/03 Output\".format(main_path)  Portfolios of Two Risky Assets   When having a multi-asset portfolio there are not many levers we can pull to alter the portfolio’s return and variance. The average return, the risk (standard deviation) of a financial asset as well as the correlation between the two assets is fixed. The only thing we have influence over is how much weight we put on each asset. That means we have to ask ourselves how we should split our capital on the acquisition of both assets, how much of each asset we should buy.   As an example we take the stocks, Apple Inc. (abbreviated as A) and Bank of America (abbreviated as B). Furthermore we denote w as the weight of a respective stock and r and σ as the return and standard deviation respectively. The return of the portfolio containing these two stocks is then calculated as the simple weighted average of theses stocks      The portfolio variance is denoted as sum of the covariance matrix of these two stocks. This becomes clear when looking at the formula below in addition to the covariance matrix.       Note that the variances are not multiplied by their weight as is, but by their quadratic weight. This happens because the variance is simply the covariance with itself.   Now we would like to see what kind of risk and returns are attainable for different weights. For that start by loading in the time-series returns for both stocks. We do that by using the powerful pandas_datareader package and the yahoo finance API.   starting_date = \"1/1/2010\" ending_date = \"1/1/2020\" stock_tickers = [\"AAPL\", \"BAC\"] # Pull the chosen tickers df_prices = pd.DataFrame() for ticker in stock_tickers:     try:         tmp = web.DataReader(ticker, \"yahoo\",                              starting_date,                              ending_date)         df_prices.loc[:, ticker] = tmp['Adj Close']     except KeyError:         pass     except RemoteDataError:         print(\"Returns for these dates not available\")         pass returns = np.log(df_prices / df_prices.shift(1)) * 252 # Renaming columns ticker_names = list(returns.columns) actual_names = [\"Apple\", \"Bank of America\"] renaming_dict = {i: j for i, j in zip(ticker_names, actual_names)} returns.rename(columns=renaming_dict, inplace=True)   From the code above we see that we cannot simply use the original company names “Apple” and “Bank of America”, but something called a stock ticker. A stock-ticker is a three to four letter abbreviation of a company’s name. Furthermore, we have to define for which time-frame we would like to load the data. In our example we arbitrarily pick the beginning of 2010 and 2020 as the starting and end date respectively.   In order to show annualized returns, we multiply the returns by the number of trading days per year. The next step is now to simulate the return and risk of our two stock portfolio for all kind of weighting possibilities. For that we define a function which returns us the portfolio return and risk for a given level of granularity and, as will be explained a bit later, the correlation between the two assets.   def two_stock_portfolio(reshaped_returns, granularity, rho):     \"\"\"     This function calculates all the risk and expected return for     all possible weight combinations for two stocks. It returns     two lists which contain the risk level and expected return     for all portfolio combination. The granularity adjusts     how many portfolios are calculated. Parameters     ----------     reshaped_returns : dataframe         DataFrame which contains exactly two time series with         the two stocks returns     granularity : int         Determines for how many portfolios we calculate the         risk and return     rho : float         Depicts the correlation which is important for calculating         the portfolios risk Returns     -------     list_return : list         List with all portfolio returns     list_std : list         List with all portfolio standard deviations \"\"\" share_names = list(reshaped_returns.columns)     weights = np.linspace(0, 1, num=granularity) exp_returns = np.mean(reshaped_returns)     std_returns = np.std(reshaped_returns) list_return, list_std = [], []     for weight in weights:         portfolio_return = (weight                             * exp_returns[share_names[0]]                             + (1-weight)                             * exp_returns[share_names[1]])         portfolio_var = (weight**2 * std_returns[share_names[0]]**2                          + (1-weight)**2                          * std_returns[share_names[1]]**2                          + (2*weight*(1-weight) *                             std_returns[share_names[0]]                             * std_returns[share_names[1]]                             * rho))         portfolio_std = portfolio_var ** (1/2)         list_return.append(portfolio_return)         list_std.append(portfolio_std) return list_return, list_std   The granularity depicts how many portfolios we would like to simulate, that means how many different weights should be used to calculate return and risk of a portfolio.   The long formulas for the variables portfolio_return and portfolio_var are nothing other than the two formulas shown earlier in this post. After initializing the function we call it and plot the results through the following snippet of code.   exp_returns = np.mean(returns) risk_level = np.std(returns) rho = returns.corr().loc[actual_names[0], actual_names[1]] granularity = 1_000 list_returns, list_std = two_stock_portfolio(returns,                                              granularity, rho) fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10, 10)) axs.plot(list_std, list_returns, label=\"Possible Portfolios\", color=\"black\") axs.scatter(risk_level[0], exp_returns[0], label=actual_names[0],             color=\"turquoise\", s=200) axs.scatter(risk_level[1], exp_returns[1], label=actual_names[1],             color=\"blueviolet\", s=200) axs.legend(fontsize=18) axs.set_ylabel(\"Annualized Expected Return\", fontsize=18) axs.set_xlabel(\"Annualized Risk\", fontsize=18) axs.set_title(\"All possible Portfolios\", {\"fontsize\": 18}) axs.tick_params(axis=\"both\", labelsize=16) fig.savefig(\"{}/markowitz_basic.png\".format(output_path),             bbox_inches=\"tight\")      The resulting portfolio possibilities give us several interesting insights. Firstly, as expected, the two possible extreme portfolio weighting are a 100% allocation of one of the two assets. Secondly, we see that there are possible portfolios with which we can reduce the overall risk level compared to when holding only one of the two stocks. Thirdly, we find there are some portfolios which are superior to others. Considering the portfolio closer to the Apple stock, we see for the same level of risk we could have create a portfolio with more or less return.   The second finding also explains the aforementioned importance of the correlation between the two assets. This is because the two company’s returns are driven by slightly different factors. Imagine a portfolio with two companies, one company producing swimming trunks and the other producing umbrellas. This portfolio is nicely hedged given that if the weather is sunny the returns of the swimming-trunks company are high while the umbrella company’s return are low. Exactly the reverse is true on a day with bad weather. Looking at the correlation of the returns of these two time-series we find a less-than-perfectly-positive correlation. This fact allows the investor to dampen negative returns of one company through higher returns of another company within their portfolio. How high this risk reduction is depends on the correlation of these two financial returns.   The dependence on the correlation is also nicely visible when altering it. Let us see how the plot from above would look like if we alter the correlation to a perfectly negative correlation (-1) and a perfectly positive correlation (+1). This is done through the following code.   potential_correlation = {     \"Perfect Negative Correlation\": -1,     \"Actual Correlation\": rho,     \"Perfect Positive Correlation\": +1     } fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10, 10)) for name, rho in potential_correlation.items():     list_returns, list_std = two_stock_portfolio(returns,                                                  granularity, rho)     axs.plot(list_std, list_returns, label=name) axs.scatter(risk_level[0], exp_returns[0], label=actual_names[0],             color=\"turquoise\", s=200) axs.scatter(risk_level[1], exp_returns[1], label=actual_names[1],             color=\"blueviolet\", s=200) axs.legend(fontsize=18) axs.set_ylabel(\"Annualized Expected Return\", fontsize=18) axs.set_xlabel(\"Annualized Risk\", fontsize=18) axs.set_title(\"The influence of correlation on the portfolio possibilities\",               {\"fontsize\": 18}) axs.tick_params(axis=\"both\", labelsize=16) fig.savefig(\"{}/markowitz_correlation.png\".format(output_path),             bbox_inches=\"tight\")      Very interestingly, we can see that in case we have a perfect positive correlation, we do not have any risk reduction effect, whereas we are able to create a completely risk-less portfolio when having a perfectly negative correlation.   Multiple Asset Case   After gaining some intuition as to how a portfolio return and risk is calculated, we are now ready to take a look what happens to multi-asset portfolio.   In general it can be said that not much changes. The return of a multi-asset portfolio is still denoted as the weighted average of the individual stock returns. Also, the risk of a multi-asset stock portfolio is calculated by summing up the weighted covariance matrix. The two formulas from above change therefore into the more general form of:      In order to show what happens empirically, we select five arbitrary stocks (Apple, Bank of America, Amazon, AT&amp;T and Google) and plot their return and risk level by using the following code snippet   starting_date = \"1/1/2010\" ending_date = \"1/1/2020\" stock_tickers = [\"AAPL\", \"BAC\", \"AMZN\", \"T\", \"GOOG\"] # Pull the chosen tickers df_prices = pd.DataFrame() for ticker in stock_tickers:     try:         tmp = web.DataReader(ticker, \"yahoo\",                              starting_date,                              ending_date)         df_prices.loc[:, ticker] = tmp['Adj Close']     except KeyError:         pass     except RemoteDataError:         print(\"Returns for these dates not available\")         pass returns = np.log(df_prices / df_prices.shift(1)) * 252 exp_return = np.mean(returns) risk_level = np.std(returns) fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(20, 10)) axs.scatter(risk_level, exp_return, color=\"red\", marker=\"+\", s=200,             label=\"Stocks\") axs.set_ylabel(\"Annualized Expected Return\", fontsize=18) axs.set_xlabel(\"Annualized Risk\", fontsize=18) axs.set_title(\"Expected Return and Risk Scatter for 5 stocks\",               {\"fontsize\": 18}) axs.legend(fontsize=16) axs.tick_params(axis=\"both\", labelsize=16) fig.savefig(\"{}/unrestricted_markowitz.png\".format(output_path),             bbox_inches=\"tight\")      As done before we will now simulated possible portfolios. This is done by creating 1,000,000 randomly generated n-column long weight vectors which are then used to simulate many potentially possible portfolios.   vector_returns = returns.mean().fillna(0) covariance_matrix = returns.cov().fillna(0) num_of_companies = len(returns.columns) number_of_weights = 1_000_000 random_weights = np.random.dirichlet(np.ones(num_of_companies),                                      size=number_of_weights) weight_times_covariance = np.matmul(random_weights, covariance_matrix.values) weights_transpose = random_weights.T portfolio_returns = np.matmul(random_weights, vector_returns.T) portfolio_std = [] for i in tqdm(range(number_of_weights)):     row = weight_times_covariance[i, :]     col = weights_transpose[:, i]     diagonal_element = np.dot(row, col)     element_std = np.sqrt(diagonal_element)     portfolio_std.append(element_std) fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(20, 10)) axs.scatter(portfolio_std, portfolio_returns, color=\"dodgerblue\", alpha=0.5,             label=\"Possible Portfolios\") axs.scatter(np.sqrt(np.diag(covariance_matrix)), vector_returns, color=\"red\",             label=\"Stocks\", marker=\"+\", s=200) axs.tick_params(axis=\"both\", labelsize=16) axs.set_ylabel(\"Annualized Expected Return\", fontsize=18) axs.set_xlabel(\"Annualized Risk\", fontsize=18) axs.set_title(\"Markowitz Mu-Sigma Diagram\", {\"fontsize\": 18}) axs.legend(fontsize=16) axs.tick_params(axis=\"both\", labelsize=16) fig.savefig(\"{}/n_asset_portfolios.png\".format(output_path),             bbox_inches=\"tight\")   We see that this result looks significantly different to the two-asset case before. Firstly, instead of being all on one line, we now have some sort of an area of possible portfolios. Furthermore, when looking at the left side of these portfolios, it looks like there is some sort of a boundary as we already found in the two-asset case. This boundary is denoted as the asset frontier. It depicts all portfolios which have the minimal risk possible for every given level of return, while not allocating more than 100% of the capital available. This minimization problem has the following formal mathematical notation.      As an investor, we are, of course, interested in finding the lowest possible risk for a given return. Therefore finding the portfolio weights which solve the equation above, are of great interest to us. Through the scipy package in Python it is possible to calculate this minimization problem. A nice tutorial on how to use the scipy.minimize is found here.   Through the following code we calculate the desired frontier line and plot it additional to our simulated portfolios.   def objective_function(w):     return np.sqrt(np.linalg.multi_dot([w, covariance_matrix, w.T])) bounds = tuple((0, 1) for x in range(num_of_companies)) initial_weights = np.random.dirichlet(np.ones(num_of_companies),                                       size=1) min_return, max_return = min(portfolio_returns), max(portfolio_returns) given_return = np.arange(min_return, max_return, .001) minimum_risk_given_return = [] for i in given_return:     constraints = [{\"type\": \"eq\",                     \"fun\": lambda x: sum(x) - 1},                    {\"type\": \"eq\",                     \"fun\": lambda x: (sum(x*vector_returns) - i)}]     outcome = solver.minimize(objective_function,                               x0=initial_weights,                               constraints=constraints,                               bounds=bounds)     minimum_risk_given_return.append(outcome.fun) fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(20, 10)) axs.scatter(portfolio_std, portfolio_returns,             color=\"dodgerblue\",             alpha=0.5,             label=\"Possible Portfolios\") axs.scatter(np.sqrt(np.diag(covariance_matrix)), vector_returns,             color=\"red\",             label=\"Stocks\", marker=\"+\", s=200) axs.scatter(minimum_risk_given_return, given_return,             color=\"salmon\",             label=\"Frontier\") axs.tick_params(axis=\"both\", labelsize=16) axs.set_ylabel(\"Annualized Expected Return\", fontsize=18) axs.set_xlabel(\"Annualized Risk\", fontsize=18) axs.set_title(\"Markowitz Mu-Sigma Diagram\", {\"fontsize\": 18}) axs.legend(fontsize=16) axs.tick_params(axis=\"both\", labelsize=16) fig.savefig(\"{}/minimization_problem.png\".format(output_path),             bbox_inches=\"tight\")      After depicting the frontier portfolios, it is to be noted that not all of those carry importance to an investor. This is because we find portfolio which have for a given risk level two levels of return. Given that we, as an investor are interested to obtain the maximum return given every risk level, only one out of two portfolios is therefore of interest to us.   Put differently, we are only interested in efficient portfolio. Efficiency is denoted here as having the highest return for every given risk level, or the lowest risk for every given return level.   When cutting off the inefficient portfolios from the frontier line, we end up with something called the efficient frontier.   Another portfolio of interest is the Minimum Variance Portfolio (MVP) which denotes the portfolio which has the lowest variance possible, out of all portfolios.   The following code snippet implements the concepts of efficient frontier and MVP.   min_var = min(minimum_risk_given_return) mvp_bool = min_var == minimum_risk_given_return mvp_risk = np.array(minimum_risk_given_return)[mvp_bool] mvp_return = np.array(given_return)[mvp_bool] ef_bool = given_return &gt;= mvp_return ef_risk = np.array(minimum_risk_given_return)[ef_bool] ef_return = np.array(given_return)[ef_bool] fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(20, 10)) axs.scatter(portfolio_std, portfolio_returns, color=\"dodgerblue\", alpha=0.5,             label=\"Possible Portfolios\") axs.scatter(np.sqrt(np.diag(covariance_matrix)), vector_returns, color=\"red\",             label=\"Stocks\", marker=\"+\", s=200) axs.scatter(ef_risk, ef_return, color=\"springgreen\",             label=\"Efficient Frontier\") axs.scatter(mvp_risk, mvp_return, color=\"orange\", marker=\"D\",             label=\"Minimum Variance Portfolio\", s=400) axs.tick_params(axis=\"both\", labelsize=16) axs.set_ylabel(\"Annualized Expected Return\", fontsize=18) axs.set_xlabel(\"Annualized Risk\", fontsize=18) axs.set_title(\"Markowitz Mu-Sigma Diagram\", {\"fontsize\": 18}) axs.legend(fontsize=16) axs.tick_params(axis=\"both\", labelsize=16) fig.savefig(\"{}/efficient_frontier.png\".format(output_path),             bbox_inches=\"tight\")   The image above shows nicely the MVP as well as the efficient portfolios out of all frontier portfolios.  Multiple Asset R Shiny App   Last but not least, we put all of our findings in one application which can be accessed here:   Link to App   This app is a hybrid between Python and R. Python was used to calculate all portfolio information, whereas R Shiny was used to build the web-application. Finally, the app was deployed using shinyapps.io. All code is put on github and can be accessed here.  ","categories": ["Portfolio","Finance","Python"],
        "tags": [],
        "url": "/portfolio/finance/python/Markowitz-Portfolio-Theory-An-Empirical-Explanation/",
        "teaser": null
      },{
        "title": "How Polarized Is America Really?",
        "excerpt":" Evidence for increasing polarization from the voting record of the United States Congress    Github Repository   The first presidential debate of 2020 seemed to reach a new low. The internet was awash with think pieces about just how contemptuous the relationship between the candidates seemed, with most people convinced that the clash between Joe Biden and Donald Trump was even dirtier than past debates. The same could have been said in 2016, comparing the debates between Hillary Clinton and Donald Trump in that year to debates prior. Is it only a coincidence that these debates get ever more heated over the years, or is this actually the product of a growing polarization in the country?   In his best-selling book “Why We’re Polarized”, Ezra Klein describes how the polarization in America came to reach its current fever pitch. America, and arguably the entire world, is getting more polarized year over year. Quoting from the book, “…when Gerald Ford ran against Jimmy Carter, only 54 percent of the electorate believed the Republican Party was conservative than the Democratic Party. Almost 30 percent said there was no ideological difference at all between the two parties”- a statement which would be unthinkable to describe today’s politics.   Klein’s book inspired us to dig deeper into the numbers and see if just how bad the polarization problem is. But how could we quantify polarization? One way would be to look at how America’s elected politicians voted in Congress. As Klein’s book mentioned, in an un-polarized system, there is not such a strong ideological divide between members of each party. In terms of voting, this means that members of each party would be more likely to cross party lines when voting on certain issues. In a more polarized system, members would be much more likely to vote only inline with their party.   We thus decided to look at the voting positions of members of both parties over the past 30 years. Every vote the House and the Senate take are publicly available online (here), in the format shown below.      About the Congress   To understand this data, we must first understand the US legislative system.   The U.S. has a bi-cameral legislature, meaning there are 2 houses that make up the legislative branch. The legislature is called Congress. The two houses (also called chambers) comprising the Congress are the House of Representatives (lower house), and Senate (upper house). The House of Representatives (often shortened to just “the House”) is based on population. There are 435 total Representatives (also called Congresswomen or Congressmen), and each state receives a different number of Representatives depending on their state population. For example, California, with a state population of 39.5 million, has 53 representatives, whereas Nebraska with its population of 1.9 million has 3. The number of representatives assigned to each state is updated every 10 years, after the US census is completed. This is why the census in America is highly politicized. Each Representative in the House serves for 2 years. This means that the entire House has the possibility to be renewed every 2 years.   The Senate works differently — each state receives 2 Senators, regardless of its population. Since the US has 50 states, there are 100 Senators in total. Senators serve 6-year terms, meaning that every 2 years, 1/3 of the total Senate has the possibility to be renewed.   Both houses of Congress can propose bills. Bills then receive an ID number. If the bill was proposed by the House, its number will start with “H.R.”. If it was proposed by the Senate, its number will start with “S”. The chamber which proposed the bill will vote on it first, but to become law, both chambers must vote on the bill. Bills are then signed into law (or vetoed) by the President.   Now that we have some background about the two chambers of Congress, we can take a closer look at the outcome statistics of one House and Senate vote, respectively.   Data   The image below shows one example of the result of a vote in the House. From the header we can see in which year the vote was held and which bill was voted on. Our data also includes a table showing how members of each party voted on the measure.      For the Senate, the data we have about each vote is more detailed. In order to find out how many Democrats or Republicans voted with either “Yea” or “Nay”, we have to extract that information from the Senator-specific table which is given below.      We used web scraping to extract and save the data for each vote in the House and Senate. The Python script for this webscraping can be found in the github repository for this project.   Empirical Evidence   After scraping all decisions of the Senate and House between 1989 and 2020, it is time to look at some visualizations to better understand our data. First, we look at how many people of each party held seats, on average, in the House and Senate. This is done by averaging the data over the years and grouping by party (Democratic or Republican) and type (House or Senate).      This image nicely shows how the majority in the House and Senate changes over time. We can see a general correlation between the majoirty party in both chambers — when the House is controlled by Republicans, generally the Senate is as well. Given that the entire House is replaced every second year, compared to only 1/3 of the Senate, it is logical that we see a drastically higher fluctuation over time. Given the fact that is replacing the Senate simply takes significantly longer than to replace the House, we can also see that after one party gained the majority in the House, the corresponding switch in majority in the Senate trails behind, switching a couple of years afterwards.   This correlation between the House and Senate is also visible when looking at the correlation matrix below. Here we can see majority in the House and Senate are strongly positively correlated with a value of 0.72 for the democratic and 0.67 for the republican party.   Especially interesting is the last row of the correlation matrix. Here we can see that between 1989 and 2020, we see on average an increase in the number of seats for the republicans in the Senate as well as House. Given that the Senate and House have a fixed number of seats, we find a negative correlation between Year and the number of seats for democrats.      Another interesting figure to look at is the number of decisions made in the House as well as the Senate over time. This number is calculated by a simple count of votes for every year between 1989 and 2020. From the chart below we can see that the House voted on a higher number of decisions, compared to the Senate. Furthermore, we can see that both lines seem highly correlated over time. This is sensible given that much legislation has to go through both the Senate as well as the House in order to be passed.      The concept of Polarization   After covering some summary statistics of the data and US legislature, we shift to focus on the polarization within American politics. It is first crucial how we define as polarization. In order to create a better understanding of that term, we will paraphrase some parts from the aforementioned, well-researched book from Ezra Klein:    There is a long-running debate whether what we witness in American politics is actually polarization or “only” sorting. In order to explain the difference we take the example of abortion and assume 100 randomly drawn people. Forty people would like to ban abortion, another forty would like to make it legal and the missing twenty have not made up their mind yet.   Let us now assume that all people who are in favor of the right to get an abortion join the Democratic Party and the ones who are not in favor join the Republican Party, and the undecided voters join evenly both parties. In this case, we would say that the beliefs are perfectly sorted by party. It is important to note that nobody changed his or her opinion compared to the point before when nobody joined a party.   In order to show what polarization is, we change the example a bit. We now say there are no more undecided voters and that we now have fifty people on each side of the spectrum. The elimination of the middle ground and the clustering around the opposite political poles of an issue is called polarization. &lt;/em&gt;   That means that as the party puts more focus on certain policy issues, it becomes harder and harder for people to stay undecided. The influence of the political parties and its members make the standpoints of the opposite parties even more distant from one another.   Furthermore, the interaction between the ever-growing distance between the two standpoints and the resulting political division between people is a self-reinforcing one. People who joined the Democratic Party given their pro-choice beliefs expect to be represented accordingly by the Party. This push by its members to fight for these issues makes the position of the Democratic Party even more contrasting to the views of the Republican Party. This polarization process will then result in a decreasing number of undecided voters over time.   The concept of polarization is further visualized in the GIF below. Note the image number at the bottom right of each image, we will use these to describe what’s happening in each polarization phase.      The first picture (1) shows a case where we have an unsorted nation — people with certain beliefs are not part of any of the two available sides of the problem. The second picture (2) shows the phenomena of sorting. All people are now on either the Republican or Democratic side. We still have a couple of undecided voters who do not have a strong opinion regarding the topic of abortion. The third picture (3) is then the crucial one. The reinforcing effect of the stronger party focus on the issue increases the contrast between the two parties to an such an extent that it is hardly possible to stay undecided. Like a magnet that grows stronger over time, undecided voters are pulled to either side.      Empirical Measurement of Polarization  After covering what we mean when we talk about polarization, it is now the time to define it mathematically, in order to quantify it.   Measuring polarization means simply assessing how strong the clustering is around certain political issues. If the clustering is strong, we would assume that the disagreement between the Democratic and Republican parties is high. When having a vote, clustering would mean that the absolute difference between the relative amount of Democrats who said yes (no) to the relative amount of Republican who said yes (no). Given that rationale, we define empirical polarization as the following:      The reason for taking the average of the absolute relative difference of the Yeas and Nays, instead of simply taking either one of them, is to account for the special case of having non-voters.   We are now able to quantify how high the polarization was for every single vote held in the Senate and House. Next, we average the resulting polarization numbers for each vote on a yearly basis, separating for House and Senate. The graph below show the result of these steps.      From the graph above it looks like the overall polarization increased over time. Since we do not want only trust our eyesight, we also fit a linear regression line through the observations. The resulting line and summary statistics are visible below.      The resulting line above strengthens our belief of the positive relationship between year and average polarization. Looking at the summary statistics below confirms that this positive linear relationship is also significant. The coefficient is to be interpret the following way: holding all else constant, the influence of increasing the year by one changes the average polarization score by 0.00679.   That the effect is significant can be seen from the p-value. The p-value tells you how likely it would be to find such a test result, or an even more extreme one, assuming the null hypothesis is correct. The null hypothesis when testing a beta coefficient within a linear regression is that the true value of beta is equal to zero.      From the regression above we learned that the average polarization increases by approximately half a percentage point every year. This might seem insignificant on first glance, but when putting this number into perspective, we see that long-lasting implications of it. If the average polarization value was around 40% in 1990, the coefficient of 0.00679 would imply an increase average polarization of 0.00679 * 30 years = 0.2037. An increase of 20 percentage points! This increase is mindblowing and truly shows the direction of american politics and reflects the mindset of the people.   Lastly, given that we could see from the graph before that the average polarization did not increase linearly but actually more in a step-function, we test whether these increases are related to a change in the party holding the majority in each chamber. In the graph below we colored the background according to the majority in the given chamber.      On first glance it does not seem that which party has the majority has any effect on the average polarization. In order to also quantify this hypothesis we measured the correlation between who has the majority (Republicans encoded as 1 and Democrats as 0) and the average polarization. From the image below we see our initial judgement of the graph above validated. With a relatively small correlation of 0.12, it does not empirically show that who has the majority of either the House and Senate has any effect on the increase of the average polarization.      Assessing Party Unity   After assessing the average polarization over the years, it would be interesting to assess the respective party unity. A higher value of polarization might imply a higher value of party unity, though, we do not know which party, if not both, developed a stronger standpoint on a topic.   For that reason we also now define mathematically what we mean with party unity:      The formula above has a maximum of one, for the case where the all Representatives or Senators of a party unanimously vote either yes or no. The minimum of the function is zero, when we have exactly half of the party votes yes and the other half votes against it.   After calculating the party unity for all votes we have in our data, we then average the unity over the years. The resulting data and fitted regression lines for the House and Senate are shown in the image below.      Given that we already established a significant trend of polarization within the Congress, it is no surprise to find positive trend lines. Overall, it looks like the House has a stronger party unity compared to the Senate for both parties. This fact could be explained by the fact that the entire House is replaced every two years, and therefore that chamber is more susceptile to increasing polarization of public opinion. The Senate, on the other hand, changes only 1/3 of its members every two years and thus is less affected by these fluctuations.   Another interesting note is that for both chambers we see a higher level of party unity for the Democratic party. Especially for the Senate the difference in the slope coefficient becomes apparent. That said, it’s also important to note that the intercept for the Republican party is higher for both chambers. Combined with the somewhat smaller slope, that means that Republicans used to be more united than the Democrats, but lost that top-spot to the Democrats over the years.   Conclusion  Statistics can help us verify our gut feelings on a given topic. In this article, we were able to empirically prove what many observers of American politics have long felt — that our country is becoming more divided. By defining equations for both polarization and party unity, we were also able to quantify the extent to which these effects are worsening.   Now that we know polarization really is a problem that’s getting worse over time, the next logical question is what we can do to stymie it. In “Why We’re Polarized”, Klein’s advice to readers is to move their politics and hopes for the issues that matter to them from the national to the local level. By engaging in local politics and community engagement, we engage directly with issues on the ground, which helps take the focus on these issues away from the parties themselves. This helps break the self-reinforcing cycle where policy polarization feeds political polarization. Other solutions are more structural and involve re-wiring our political system to better serve the needs of the country today. Klein also mentions that institutions that block the will of the people from being heard, such as the Electoral College, further fuel polarization and should be eliminated.   It’s clear that we must take look for both cultural and structural solutions to the problem of polarization if we want to see civility brought back to our politics, and their biggest stage, our presidential debates.  ","categories": ["Data Journalism","Web Scraping","R"],
        "tags": [],
        "url": "/data%20journalism/web%20scraping/r/How-Polarized-Is-America-Really/",
        "teaser": null
      },{
        "title": "Polarization in the Senate",
        "excerpt":" Polarization is increasing. How does this play out in the Senate?    Code for this project can be found on Github   In our last article, we showed empirically that polarization in the American Congress in increasing. In this article, we dive deeper and explore these trends in the Senate specifically. We specify another metric, party loyalty, as a proxy value to polarization. This metric helps us further confirm our empirical findings on the trend of polarization and to answer the question: are individual Senators becoming more loyal to their party? Do Senators who are more loyal to their party stay in power longer than those who reach across the aisle?   Party loyalty is defined as the percentage of Senators of a given party which votes in line with the majority of the party. To illustrate the concept we take the vote for the Affordable Care Act as an example. In this vote all Democrats voted for it, while all Republicans voted against it. The party loyalty is therefore 100% for both parties.   As can be seen from the screenshot below, which shows this result of the vote for the Affordable Care Act, we can also see the name and state of each Senator. That allows us to investigate the party loyalty for each Senator and state. The website provides us with all bills voted on in the Senate between 1989 and 2020. Having data for over 30 years gives us the possibility to investigate the dynamics over time.      In order to calculate the average party loyalty for a given year, we proceed as follows: for every bill a Senator faces, we assign them the value1 if the voted with party majority, and a zero if they did not. Afterwards we average all these values a Senator accumulated over a years time. With that we obtain the average party loyalty of an individual Senator. If we now average the values of all Senators within a party, we get the average party loyalty of the entire party within a year. Below, a mathematical notation for this concept is given.      Furthermore, it is important to talk about what the numerical boundaries of our concept of party loyalty are. From our Affordable Care Act example, it should be clear that the maximum average party loyalty across all Senators of one party is equal to 100% or 1. That is because everybody voted in line with the majority of the party.   The lower boundary of the average party loyalty across all Senators of one party, though, cannot be any lower than 0.5. That is because how we define a party’s majority. A majority is defined as having more people voting for it than against it.   Assuming a positive even amount of Senators, N, within one party, and given the discrete nature of votes, the minimum amount of votes needed for a majority is equal to (N/2 + 1). All these Senators will, following our definition, be assigned a value of one, whereas the rest (N/2 - 1) gets assigned the zero. If we now calculate the average across all Senators we get the following equation which shows that the average value is always going to be strictly above 0.5. The mathematical notation below also shows that this holds for an uneven number of Senators within a party.      Of course we also have to address the case of an even amount of Senators where exactly half of the Senate votes one way, and the rest votes the otherway. In that case we manually set the party loyalty to 0.5.   Average Party Loyalty across all Senators and Parties over Time  Before looking into how the average party loyalty developed within each party, we start by looking into the development average party loyalty across Senators and parties. This figure is obtained by averaging the party loyalty figure of each Senator over both parties for a given year.      As we can see in the graph above, party loyalty has been increasing over the period of study. A regression analysis of the figure over the years, shown below, also confirms that the suspected trend is indeed statistically significant.      This finding is not all that surprising, given our findings of the positive slope of polarization in the US congress over time. Though, the result here serves as a valid robustness check of our previous finding.   Difference between Party Loyalty and Polarization   One might be asking now what the difference is between Polarization and Party Loyalty. The biggest benefit of party loyalty is that the figure is, unlike Polarization, applicable not only for the entire Senate as a whole but also for individual parties and even Senators.   It should be noted that it’s possible for Polarization and Party Loyalty to go the opposite way. For example, it’s possible to have high polarization coexist with high party loyalty. The Affordable Care Act is a good example for that. All Democrats and Republicans vote in line of their party.   However, it is also possible to have low polarization but high party loyalty. That case would describe a vote where both parties voted for the same thing. Here we would have low polarization, because the parties are not opposing each other, but still high party loyalty.   The case of both low polarization and low loyalty is also possible. That case would describe a vote where half of both parties would vote Yea and the respective other votes Nay. The only impossible combination is to have high polarization and low party loyalty.   In short, party loyalty and polarization tend to correlate with each other, but do not necessarily need to. The advantage of party loyalty is though that it can be analyzed on a deeper level.   Party Loyalty by State   It is widely known that politics in America differ widely by state. There are states that consistently vote Democrat, where a Republican has almost no chance of winning the Senate (like New York and California), and red states like Oklahoma and Wyoming where the opposite is true. More interesting perhaps are so-called “purple states” that have long had one Senator from each party, like Missouri, West Virginia, and Indiana.   In the chart below we plotted the average party loyalty by state over all the years in our dataset (1989–2020). Even though we find many of the aforementioned states in the bottom half of the chart, the difference between states is not statistically significant between most states.   The aggregated results, averaged over all 30 years for all Senators of both parties, do not show very large differences between the states, with the exception of Maine. This makes sense given the state’s strong history of bipartisanship.   In order to get a better view how the average party loyalty changed over time in each state we look at a dynamic version of the data in the next section.         Looking at party loyalty for all Senators in each state over time gives a much clearer picture and helps us to better understand the rising polarization we see today.   Beginning with the overall trends, it is clear that the entire map has darkened significantly over the period of our data (darker colors = stronger Party Loyalty). This is again in line with our finding shown above that there is a statistically significant increase in party loyalty over this time period. Certain states like Michigan, California, and Washington seem to stay stubbornly dark throughout the time period observed, whereas some surprising states like Nebraska and Alabama have periods where they appear quite light on our map.   In the next section, we investigate some of the “white spots” (states with low party loyality at that time), in hopes that they shed some light on the deepening party loyalty and polarization problem in the United States.   The curious case of Alabama   Starting chronologically at the start of our dataset in 1989, we see something that we would not expect to see in 2020: Alabama as one of the states with the lowest average party loyalty. The state even seems to shine brighter (lighter colors on the graph show a lower average party loyalty) for the first few years of the 1990s, before dimming a bit in 1994, and blending completely in with its dark grey surroundings of other high-loyalty states by 1997.   To understand this change in party loyalty in Alabama over these years, it is important to first understand the history of the Democratic Party, and the party system in America in general. In his book “Why We’re Polarized”, Ezra Klein discusses extensively that throughout much of the 20th century, the two parties were not nearly as polarized as they are today. The average person in the 1950s and 60s had a hard time saying which party was more liberal or conservative. This trend is perfectly encapsulated by the phenomenon of the “Dixiecrat” Democrats, a now defunct political group to which Ezra Klein devotes a chapter in his book. The Dixiecrats were Southern members of the party who were deeply conservative, often white supremicist, and essentially untouchable politically. Political scientists today classify them as political thugs who used dubious means like threatening dissidents to maintain power. For much of the twentieth century, many of them ran race after race unopposed. The slow breakdown of the Democratic party’s power in this region is considered to have begun with Richard Nixon’s so-called “Southern Strategy”, where he sensed an opportunity to win over Southern whites and began to pander to racist fears with his “law and order” policies. This strategy is also in large part responsible for pushing the Republican party further to the right and was one of the first major examples of a campaign pushing for race-based identity politics to influence votes. This was underscored by a statement made by Nixon’s campaign strategist, Kevin Phillips, to the New York Times in 1970:    From now on, the Republicans are never going to get more than 10 to 20 percent of the Negro vote and they don’t need any more than that… but Republicans would be shortsighted if they weakened enforcement of the Voting Rights Act. The more Negroes who register as Democrats in the South, the sooner the Negrophobe whites will quit the Democrats and become Republicans. That’s where the votes are. Without that prodding from the blacks, the whites will backslide into their old comfortable arrangement with the local Democrats.  In the quote, Nixon’s political strategist explicitly states the intent of the Republican party to try to tear away Southern white voters from the Democrats.   By the early 1990s, this ideological and identity politics polarization between the parties started by Nixon had continued, and the “Solid South” was quite thoroughly broken apart. At the time when our data starts, Alabama was a last vestige of, and thus very interesting case study for, this phenomenon of shifting party loyalties in the Deep South.   In 1992, Alabama had 2 Democratic Senators: Howell Helfin and Richard Shelby. We will analyze each of them in turn, along with a look at how party loyalty in their state evolved during their time in office.   Howell Helfin was, without a doubt, a holdout from the Dixiecrat era. He was a conservative democrat who strongly opposed abortion and gun control and was in favor of allowing prayer in schools. His uncle, another former Senator from Alabama named James Thomas Helfin, was an avowed white supremicist and member of the KKK who is quoted to have said “God Almighty intended the negro to be the servant of the white man.” It is no surprise, then, that by the time Bill Clinton was running as the Democratic candidate for president in 1992, that Helfin’s views and votes were misaligned with those of his party. In 1996, he chose to retire, and his seat was taken by conservative Republican Jeff Sessions. Jeff Sessions kept the seat until he was selected by Donald Trump to serve as Attorney General in 2016.   Alabama’s other Democratic Senator at the time, Richard Shelby, is actually still in office… as a Republican. In 1994, he switched parties as part of the Republican Revolution, where Republicans gained a substantial number of seats in the midterm elections. Shelby’s switch to the Republican party explains why the state darkens on our map in that year. Helfin’s succession by Jeff Sessions 2 years later explains why it turns fully dark in 1997, when Sessions’ term began.   Loyalty points   It’s clear that Senators from nearly every state, and from both parties, are uniting more strongly with their party and falling in line with how their party votes. For the next stage of our analysis, we wanted to see how this is impacting average tenure in the senate. Do more loyal Senators who vote in line with their party more often stay in power longer?   To do this, we looked at tenure for each senator within the time of our dataset. We did not include Senators who served for less than 6 years, as these Senators did not serve a full term.   What we found was that Senators in the middle loyalty quintile - with a party loyalty between .89 and .92 - lasted the longest. This seemed to be the “sweet spot” as far as party loyalty is concerned. Unsurprisingly, if a Senator voted against their party very often (partly loyalty less than .86), like those in the lowest quintile, they averaged less than 2 terms. What is also somewhat surprising, however, is that Senators who are extremely loyal to their party, those in the top quintile (party loyalty greater than .94), also have a shorter average tenure, although they have a wider spread in tenure than those in the lowest quintile.      The tale of 2 Senators   Part of the impetus for us to further explore the effects of polarization and deepening party loyalty in the Senate came from watching a recent debate between Senator Lindsey Graham and his Senate challenger Jaime Harrison. Graham is currently facing one of the toughest fights of his career, and is barely leading his Democratic challenger in the poles. In the debate, Graham laments what he views as the polarization of Senate Democrats when he underscores multiple times how he crossed the aisle to vote for two of President Obama’s Supreme Court nominees. His opponent is quick to point out his change of heart in 2016, however, when he supported Senate Majority Leader Mitch McConnell’s decision not to grant a hearing to Marrick Garland, President Obama’s final nominee. When watching the debate, it was clear from listening to Mr. Graham that his personal commitment to bipartisanship had waned since he reached across the aisle to vote for justices Sonia Sotomayor and Elena Kagan in 2009 and 2010, respectively. This could also be caused by the tougher primary challenge Graham faced in his last Senate race in 2014, including against Tea Party candidates and other pressure from the Right. In 2016, he ran for president and staunchly opposed then-candidate Donald Trump. After Trump’s victory, Graham was forced to change his view, and then became much more aligned with his party. In his current Senate race, Graham’s talking points mainly involve aligning himself with the current president and defending his administration.   As a comparison to Mr. Graham, we wanted to also investigate the changing party loyalty of a high-ranking and long-running Senator on the Democrat side. We chose the current Senate minority leader Chuck Schumer.   As can be seen in the graphs below, Mr.Schumer doesn’t have the same strong swings in party loyalty that can be observed on Graham’s side. Mr. Schumer also reached higher levels of party loyalty than Graham ever did, nearing a party loyalty score of 1.0 during the 2010s. Perhaps the most interesting observation on Schumer’s side, however, is how his party loyalty is actually precipitously decreasing. This could potentially be explained by his position as minority leader - he may feel that he has to lead by example and work across the aisle in his current role. Indeed, a recent investigation from Vox seems to confirm that his commitment to bipartisanship has strengthened since becoming minority leader in 2014. However, there are also signs that his party is tiring with his bipartisanship as it moves further to the left, and that his increasing bipartisanship may put him at risk for a challenge from the more progressive wing of his party in 2022.      This investigation into a high-ranking member of both parties shows that when a Senator has support from their home district, they are more likely to work across the aisle.   Looking at the two graphs above, it might be easy to try to generalize the patterns of Schumer and Graham and think that Democrats became less loyal to their party, while Republicans are the drivers of the problem. The two images below, however, show that this is not entirely the case.         One thing that we notice right away is how many seats the Democrats lost in 2018 - with their map becoming much more grey at that time. Another important takeaway is that there are markedly fewer, where a state has a representative from both parties.   Another striking moment in the images appears on the Republican side in 2017 - the map turns almost completely dark red as Republicans fell in line with their party to pass legislation after the election of President Trump.   These maps separated by party confirm our analysis of Alabama in the grey composite map above showing average party unity across all Senators. The light color, representing low party unity, of the country is seen very clearly during the years only on the Democratic map. When Richard Shelby changed party in 1995 (the first Congress after his election in 1994), the state turns quite dark red on the Republican side of the map, while the Democratic side stays fairly light. In 1997, when Jeff Sessions joined as Senator, the state falls off the Democratic map and turns even darker red on the Republican side.   Nebraska once again shines as a light state at many points on our map, likely because it is at many points a purple state with one Senator from each party.   Looking forward   What will these trends look like in the future? Will our polarized 2-party system continue to worsen, leading to ever more stalemates, government shutdowns, and drastic policy swings between parties in power? It can be easy to assume so when looking at the information we’ve presented here that we are heading towards a further polarized outcome.   However, that is certainly not the only possibility. The electoral map is changing, and many red states that were previously untouchable by democrats, like Texas and Georgia, are now very much in play. If bigger states like these switch from red to purple, it could force their incumbent politicians to move towards the center. Similarly, it’s likely that new politicians they elect will be more moderate. This has already been seen with cases like Kirsten Synema in Arizona, a formerly red state that has since turned purple.   This year’s presidential election will also offer clues as to where our political polarization is headed. Voters in the Democratic primary rejected further polarization of their party when they selected moderate Joe Biden to be their nominee. If Joe Biden is able to pull off wins in swing states like Iowa, North Carolina and Pennsylvania, this could also signal a tend away from polarization in the future.  ","categories": ["Data Journalism","Web Scraping","R"],
        "tags": [],
        "url": "/data%20journalism/web%20scraping/r/Polarization-in-the-Senate/",
        "teaser": null
      },{
        "title": "DengAI: Predicting Disease Spread - Imputation and Stationarity Problems",
        "excerpt":" Github Repository    One of the biggest data challenge on DrivenData, with more than 9000 participants is the DengAI challenge. The objective of this challenge is predict the number of dengue fever cases in two different cities.   This blogpost series covers our journey of tackling this problem, starting from initial data analysis, imputation and stationarity problems up un to the different forecasting attempts. This first post covers the imputation and stationarity checks for both cities in the challenge, before moving on to trying different forecasting methdologies.   Throughout this post, code-snippets are shown in order to give an understanding of how the concepts discussed are implemented into code. The entire Github repository for the imputation and stationary adjustment can be found here.   Furthermore, in order to ensure readability we decided to show graphs only for the city San Jose instead showing it for both cities.   Imputation   Imputation describes the process of filling missing values within a dataset. Given the wide range of possibilities for imputation and the severe amount of missing data within this project, it is worthwhile to go over some of the methods and empirically check which one to use.   Overall, we divide all imputation methods into the two categories: basic and advanced. With basic methods we mean off the shelf, quick imputation methods, which are oftentimes already build into Pandas. Advanced imputation methods deal with model-based approaches where the missing values are attempted to be predicted, using the remaining columns.   Given that the model-based imputation methods normally result in superior performance, the question might arise why we do not simply use the advanced method for all columns. The reason for that is that our dataset has several observations where all features are missing. The presence of these observations make multivariate imputation methods impossible.   We therefore divide the features into two categories. All features, or columns, which have fewer than 1 percent missing observations are imputed using more basic methods, whereas model-based approaches are used for features which exhibit more missing observations than this threshold.   The code snippet below counts the percentage of missing observations, divides all features into one of two aforementioned categories and creates a graph to visualize the results.   def pct_missing_data(self, data, cols, threshold, city_name):         \"\"\"         This method does two things. First, it creates a chart showing the         percentages of missing data. Second, it returns which columns         have less than a certain threshold percentage of data, and which         columns have more.          Parameters         ----------         data : DataFrame             DataFrame containing all the data         cols : list             List containing all the columns we have to fill         threshold : int             The threshold which divides the easy and difficult columns         city_name : str             A string containing the city name for which we calculate          Returns         -------         easy_columns : list             List containing the columns which have less then the threshold             missing data         diff_columns : list             List containing the columns which have more than the threshold             missing data          \"\"\"          # Calculating the percentage missing         num_of_obs = len(data)         num_of_nans = data.loc[:, cols].isna().sum()         df_pct_missing = pd.DataFrame(num_of_nans                                       / num_of_obs*100).reset_index()         df_pct_missing.rename(columns={\"index\": \"columns\",                                        0: \"pct_missing\"}, inplace=True)         df_sorted_values = df_pct_missing.sort_values(by=\"pct_missing\",                                                       ascending=False)          # Column division         bool_easy = df_sorted_values.loc[:, \"pct_missing\"] &lt; threshold         easy_columns = df_sorted_values.loc[bool_easy, \"columns\"]         diff_columns = list(set(cols) - set(easy_columns))          # Plotting the data         fig, axs = plt.subplots(figsize=(20, 10))         axs.bar(df_sorted_values.loc[:, \"columns\"],                 df_sorted_values.loc[:, \"pct_missing\"])         axs.tick_params(axis=\"both\", labelsize=16, labelrotation=90)         axs.set_ylabel(\"Percentage of missing observations\", fontsize=18)         axs.set_xlabel(\"Variables\", fontsize=18)         axs.axhline(threshold, linestyle=\"dashed\", color=\"red\",                     label=\"{} percent threshold\".format(threshold))         axs.legend(prop={\"size\": 20})         fig.savefig(r\"{}/{}_miss_data_pct.png\".format(output_path, city_name),                     bbox_inches=\"tight\")          return easy_columns, diff_columns   The resulting graph below shows four features which have more than 1 percent of observations missing. Especially the feature ndvi_ne, which describes satellite vegetation in the north-west of the city has a severe amount of missing data, with more around 20% of all observation missing.      All imputation methods applied are compared using the normalized root mean squared error (NRMSE). We use this quality estimation method because of its capability for making variables with different scales comparable. Given that the NRMSE is not directly implemented in Python, we use the following snippet to implement it.   def nrmse(self, y_true, y_pred, n):     \"\"\"     This function calculates the normalized root mean squared error.      Parameters     ----------     y_true : array         The true values     y_pred : array         The predictions     n : int         The number of rows we testing for performance      Returns     -------     rounded_nrmse : float         The resulting, rounded nrmse      \"\"\"     ts_min, ts_max = np.min(y_true), np.max(y_true)     mse = sum((y_true-y_pred)**2) / n     nrmse_value = np.sqrt(mse) / (ts_max-ts_min)     rounded_nrmse = np.round(nrmse_value, 2)     return rounded_nrmse   Basic imputation methods   Python, and in particular the library Pandas, has multiple off-the-shelf imputation methods available. Arguably the most basic ones are forward fill (ffill) and backward fill (bfill), where we simply set the missing valueequal to the prior value (ffill) or to the proceeding value (bfill).   Other methods include the linear or cubic (the Scipy package also includes higher power if wanted) interpolation around a missing observation.   Lastly, we can use the average of the k nearest neighbours of a missing observations. For this problem we took the preceding and proceeding four observations of a missing observation and imputed it with the average of these eight values. This is not a build in method and therefore defined by us in the following way:   def knn_mean(self, ts, n):       \"\"\"       This function calculates the mean value of the n/2 values before       and after it. This approach is therefore called the k nearest       neighbour approach.        Parameters       ----------       ts : array           The time series we would like to impute       n : int           The number of time period before + after we would like           to take the mean of        Returns       -------       out : array           The filled up time series.        \"\"\"       out = np.copy(ts)       for i, val in enumerate(ts):           if np.isnan(val):               n_by_2 = np.ceil(n/2)               lower = np.max([0, int(i-n_by_2)])               upper = np.min([len(ts)+1, int(i+n_by_2)])               ts_near = np.concatenate([ts[lower:i], ts[i:upper]])               out[i] = np.nanmean(ts_near)       return out   Now it is time to apply and compare of these methods. We do that by randomly dropping 50 observations of all columns, which are afterwards imputed by all before mentioned methods. Afterwards we assess each method’s performance through their NRMSE score. All of that, and the graphing of the results is done through the following code snippet.   def imputation_table(self, data, cols, city_name):         \"\"\"         This method calculates the nrmse for all columns and inserts them         in a table. Additionally a graph is plotted in order for visual         inspection afterwards. The score is calculated by randomly dropping         50 values and then imputing them. Afterwards the performance is         assessed.          Parameters         ----------         data : DataFrame             Dataframe which includes all columns         cols : list             List of columns we would like to impute         city_name : str             In order to know which city data was used, we specify the name          Returns         -------         nrmse_df : DataFrame             The results of each method for each column.          \"\"\"          nrmse_df = pd.DataFrame(index=cols)         print(\"Create imputation table\")         for col in tqdm(cols):              original_series = data.loc[:, col]             time_series = original_series.dropna().reset_index(drop=True)              n = 50             random.seed(42)             rand_num = random.sample(range(0, len(time_series)), n)              time_series_w_nan = time_series.copy()             time_series_w_nan[rand_num] = np.nan              # Forward fill ----             ts_ffill = time_series_w_nan.ffill()             nrmse_df.loc[col, \"ffill\"] = self.nrmse(time_series, ts_ffill, n)              # Backward fill ----             ts_bfill = time_series_w_nan.bfill()             nrmse_df.loc[col, \"bfill\"] = self.nrmse(time_series, ts_bfill, n)              # Linear Interpolation ----             ts_linear = time_series_w_nan.interpolate(method=\"linear\")             nrmse_df.loc[col, \"linear\"] = self.nrmse(time_series,                                                      ts_linear, n)              # Cubic Interpolation ----             ts_cubic = time_series_w_nan.interpolate(method=\"cubic\")             nrmse_df.loc[col, \"cubic\"] = self.nrmse(time_series, ts_cubic, n)              # Mean of k nearest neighbours ----             ts_knn = self.knn_mean(time_series_w_nan, 8)             nrmse_df.loc[col, \"knn\"] = self.nrmse(time_series, ts_knn, n)          # Plotting results         adj_df = nrmse_df.reset_index()         long_format = pd.melt(adj_df, id_vars=[\"index\"], var_name=[\"nrmse\"])         fig, axs = plt.subplots(figsize=(20, 10))         sns.barplot(x=\"index\", y=\"value\", hue=\"nrmse\",                     data=long_format, ax=axs)         axs.tick_params(axis=\"both\", labelsize=16, labelrotation=90)         axs.set_ylabel(\"Normalized Root Mean Squared Root Error\", fontsize=18)         axs.set_xlabel(\"Variables\", fontsize=18)         axs.legend(prop={\"size\": 20})         fig.savefig(r\"{}/{}_imput_performance.png\".format(output_path,                                                           city_name),                     bbox_inches=\"tight\")          return nrmse_df  The resulting graph below clearly shows which method is to be favored, namely the k nearest neighbours approach. The linear method also performs well, even though not as well as the knn method. The more naive methods like ffill and bfill do not perform as strongly.      Afterwards, we impute all features which had fewer observations missing than our threshold of one percent. That means all features except the first four. The code below selects the best method for each column and afterwards imputes all actual missing values.   def fill_by_method(self, original_series, method):     \"\"\"     After we know what the best method is for each column, we would     like to impute the missing values. This function lists all     potential methods, except the model build one.     Parameters     ----------     original_series : array         The original array with all its missing values     method : str         A string describing the best working method     Returns     -------     time_series : array         The original array now filled the missing values with the         method of choice     \"\"\"      if method == \"ffill\":         time_series = original_series.ffill()     elif method == \"bfill\":         time_series = original_series.bfill()     elif method == \"linear\":         time_series = original_series.interpolate(method=\"linear\")     elif method == \"cubic\":         time_series = original_series.interpolate(method=\"cubic\")     elif method == \"knn\":         time_series = self.knn_mean(original_series, 8)     return time_series  def fill_easy_columns(self, data, easy_columns, nrmse_df):     \"\"\"     This method goes through all easy declared columns and fills them     up     Parameters     ----------     data : Dataframe         DataFrame containing all columns     easy_columns : list         List of all columns which can undergo the easy imputation     nrmse_df : DataFrame         Dataframe which contains the performance metrices of         all imputation methods     Returns     -------     data : Dataframe         Dataframe with imputated columns     \"\"\"      print(\"Filling easy columns\")     for col in tqdm(easy_columns):         time_series = data.loc[:, col]         best_method = nrmse_df.loc[col, :].sort_values().index[0]         ts_filled = self.fill_by_method(time_series, best_method)         data.loc[:, col] = ts_filled          assert sum(data.loc[:, col].isna()) == 0, \\             \"Easy imputation went wrong\"     return data   The potential flaws of the knn approach   Unfortunately, the superior performance of the knn model comes with a price. For some features, we do not have a only one observation missing at a time, but multiple consecutively missing observations.   If for example we have 12 consecutive missing observations, the knn method cannot calculate any average out of the preceding and proceeding four observations, given that they are missing as well.   The image below, which was created with the beatiful missingno package, shows us that all four columns which were classified as being above our one percent threshold have at one point 15 consecutive missing observations. This makes it impossible to use the knn method for these columns and is the reason why we cannot use this imputation method for the heavily sparse columns.      Model-based imputation methods   The model-based imputation methods use, as already described earlier, the column with the missing observations as the target and uses all other possible columns as the features. After imputing all columns with fewer than one percent missing observations, we can now use all of them as features.   The model we are using is a RandomForestRegressor because of its good handling of noisy data. The code snippet below which hyperparameters were gridsearched.   imputation_model = {         \"model\": RandomForestRegressor(random_state=28),         \"param\": {             \"n_estimators\": [100],             \"max_depth\": [int(x) for x in                           np.linspace(10, 110, num=10)],             \"min_samples_split\": [2, 5, 10, 15, 100],             \"min_samples_leaf\": [1, 2, 5, 10]             }         }  We now run all four columns through the model-based approach and compare their performance to all aforementioned basic imputation methods. The following code snippet takes care of exactly that.   def fill_diff_columns(self, data, model, diff_columns,                           easy_columns, nrmse_df, city_name):         \"\"\"         This method imputes the difficult columns. Difficult means that         these columns miss more than the specified threshold percentage         of observations. Because of that a model based approach is tried.         If this approach proves better than the normal methods, it is         applied.         Furthermore, we plot the nrmse of the model based approach in order         to compare these with the normal methods         Parameters         ----------         data : DataFrame             Dataframe containing all data         model : dictionary             Here we specify the model and the parameters we would like to try         diff_columns : list             List of columns we would like to try         easy_columns : list             List of columns which have less than the threshold percentage             data missing         nrmse_df : Dataframe             Dataframe with the nrmse for all methods and columns         city_name : str             String specifying which city we are talking about         Returns         -------         data : Dataframe             Dataframe with imputated columns         diff_nrmse_df : Dataframe             Dataframe showing the nrmse performance of the difficult             columns and all methods         \"\"\"         non_knn_method = list(set(nrmse_df.columns) - set([\"knn\"]))         diff_nrmse_df = nrmse_df.loc[diff_columns, non_knn_method]         print(\"Filling difficult columns\")         for col in tqdm(diff_columns):              # Getting data ready             time_series = data.loc[:, col]             non_nan_data = data.dropna(subset=[col])             features = non_nan_data.loc[:, easy_columns]             scaler = StandardScaler().fit(features)             scaled_features = scaler.transform(features)             target = non_nan_data.loc[:, col]              # Model building and evaluation             model_file_name = \"{}/{}_{}_model.pickle\".format(data_path,                                                              city_name,                                                              col)             if not os.path.isfile(model_file_name):                 model_info = model_train(model, scaled_features,                                          target,                                          \"neg_mean_squared_error\",                                          False)                 with open(model_file_name, \"wb\") as file:                     pickle.dump(model_info, file)             else:                 with open(model_file_name, \"rb\") as f:                     model_info = pickle.load(f)             target_min, target_max = np.min(target), np.max(target)             rmse = np.sqrt(abs(model_info[\"scores\"][0]))             nrmse_value = rmse / (target_max-target_min)             diff_nrmse_df.loc[col, \"model\"] = nrmse_value              # Imputing the difficult ones             argmin_method = np.argmin(diff_nrmse_df.loc[col, :])             best_method = diff_nrmse_df.columns[argmin_method]             bool_target_nan = time_series.isna()             if best_method == \"model\":                 features = data.loc[bool_target_nan, easy_columns]                 scaled_features = scaler.transform(features)                 pred = model_info[\"model\"].predict(scaled_features)                 data.loc[bool_target_nan, col] = pred             else:                 pred = self.fill_by_method(time_series, best_method)                 data.loc[bool_target_nan, col] = pred          assert data.loc[:, list(easy_columns) + diff_columns]\\             .isna().any().any() == False, \"Still missing data\"          # Plotting results         adj_df = diff_nrmse_df.reset_index()         long_format = pd.melt(adj_df, id_vars=[\"index\"], var_name=[\"nrmse\"])         fig, axs = plt.subplots(figsize=(20, 10))         sns.barplot(x=\"index\", y=\"value\", hue=\"nrmse\",                     data=long_format, ax=axs)         axs.tick_params(axis=\"both\", labelsize=16, labelrotation=90)         axs.set_ylabel(\"Normalized Root Mean Squared Root Error\", fontsize=18)         axs.set_xlabel(\"Variables\", fontsize=18)         axs.legend(prop={\"size\": 20})         fig.savefig(r\"{}/{}_diff_columns.png\".format(output_path, city_name),                     bbox_inches=\"tight\")          return data, diff_nrmse_df   Below we can see that our work was worthwhile. For three out of four columns we find a superior performance of the model-based approach compared to the basic imputation methods. We are now left with a fully imputed dataset with which we can proceed.      Stationarity Problems - Seasonality and Trend   In contrast to cross-sectional data, time series data comes with a whole bunch of different problems. Undoubtedly one of the biggest issues is the problem of stationarity. Stationarity describes a measure of regularity. It is this regularity which we depend on to exploit when building meaningful and powerful forecasting models. The absence of regularity makes it difficult at best to construct a model.   There are two types of stationarity, namely strict and covariance stationarity. In order for a time series to be fulfil strict stationarity, the series needs to be time independent. That would imply that the relationship between two observations of a series is only driven by the timely gap between them, but not on the time itself. This assumption is difficult, if not impossible for most time series to meet and therefore more focus is drawn on covariance stationarity.   For a time series to be covariance stationary, it is required that the unconditional first two moments, so the mean and variance, are finite and do not change with time. It is important to note that the time series is very much allowed to have a varying conditional mean. Additionally, it is required that the auto-covariance of a time series is only depending on the lag number, but not on the time itself. All these requirements are also stated below.      There are many potential reasons for a time series to be non-stationary, including seasonalities, unit roots, deterministic trends and structural breaks. In the following section we will check and adjust our exogenous variable for each of these criteria to ensure stationarity and superior forecasting behavior.   Seasonality   Seasonality is technically a form of non-stationarity because the mean of the time series is dependent on time factor. An example would be the spiking sales of a gift-shop around Christmas. Here the mean of the time series is explicitly dependent on time.   In order to adjust for seasonality within our exogenous variables, we first have to find out which variables actually exhibits that kind of behavior. This is done by applying a Fourier Transform. A Fourier transform disentangles a signal into its different frequencies and assesses the power of each individual frequency. The resulting plot, which shows power as a function of frequency is called a power spectrum. The frequency with the strongest power could then be potentially the driving seasonality in our time series. More information about Fourier transform and signal processing in general can be read up on an earlier blogpost of ours here.   The following code allows us to take a look into the power-plots of our 20 exogenous variable.   def spike_finder(self, data, cols, city_name):         \"\"\"         This method calculates the power-plots for all specified         variables. Afterwards spikes above a certain threshold and         which exhibit the desired prominence are marked. Afterwards         an image of all columns is saved         Parameters         ----------         data : DataFrame             Dataframe containing all the columns for which we would             like to calculate the power-plots of         cols : list             Columns which we would like to examine         city_name : str             A string denoting which city we are looking at         Returns         -------         spikes_dict : dict             Dictionary which saves the dominant and prominent             frequencies for each exogenous variables         \"\"\"         fig, axs = plt.subplots(nrows=4, ncols=5, figsize=(20, 20),                                 sharex=True)         plt.subplots_adjust(right=None)         fig.add_subplot(111, frameon=False)         plt.tick_params(labelcolor=\"none\", top=False, bottom=False,                         left=False, right=False)         plt.grid(False)         plt.xlabel(\"Frequency [1 / Hour]\", fontsize=22, labelpad=20)         plt.ylabel(\"Amplitude\", fontsize=22, labelpad=50)         spikes_dict = {}         axs = axs.ravel()         for i, col in enumerate(cols):              signal = data.loc[:, col].copy()             fft_output = fft.fft(signal.values)             power = np.abs(fft_output)             freq = fft.fftfreq(len(signal))              mask = freq &gt; 0             pos_freq = freq[mask]             power = power[mask]              axs[i].plot(pos_freq, power)             axs[i].tick_params(axis=\"both\", labelsize=16)             axs[i].set_title(col, fontsize=12)                          relevant_power = power[:int(len(power)/4)]             prominence = np.mean(relevant_power) * 5             threshold = np.mean(relevant_power) + 3 * np.std(relevant_power)             peaks = sig.find_peaks(relevant_power, prominence=prominence,                                    threshold=threshold)[0]             peak_freq = pos_freq[peaks]             peak_power = power[peaks]             axs[i].plot(peak_freq, peak_power, \"ro\")             if len(peak_freq) &gt; 0:                 spikes_dict[col] = (1/peak_freq).tolist()[0]             else:                 spikes_dict[col] = np.nan   The plot below shows the resulting 20 exogenous variables. Whether or not a predominant and significant threshold is met for a variable is indicated by a red dot on top of a spike. If a red dot is visible, that means that the time series has a significantly driving frequency and therefore a strong seasonality component.      One possibility to cross-check the results of the Fourier Transforms is to plot the Autocorrelation function. If we would try have a seasonality of order X, we would expect a significant correlation with lag X. The following snippet of code plots the autocorrelation function for all features and highlights those features which are found to have a seasonal affect according to the Fourier Transform.   def acf_plots(self, data, cols, spike_dict, city_name):     \"\"\"     This method plots the autocorrelation functions for all     specified columns in a specified dataframe. Furthermore,     the biggest possible spike for each column, if there is any,     is made visible through a vertical line and a legend     Parameters     ----------     data : DataFrame         The dataframe which contains all exogenous variables.     cols : list         A list containing the columns which should be         analysed     spike_dict : dict         A dictionary having all columns as the keys and the         potential spike as the value     city_name : str         A string to save the resulting png properly     Returns     -------     None.     \"\"\"     fig, axs = plt.subplots(nrows=4, ncols=5, sharex=True, sharey=True,                             figsize=(20, 20))     plt.subplots_adjust(right=None)     fig.add_subplot(111, frameon=False)     plt.tick_params(labelcolor=\"none\", top=False, bottom=False,                     left=False, right=False)     plt.grid(False)     plt.xlabel(\"Lags\", fontsize=22, labelpad=20)     plt.ylabel(\"Correlation\", fontsize=22, labelpad=50)     axs = axs.ravel()     max_lags = round(np.nanmax(list(spike_dict.values())))     for i, col in enumerate(cols):         series = data.loc[:, col].copy()         sm.graphics.tsa.plot_acf(series.values.squeeze(),                                  lags=max_lags, ax=axs[i], missing=\"drop\")         axs[i].set_title(col, fontsize=12)         axs[i].tick_params(axis=\"both\", labelsize=16)         if not np.isnan(spike_dict[col]):             axs[i].axvline(spike_dict[col], -1, 1, color=\"red\",                            label=\"Periodicity: {}\".format(spike_dict[col]))             axs[i].legend(loc=\"upper center\", prop={'size': 16})     fig.tight_layout()     fig.savefig(r\"{}/{}_autocorrelation_function.png\".format(output_path,                                                              city_name),                 bbox_inches=\"tight\")   From the ACF plots below, we can extract a lot of useful information. First of all, we can clearly see that for all columns where the Fourier transforms find a significant seasonality, we also find confirming picture. This is because we see a peaking and significant autocorrelation at the lag which was found by the power-plot.   Additionally, we find some variables (e.g. ndvi_nw) which exhibit a constant significant positive autocorrelation. This is a sign of non-stationarity, which will be addressed in the next section which will be dealing of stochastic and deterministic trends.      In order to get rid of the seasonal component, we decompose each seasonality-affected feature into its unaffected version its seasonality component and trend component. This is done by the STL decomposition which was developed by Cleveland, McRae &amp; Terpenning (1990). STL is an acronym for “Seasonal and Trend decomposition using Loess”, while Loess is a method for estimating non-linear relationships.   The following code snippet decomposes the relevant time series, and subtracts (given that we face additive seasonalities) the seasonality and the trend from the time series.   def season_trend(self, data, cols, spike_dict):         \"\"\"         This method decomposes the time series by removing         (subtracting) the modelled seasonality and trend.         Parameters         ----------         data : DataFrame             A dataframe containing the relevant time series         cols : list             A list which specifies all potentially affected columns         spike_dict : dict             A dictionary stating the significant seasonality for             each column         Returns         -------         data : Dataframe             After decomposing and 'cleaning', we put the variables             back into the dataframe which is returned         \"\"\"         for col in cols:             period = spike_dict[col]             time_series = data.loc[:, col]             if not np.isnan(period):                 res = STL(time_series, period=int(spike_dict[col])+1).fit()                 adjusted_series = time_series - res.seasonal                 data.loc[:, col] = adjusted_series          return data   Deterministic Trends   One more obvious way to breach the assumptions of covariance stationarity is if the series has a deterministic trend. It is important to stress the difference between a deterministic and not a stochastic trend (unit root). Whereas it is possible to model and remove a deterministic trend, this is not possible with a stochastic trend, given its unpredictable and random behavior.   A deterministic trend is the simplest form of a non-stationary process and time series which exhibit such a trend can be decomposed into three components:      The most common type of trend is a linear trend. It is relatively straight forward to test for such a trend and remove it, if one is found. We apply the original Mann-Kendall test, which does not consider seasonal effects, which we already omitted in the part above. If a trend is found, it is simply subtracted from the time series. These steps are completed in the method shown below.   def trend_detecter(self, data, cols):       \"\"\"       This method tests for a deterministic trend using the       Mann-Kendall test. If the test is found to be significant,       the trend is removed (subtracted).       Parameters       ----------       data : DataFrame           A dataframe containing all the relevant columns       cols : list           A list of column names for which we apply the test       Returns       -------       no_nan_data : DataFrame           A dataframe with the potentially removed trend series       trend_dict : dict           A dictionary containing the information of the detrending       \"\"\"       trend_dict = {}       for col in cols:           trend_dict[col] = {}            time_series = data.loc[:, col]           result = mk.original_test(time_series)           trend_dict[col][\"pre_trend\"] = result.trend            if result.trend != \"no trend\":               d_trend = [(result.intercept + result.slope * x)                          for x in np.arange(len(time_series))]               trend_dict[col][\"intercept\"] = result.intercept               trend_dict[col][\"slope\"] = result.slope                adj_time_series = time_series - d_trend               result = mk.original_test(adj_time_series)               trend_dict[col][\"post_trend\"] = result.trend               data.loc[:, col] = adj_time_series        no_nan_data = data.dropna(subset=cols).reset_index(drop=True)       return no_nan_data, trend_dict   The result can be viewed here. As we can see, most time series exhibited a linear trend, which was then removed.   | ndvi_ne    | ndvi_nw                | ndvi_se                 | ndvi_sw  | precipitation_amt_mm    | reanalysis_air_temp_k | reanalysis_avg_temp_k | reanalysis_dew_point_temp_k | reanalysis_max_air_temp_k | reanalysis_min_air_temp_k | reanalysis_precip_amt_kg_per_m2 | reanalysis_relative_humidity_percent | reanalysis_sat_precip_amt_mm | reanalysis_specific_humidity_g_per_kg | reanalysis_tdtr_k     | station_avg_temp_c     | station_diur_temp_rng_c | station_max_temp_c     | station_min_temp_c      | station_precip_mm     |                     | |------------|------------------------|-------------------------|----------|-------------------------|-----------------------|-----------------------|-----------------------------|---------------------------|---------------------------|---------------------------------|--------------------------------------|------------------------------|---------------------------------------|-----------------------|------------------------|-------------------------|------------------------|-------------------------|-----------------------|---------------------| | Trend      | decreasing             | decreasing              | no trend | decreasing              | decreasing            | increasing            | increasing                  | increasing                | increasing                | increasing                      | decreasing                           | decreasing                   | decreasing                            | increasing            | increasing             | increasing              | decreasing             | decreasing              | increasing            | increasing          | | Slope      | -8.804180148944889e-05 | -0.00010508442895612228 | NA       | -1.9969121004566154e-05 | -0.003975390783667321 | 0.00079050539377047   | 0.0007647709685904174       | 0.0004375871000444891     | 0.000685165752960625      | 0.0006468026767682056           | -0.012109224127889808                | -0.0017669399624736161       | -0.003975390783667321                 | 0.0004258943155599426 | 0.00032172583198841135 | 0.0001547004468416762   | -0.0009195402298850562 | -0.00030887303593728855 | 0.0006413032891119354 | 0.00986137183746052 | | Intercept  | 0.10424117470021918    | 0.12204044630128306     | NA       | 0.17419584980022826     | 31.7450108145569      | 298.76309596706034    | 298.8943108197956           | 294.9033123255635         | 301.01619501182535        | 297.00822681183297              | 32.6573650987486                     | 79.43547713504724            | 31.7450108145569                      | 16.345457478691138    | 2.292595026925666      | 26.96210500459904       | 7.149425287356321      | 31.81129219530351       | 22.338233161530237    | 19.02484232581643   | | Post Trend | no trend               | no trend                | NA       | no trend                | no trend              | no trend              | no trend                    | no trend                  | no trend                  | no trend                        | no trend                             | no trend                     | no trend                              | no trend              | no trend               | no trend                | no trend               | no trend                | no trend              | no trend            |   Even though we removed a deterministic trend, this did not ensure that our time series are actually stationary now. That is because what works for a deterministic trend does not work for a stochastic trend, meaning that the trend-removing we just did does not ensure stationary of unit-roots.   We therefore have to explicitly test for a unit-root in every time series.   Stochastic Trends - Unit roots   A unit root process is the generalization of the classic random walk, which is defined as the succession of random steps. Given this definition, the problem of estimating such a time series are obvious. Furthermore, a unit root process violates the covariance stationarity assumptions of not being dependent on time.   To see why that is the case, we assume an autoregressive model where today’s value only depends on yesterday’s value and an error term.      If we parameter a_1 would now be equal to one, the process would simplify to      By repeated substitution we could also write this expression as:      When now calculating the variance of y_t, we face a variance which is positively and linearly dependent on time, which violates the second covariance stationarity rule.      This would have not been the case if a_1 would be smaller than one. That is also basically what is tested in an unit-root test. Arguably the most well-known test for an unit root is the Augmented Dickey Fuller (ADF) test. This test has the null hypothesis of having a unit root present in an autoregressive model. The alternative is normally that the series is stationary or trend-stationary. Given that we already removed a (linear) trend, we assume that the alternative is a stationary series.   In order to be technically correct, it is to be said that the ADF test is not directly testing that a_1 is equal to zero, but rather looks at the characteristic equation. The equation below illustrates what is meant by that:      We can see that the difference to the equation before is that we do not look at the level of y_t, but rather at the difference of y_t. Capital Delta represent here the difference operator. The ADF is now testing whether the small delta operator is equal to zero. If that would not be the case, then the difference between yesterday’s and tomorrow’s value would depend on yesterday’s value. That would mean if the today’s value is high, the difference between today’s and tomorrow’s value will also be large which is a self-enforcing and explosive process which clearly depends on time and therefore breaks the assumptions of covariance stationarity.   In case of a significant unit-root (meaning a pvalue above 5%), we difference the time series as often as necessary until we find a stationary series. All of that is done through the following two methods.   def dickey_fuller_test(self, data, cols):     \"\"\"     Method to test certain rows from a dataframe whether     an unit root is present through the ADF test     Parameters     ----------     data : Dataframe         A dataframe which contains all series we would like to         test     cols : list         A list containing all columns names for which we would         like to conduct the test for.     Returns     -------     adf_dict : dict         Dictionary containing the test result for every series.     \"\"\"     adf_dict = {}     for col in cols:         time_series = data.loc[:, col].dropna()         result = adfuller(time_series, autolag=\"AIC\", regression=\"c\")         adf_dict[col] = result[1]     return adf_dict  def diff_sign_columns(self, data, cols, adf_dict):     \"\"\"     This method differences the time series if a non significant     dickey fuller test is shown. This is done as long as the     adf is not significant.     Parameters     ----------     data : Dataframe         A dataframe containing all the time series we would like to test     cols : list         List of column names we would like to test     adf_dict : dict         dictionary containing the test results of the dickey fuller test     Returns     -------     data : DataFrame         A dataframe with the now potentially differenced series     adf_dict : dict         A dictionary with the now significant dickey fuller tests     number_of_diff : dict         A dictionary telling how often each series was differenced.     \"\"\"     number_of_diff = {}     for col in cols:         pvalue = adf_dict[col]         time_series = data.loc[:, col].dropna()         while pvalue &gt; 0.05:             time_series = time_series.diff(periods=1)             pvalue = adfuller(time_series.dropna(),                               autolag=\"AIC\",                               regression=\"c\")[1]             number_of_diff[col] = sum(time_series.isna())         adf_dict[col] = pvalue         data.loc[:, col] = time_series      return data, adf_dict, number_of_diff   The following table shows that we do not find any significant ADF test, meaning that no differencing was needed and that no series exhibited a significant unit root.   Finishing up  Last but not least we take a look at our processed time series. It is nicely visible that none of the time series are trending anymore and they do not exhibit significant seasonality anymore.      Additionally we take a look at how the distributions of all of the series look. It is important to note that there are no distributional assumptions of the feature variables when it comes to forecasting. That means that even if we find highly skewed variables, it is not necessary to apply any transformation.      After sufficiently transforming all exogenous variables, it is now time to shift our attention on the forecasting procedure of both cities.  ","categories": ["Time Series","Python"],
        "tags": [],
        "url": "/time%20series/python/DengAI-Predicting-Disease-Spread-Imputation-and-Stationarity-Problems/",
        "teaser": null
      },{
        "title": "How to create Country Heatmaps in R",
        "excerpt":" One of the most powerful visualization tools for regional Panel data there is.    Much of the data that we interact with in our daily lives has a geographical component. Google maps records our favorite places, we calculate how many customers frequent each location for a given brand shop, and we compare eachother by regional difference. Next to bar-charts and line-charts, are there better ways that we can visualize geospatial data? A geographical heatmap can be a powerful tool for displaying temporal changes over geographies.   The question is how to build such a visualization. This tutorial explains how to build a usable GIF from panel data- time series for multiple entities, in this case German states. The visualization problem to which we apply this heatmap GIF is displaying data about the economic development (change in GDP per Capita) of Germany between 1991 and 2019. The data for this example can be found here on our GitHub.   Data Import and Basic Cleaning   To begin with, we import the relevant packages, define the relevant paths and import the needed data. Note that next to the GDP (referred to as BIP in german) per state, we also import the annual inflation rate for Germany using Quandl. The Quandl API provides freely usable financial and economic datasets. In order to use the API we have to create an Account and generate an API-Key, which then has to be specified in the data-generating command.   The reason we need the inflation rate of Germany over the years is that the GDP per Capita is currently in nominal terms, which means that it is not adjusted for inflation and therefore non-comparable between years.   # Packages library(\"readxl\") library(Quandl) library(dplyr) library(tidyverse) library(sp) library(raster) library(ggplot2) library(ggthemes)  # Paths main_path = \"/Users/paulmora/Documents/projects/germany_bip_state\" raw_path = paste(main_path, \"/00 Raw\", sep=\"\") code_path = paste(main_path, \"/01 Code\", sep=\"\") data_path = paste(main_path, \"/02 Data\", sep=\"\") output_path = paste(main_path, \"/03 Output\", sep=\"\")  # Importing data bip_per_state = read_excel(paste(raw_path, \"/bip_states.xlsx\", sep=\"\"),                            skip=10) german_inflation = Quandl(\"RATEINF/CPI_DEU\", api_key=\"XXXXXXXX\", # Insert API-Key here                           collapse=\"annual\")   After importing our data, it is now time for some basic data cleaning. This involves reshaping the data, creating the inflation adjusted GDP per Capita figures and creating an unique identifier for each state.   Why exactly reshaping the data is required can be better explained by looking at the raw dataframe, which contains the currently nominal GDP per Capita figures.      The data representation we face is referred to as a wide-format. A wide-format means that one variable (in that case the year information) is used as columns. Therefore, the entire dataset is much wider than if the variable year would be represented by only one column. This would then lead to a much longer dataset, which is why this format is referred to as the long-format. The change from a wide-format to a long-format is necessary because of our plotting tool ggplot, which could be seen as R’s equivalent to Python’s Seaborn.   Inflation      After knowing why and how we have to reshape the GDP per Capita dataset, it is now time to elaborate on how to process the inflation data. For that we briefly cover what Inflation is and why it is needed.   Inflation describes the measure of the rate by which the average price level increases over time. A positive inflation rate would imply that the average cost of a certain basked of goods increased over time and that we can buy fewer things with the same amount of money. That implies that 100 Euros in 2019 can buy us less than the same amount in 1991. To make these number comparable nevertheless, we have to adjust them for inflation.   The way we measure inflation is through something which is referred to as the Consumer-Price-Index (CPI). This index represents the price of a certain basket of goods at different points in time. From the data-frame on the left side we can see that this basket cost 67.2 in 1991 and 105.8 in 2020. Given that our GDP per Capita data only ranges until 2019, and that a CPI index is normalized to a certain year, we divide all values of this data-frame by the CPI value in 2019.      The result of dividing all CPI values by the index-level of 2019 can be seen in the data-frame on the left. Now these values are easier to interpret. An average product in 1991 cost 63.5% of what it cost in 2019.   We can use these values now to adjust the GDP per Capita values over time in order to make them comparable over the years. This is done by simply dividing all GDP values by the respective inflation value of a given year.   Additionally we extract the year information from the Date column in order to match with the year information from the GDP per Capita sheet.   All of the described steps of reshaping and handling inflation are done through the code snippet below.   bip_per_state$id = seq(nrow(bip_per_state)) reshaped_data = gather(bip_per_state, year, gdp, \"1991\":\"2019\",                        factor_key=TRUE)  german_inflation$year = substr(german_inflation$Date, 0, 4) german_inflation$benchmarked_inflation = (german_inflation$Value                                           / max(german_inflation$Value)) subset_inflation = german_inflation %&gt;%   dplyr::select(year, benchmarked_inflation) gdp_data = merge(reshaped_data,                  subset_inflation,                  by=c(\"year\")) gdp_data$real_gdp_per_capita = gdp_data$gdp / gdp_data$benchmarked_inflation   Importing Geospatial data for Germany   After cleaning our data and bringing it into a ggplot-friendly long-format, it is now time to import geospatial data of Germany. Geospatial or spatial data contains information needed to build a map of a location, in this case Germany.   Importing that information is done through the handy getData function. This function takes, amongst other things, the country name and the level of granularity as an input. In our case we specify “Germany” as the country and because we are interested not only in the country as a whole, but rather in the different states we specify a granularity level which also gives State information. The API gives us a so called Large SpatialPolygonsDataFrame. When opened this object looks like this:      We can see that we find 16 polygons. This makes sense given that we have 16 states in Germany. Each state is therefore represented by one Polygon.   It is important to note that the order of these 16 Polygons does not necessarily align with the alphabetical order of the German states. Therefore, we have to make sure that the information of GDP per Capita is matched up with the right Polygon. Otherwise it could be that we plot the information for e.g. Berlin in Hamburg or vice versa.   Lastly, we have to bring the information into a data frame into the long-format that ggplot prefers. This is done through the broom package, which includes the tidy function. This function, like the entire package, is for tidying up messy datatypes and bring them into a workable format.   The code for the aforementioned steps looks the following:    map_germany = getData(country=\"Germany\", level=1)  from_list = c() for (i in 1:(nrow(bip_per_state)-1)) {   id_num = map_germany@polygons[[i]]@ID   from_list = append(from_list, id_num) }  to_list = seq(1:(nrow(bip_per_state)-1)) map = setNames(to_list, from_list)  spatial_list = broom::tidy(map_germany) spatial_list$id = map[spatial_list$id]   The resulting data frame below now shows us all the desired information. The longitude and latitude information (long and lat) is needed for ggplot to know the boundaries of a state and the id column tells us which state we are looking at.      The only remaining steps are to merge the GDP per Capita onto the mapping information and plotting it all.   Plotting   Our final plotting code has to be fine-tuned to fit the purpose of the visualization. In our case, we would like to have one heatmap for every year between 1991 and 2019. To achieve that one could run the plotting code in a loop, iterating over all years, which is also what we will do later on. For better readability though, we start by showing how to plot a single year.   We start by subsetting the GDP per Capita data frame so that it only contains information for a certain year (e.g. 1991). Afterwards, we merge these 16 rows for this single year onto the mapping data frame we showed earlier. The merging variable is the id column we discussed earlier. Afterwards we can already call ggplot and fine-tune our plot.   Germany has two states (Bremen and Berlin) which are fully embedded in other, bigger states. It could therefore happen that the bigger state around them is simply plotted over the smaller state. Therefore it is important to tell ggplot that the bigger states should be plotted before these smaller states, which is also shown in the upcoming code snippet (line 7–11).   gdp_per_year = gdp_data %&gt;% filter(year==1991) spatial_data_per_year = merge(spatial_list, gdp_per_year, by=\"id\")  gdp_plotting = spatial_data_per_year %&gt;%   ggplot(aes(x = long, y = lat, group = group)) +   geom_polygon(aes(fill=real_gdp_per_capita), color=\"white\",                data=filter(spatial_data_per_year,                            !Bundesland %in% c(\"Berlin\", \"Bremen\"))) +   geom_polygon(aes(fill=real_gdp_per_capita), color=\"white\",                data=filter(spatial_data_per_year,                            Bundesland %in%  c(\"Berlin\", \"Bremen\"))) +   theme_tufte() +   coord_fixed() +   scale_fill_gradient(name=\"Real Euros per Capita\",                       limits=c(overall_min, overall_max),                       guide=guide_colourbar(barheight=unit(80, units=\"mm\"),                                             barwidth=unit(5, units=\"mm\"),                                             draw.ulim=F,                                             title.hjust=0.5,                                             label.hjust=0.5,                                             title.position=\"top\")) +   coord_map() +   ggtitle(paste(\"GDP Per Capita -\", year_num)) +   theme(axis.title.x=element_blank(),         axis.text.x=element_blank(),         axis.ticks.x=element_blank(),         axis.title.y=element_blank(),         axis.text.y=element_blank(),         axis.ticks.y=element_blank(),         legend.title=element_text(size=25),         legend.text=element_text(size=20),         legend.position=\"right\",         plot.title = element_text(lineheight=.8, face=\"bold\", size=45)) +   ggsave(paste(output_path, \"/maps/\", year_num, \".png\", sep=\"\"))   The code above will now generate and save the following image:      Putting all together into a GIF   If we would now even like to include a time component we could also create multiple heatmaps, one for every year, and play them one after another through a GIF. For that we make use of the beautiful ImageMick package which simply takes all available images in a specified folder and converts them into a GIF.   The following snippet of code shows the procedure to first create heatmaps for all years before turning them into a GIF like it can be seen below.   for (year_num in unique(gdp_data$year)) {    gdp_per_year = gdp_data %&gt;% filter(year==year_num)   spatial_data_per_year = merge(spatial_list, gdp_per_year, by=\"id\")    gdp_plotting = spatial_data_per_year %&gt;%     ggplot(aes(x = long, y = lat, group = group)) +     geom_polygon(aes(fill=real_gdp_per_capita), color=\"white\",                  data=filter(spatial_data_per_year,                              !Bundesland %in% c(\"Berlin\", \"Bremen\"))) +     geom_polygon(aes(fill=real_gdp_per_capita), color=\"white\",                  data=filter(spatial_data_per_year,                              Bundesland %in%  c(\"Berlin\", \"Bremen\"))) +     theme_tufte() +     coord_fixed() +     scale_fill_gradient(name=\"Real Euros per Capita\",                         limits=c(overall_min, overall_max),                         guide=guide_colourbar(barheight=unit(80, units=\"mm\"),                                               barwidth=unit(5, units=\"mm\"),                                               draw.ulim=F,                                               title.hjust=0.5,                                               label.hjust=0.5,                                               title.position=\"top\")) +     coord_map() +     ggtitle(paste(\"GDP Per Capita -\", year_num)) +     theme(axis.title.x=element_blank(),           axis.text.x=element_blank(),           axis.ticks.x=element_blank(),           axis.title.y=element_blank(),           axis.text.y=element_blank(),           axis.ticks.y=element_blank(),           legend.title=element_text(size=25),           legend.text=element_text(size=20),           legend.position=\"right\",           plot.title = element_text(lineheight=.8, face=\"bold\", size=45)) +     ggsave(paste(output_path, \"/maps/\", year_num, \".png\", sep=\"\")) }  setwd(paste(output_path, \"/maps/\", sep=\"\")) system(\"convert -delay 80 *.png maps_over_time.gif\") file.remove(list.files(pattern=\".png\"))      By tweaking these code snippets, you can create your own geographical heatmaps for other visualization challenges.  ","categories": ["Tutorial","R"],
        "tags": [],
        "url": "/tutorial/r/How-to-create-Country-Heatmaps-in-R/",
        "teaser": null
      },{
        "title": "DengAI: Predicting Disease Spread — STL Forecasting/ ARIMA/ Box-Jenkins",
        "excerpt":"Using the STL Forecasting Method with an ARIMA model, which is parameterized through the Box-Jenkins Method.   This post builds on our first blogpost which dealt with the initial data-transformation of the exogenous variables. Now we build a first model using only target variable itself. This is done by using the STL Forecasting method. This allows us to model time series which are affected by seasonal effects, by first removing the specified seasonality through a STL decomposition and then modeling the deseasonalized time series with a model of our choice — which is an Autoregressive Integrated Moving Average Model (ARIMA).   For those not familiar with the forecasting challenge, this competition deals with the prediction of dengue fever in two cities, or in the words from DrivenData themselves:    Your goal is to predict the total_cases label for each (city, year, weekofyear) in the test set. There are two cities, San Juan and Iquitos, with test data for each city spanning 5 and 3 years respectively. You will make one submission that contains predictions for both cities.    In order for better readability, this blogpost only shows and discusses graphs from the city San Juan. Further, the related code for every graph is found directly below each graph. Additionally the entire code (for both cities) is found at the bottom of this blogpost, or on GitHub.   In the center of our prediction approach is the aforementioned so-called STL method. STL stands for Seasonal-Trend decomposition method using LOESS. This method decomposes a time series into three components, namely its trend, its seasonality and its residuals. LOESS stands for locally estimated scatterplot smoothing and is extracting smooth estimates of the three aforementioned components.   Structural Changes   Looking at the time series of dengue fever in San Juan, several things are worth pointing out. For once there is a strong seasonal component visible. In the first half of the data it looks like the seasonal pattern is yearly (52 weeks), whereas in the second half of the time series the pattern somewhat switched to a two-yearly pattern. The other predominant characteristic of the time series are the few but stark spikes. Especially in the first half of the time series we see two outbursts in the target variable of a magnitude which is unparalleled in the second half.      def plot_cutoff_comparison(time_series, cutoff, city_name):       first_half = time_series[:cutoff]     first_half.name = \"First Half\"     second_half = time_series[cutoff:]     second_half.name = \"Second Half\"     fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 10))     axs[0].plot(time_series, label=\"Complete Series\")     axs[0].axvline(cutoff, color=\"r\", linestyle=\"--\", label=\"Cutoff\")     axs[1].plot(second_half,                 label=\"Series from observation {} onwards\".format(cutoff))     axs = axs.ravel()     for axes in axs.ravel():         axes.legend(prop={\"size\": 16}, loc=\"upper right\")         axes.tick_params(axis=\"both\", labelsize=16)     fig.tight_layout()     fig.savefig(r\"{}/{}/{}_cutoff_plot.png\".format(output_path,                                                    approach_method,                                                    city_name),                 bbox_inches=\"tight\")     return first_half, second_half   At this point it is important to stress the higher importance of newer data compared to older data within time series problems. Normally within data science projects, we are very always interested in gathering more data, as this would in general increase the robustness and accuracy of our model. Though, when it comes to time series data, we have to weight the importance of new data on when exactly the data was sampled. If we are for example interested in predicting the stock price for a certain company, the company’s financial statements of the last couple of years, are undoubtely more important than the company’s performance 100 years ago. Why is that so? Because it could be possible that underlying data generating process changed over time. That would imply that older data is not giving us any information about future data and is therefore not only incorrect, but could also hurt our model performance.   The three charts below visualize the differences between the first and second half of the time series.      def difference_in_distribution(series1, series2, city_name):      # CDF     def ecdf(data):         \"\"\" Compute ECDF \"\"\"         x = np.sort(data)         n = x.size         y = np.arange(1, n+1) / n         return x, y      test_results = pd.DataFrame(index=[\"KS 2 Sample Test\", \"ANOVA\"],                                 columns=[\"Statistic\", \"P Value\"])     test_results.iloc[0, :] = st.ks_2samp(series1, series2)     test_results.iloc[1, :] = st.f_oneway(series1, series2)      fig, axs = plt.subplots(ncols=3, figsize=(40, 15))      # Time series     axs[0].plot(np.arange(len(series1)), series1, color=\"b\",                 label=series1.name)     axs[0].plot(np.arange(len(series2)), series2, color=\"r\",                 label=series2.name)     axs[0].set_title(\"Level Data\", fontsize=40)     axs[0].legend(prop={\"size\": 30})      # Boxplots     axs[1].boxplot([series1, series2])     axs[1].set_title(\"Boxplots\", fontsize=40)     axs[1].set_xticklabels([series1.name, series2.name],                            fontsize=30,                            rotation=45)      x, y = ecdf(series1)     axs[2].scatter(x, y, color=\"b\", label=series1.name)     x, y = ecdf(series2)     axs[2].set_title(\"Empirical Cumulative Distribution Function\",                      fontsize=40)     axs[2].scatter(x, y, color=\"r\", label=series2.name)     axs[2].legend(prop={\"size\": 30})      for ax in axs.ravel():         ax.tick_params(axis=\"both\", labelsize=30)      fig.tight_layout()     fig.savefig(r\"{}/{}/{}_cutoff_plot.png\".format(output_path,                                                    approach_method,                                                    city_name),                 bbox_inches=\"tight\")      return test_results   Next to individual visual judgement, there are also several tests to formally check whether the distribution of the data is different, namely the Chow Test, the Kolmogrov Smirnov 2-Sample test, and an Analysis of Variance (ANOVA). For the latter two we find the test results between the first half and the second half of the data below.      The significance test results confirm our initial hypothesis of a structural difference in the data generating process between the first and second half of the data.   Though it is important to note that only because the data is found to be different, we do not necessarily have to discard all observations from the first half. Instead we will winsorize and use the seasonality found in the second half of the data.   Winsorizing   Winsorizing presents a valid alternative compared to simply dropping the data. This method is used when instead of throwing out potential outliers, we would like to keep them but alter their magnitude. This is done by specifying by much a potential outlier should be adjusted. A winsorizing value of X% for example means that all values which are higher than the (100-X) percentile are set to the value of the (100-X) percentile. A more thorough explanation of the process can be found here.   The chart below shows the impact of winsorizing our data to the 2.5% level (a value that is chosen to equalize the outbursts in the first half of the data). When looking at the scale of the y-axis the effect of the measure becomes apparent, the magnitude of the outliers has dampened.      def winsorizer(time_series, level, city_name):      decimal_level = level / 100     wind_series = st.mstats.winsorize(time_series, limits=[0, decimal_level])     fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 10))     axs[0].plot(time_series, label=\"Original Time Series\")     axs[1].plot(wind_series,                 label=\"Winsorized at the {}% level\".format(level))     axs = axs.ravel()     for axes in axs.ravel():         axes.legend(prop={\"size\": 16}, loc=\"upper right\")         axes.tick_params(axis=\"both\", labelsize=16)     fig.tight_layout()     fig.savefig(r\"{}/{}/{}_win.png\".format(output_path,                                            approach_method,                                            city_name),                 bbox_inches=\"tight\")     return wind_series   Of course this does measure does not come without any risks. Winsorizing, or outlier altering in general, implies that we do not expect to see levels as high as the altered values. Given that the second half of the time series, which spans eight years of data, does not exhibit outbursts in any comparable magnitude, we feel confident to proceed with the winsorization.   Seasonality Detection   When using a STL decomposition, we need to specify which periodicity the seasonality is supposed to have. In order to find that out, we can either specify an appropriate time index and let the frequency be inferred automatically, or we can look at the autocorrelation function (ACF) of the time series. The ACF shows us the correlation between time series observations and observations with various lagged version of the very same time series. A high autocorrelation with the first lag would for example describe a process where the value today is very much alike the value yesterday. Following the same logic, it is also possible to spot a potential seasonality in the data. Namely, by finding a timely reoccurring amplitude in the ACF of a time series.      Below we can see in the upper plot the actual target variable, namely the number of Dengue Fever cases in San Juan. The plot underneath shows the discussed autocorrelation function of that time series. The ACF plot indicates that initially (up until lag 400) we find a yearly pattern. That means that the amplitude is occurring at multiple of 52 (52 weeks equal a year). Beyond lag 400 we find a different pattern though. Something that resembles rather a two-year seasonality. Given the aforementioned higher importance of more recent data, we will therefore continue with a 104 week (or 2 year) seasonality.      def acf_plots(y, max_lags, city_name):     fig, axs = plt.subplots(nrows=2, figsize=(20, 10))     sm.graphics.tsa.plot_acf(y.values.squeeze(),                              lags=max_lags, ax=axs[1], missing=\"drop\")     axs[1].set_title(\"Autocorrelation Plot\", fontsize=18)     axs[0].set_title(\"Original Time Series\", fontsize=18)     axs[0].plot(y)     for ax in axs.ravel():         ax.tick_params(axis=\"both\", labelsize=16)     fig.tight_layout()     fig.savefig(r\"{}/{}/{}_raw_acf.png\".format(output_path, # Individual output path                                                approach_method, # Individual folder                                                city_name),                 bbox_inches=\"tight\")   Autoregressive Integrated Moving Average Model   Next in line is the parameterization of the ARIMA model. For that we have to remember that the STL Forecasting function is applying the prediction model on the deasonalized time series data, and not on the raw data. We therefore have to deseasonalize our data before examining it. This is by using the STL decomposition, specifying aforementioned 14 as our period.   In the image below we can see the original series at the very top, followed by the seasonality and the difference between the original series and the seasonality. It is the latter time series we will use to identify the appropriate paramerterization of the ARIMA model.      def stl_decomposing(y, period, city_name):     time_series = y.copy()     res = STL(time_series, period=period, robust=True).fit()     fig, axs = plt.subplots(3, 1, figsize=(30, 20))     time_series.plot(ax=axs[0], label=\"Original Series\")     time_series_wo_season = time_series - res.seasonal     res.seasonal.plot(ax=axs[1])     time_series_wo_season.plot(ax=axs[2])     axs[0].set_title(\"Original Series\", fontsize=30)     axs[1].set_title(\"Seasonality\", fontsize=30)     axs[2].set_title(\"Original Series Minus Seasonality\", fontsize=30)     for ax in axs.ravel():         ax.tick_params(axis=\"both\", labelsize=25)     fig.tight_layout()     fig.savefig(r\"{}/{}/{}_decomposed_acf.png\".format(output_path,                                                       approach_method,                                                       city_name),                 bbox_inches=\"tight\")      return time_series_wo_season   Given that the deasonalized time series does not suffer from any kind of non-stationarity, we do not have to worry about the number of integration within the ARIMA model, since none is necessary. The bigger question is how many autoregressive (AR) and moving averages (MA) lags are necessary. In practice the lag length of the AR and MA processes are denoted by the letter p and q respectively.   One well-known approach for specifying the optimal lag length of AR and MA processes is the so-called Box-Jenkins method. This method involves three steps:      After ensuring stationarity, use plots of the autocorrelation function (ACF) and the partial autocorrelation function (PACF) to make an initial guess of the lag length for the AR and MA process   Use the parameters gauged from the first step and specify a model   Lastly check the residuals of the fitted model for serial correlation (independence of each other) and for stationarity. The former can be achieved using a Ljung-Box test. If the specified model fails these tests, go back to step 2 and use slightly different parameter for p and/or q   Following the three steps outlined above, we start by plotting the autocorrelation function and the partial autocorrelation function. These can be seen in the graph below.   A partial autocorrelation is the amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower-order-lags. That is done by regressing the time series with all n-1 lags and therefore controlling for them when assesing the n-th partial correlation. It contrasts with the autocorrelation function, which does not control for other lags.      def acf_pacf_plots(time_series, nlags, city_name):      # Plotting results     no_nan_time_series = time_series.dropna()     fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)     sm.graphics.tsa.plot_acf(no_nan_time_series,                              lags=nlags, fft=True, ax=axs[0])     sm.graphics.tsa.plot_pacf(no_nan_time_series,                               lags=nlags, ax=axs[1])     for ax, title in zip(axs.ravel(), [\"ACF\", \"PACF\"]):         ax.tick_params(axis=\"both\", labelsize=16)         ax.set_title(title, fontsize=18)         ax.set_title(title, fontsize=18)     fig.tight_layout()     fig.savefig(r\"{}/{}/{}_acf_pacf.png\".format(output_path,                                                 approach_method,                                                 city_name),                 bbox_inches=\"tight\")   The way of how to identify the appropriate lag length from the two plots above is summarized in the following table:      From that we can see that the appropriate lag length of the AR process is the number of initial non-zero lags in the partial autocorrelation function. Looking at the plot above, we can see that this should be around 6, given that the seventh lag lies within the boundaries of the 95% confidence interval and is therefore not significant. It is to be noted that the very first vertical line represent the partial autocorrelation with itself and should be not counted.   The number of MA processes is taken by counting the number of initial non-zero terms within the autocorrelation function. In our example, we find rather many, namely around 20. It is important to note that the exact number is not too important as will empirically check a multitude of models before deciding for one.   It is important to stress that the Box-Jenkins method builds on the parsimony principal. Quoting the Oxford econometrician Kevin Sheppard:    Parsimony is a property of a model where the specification with the fewest parameters capable of capturing the dynamics of a time series is preferred to other representations equally capable of capturing the same dynamics.    That means that if we find multiple models which meet our conditions regarding serial correlation, we will choose the model with the smallest amount of lags. Therefore we check the results of the Ljung-Box serial correlation test not only for the lag lengths 6 and 20 (which is already way too many), but for all smaller lengths as well. The p-values of the Ljung-Box tests are captured in the image below.   It is visible that nearly all model parameterization find a significant Ljung-Box test, signaling serious serial correlation. Luckily, we also find some model parameterizations with non-significant serial correlation within the residuals. Applying the parsimony principle leads us then to take the AR-2 MA-2 model.      def box_jenkins_lb_finder(y, p, q, year_in_weeks, city_name):     lb_df = pd.DataFrame(columns=[\"MA_{}\".format(x) for x in range(1, q)],                          index=[\"AR_{}\".format(x) for x in range(1, p)])     for i in range(1, p+1):         for j in range(1, q+1):             stlf = STLForecast(y, ARIMA, period=year_in_weeks, robust=True,                                model_kwargs=dict(order=(i, 0, j), trend=\"c\"))             stlf_res = stlf.fit()             results_as_html = stlf_res.summary().tables[2].as_html()             results_df = pd.read_html(results_as_html, index_col=0)[0]             lb_df.loc[\"AR_{}\".format(i),                       \"MA_{}\".format(j)] = results_df.iloc[0, 0]      # Create heatmaps out of all dataframes     fig, axs = plt.subplots(figsize=(20, 10))     sns.heatmap(lb_df.astype(float), annot=True, fmt=\".2f\",                 ax=axs, annot_kws={\"size\": 18},                 vmin=np.min(lb_df.values),                 vmax=np.percentile(lb_df.values, 25))     axs.set_xlabel(\"MA Terms\", fontsize=20)     axs.set_ylabel(\"AR Terms\", fontsize=20)     axs.tick_params(axis=\"both\", labelsize=20)     fig.tight_layout()     fig.savefig(r\"{}/{}/{}_lb_comp.png\".format(output_path, approach_method,                                                city_name),                 bbox_inches=\"tight\")   In-Sample Check and Forecasting   Finally it is time to look at some predictions from our model. It is important to note that in this forecasting challenge we are not interested in one-step ahead predictions, but a 260-step ahead forecast. In order to validate our performance we therefore need to specify a dynamic prediction method. That means that the model does not treat prior values as realized from a defined point onward, but is aware that these are predictions themselves.The graph below shows predictions of 260 values with the aforementioned dynamic forecasting method. The mean absolute error of 15.7 still represents an in-sample value and can therefore not be taken as representative for out-of-sample performance.      After applying the same steps for the other city in our data, we can then hand in our prediction results. From the picture below we can see that our predictions are far away from being competitive. Though, it has to be said that this performance, which is more or less equally good compared to the benchmark model provided by DrivenData, did not make use of any exogenous variable, meaning that there is still a lot improvement potential.      ","categories": ["Time Series","Python"],
        "tags": [],
        "url": "/time%20series/python/DengAI-Predicting-Disease-Spread-STL-Forecasting-ARIMA-Box-Jenkins/",
        "teaser": null
      },{
        "title": "United for 30 Years — Catching up to West Germany",
        "excerpt":" Visualizing 30 years of economic data between East and West Germany    The year 2020 is certainly a special year for many people. A global pandemic and the election of the next US president are just two out of many examples. For Germany, the year 2020 is historically important for another reason. Thirty years earlier, on October 3rd, 1990, the reunification of Germany took place. With five “new federal states” and the reunited city state of Berlin, the territory of what had been Communist East Germany, the German Democratic Republic became part of the Federal Republic of Germany (West Germany).   It is important to stress that the historical context of the split and reunification of Germany fills several textbooks. This blogpost is trying to provide the most remarkable historical pieces to give an overall understanding of the situation to make the reader be able to put the number into better context.   Image for post      Historical Context   In the aftermath of the second world war, Germany was divided into four occupation zones for each of the four allied forces, namely the Soviet Union, the United States of America, the United Kingdom and France. The capital, Berlin, was equally divided amongst these four.   Between 1947 and 1949, the three zones of the Western allies merged together to form the Federal Republic of Germany, with its capital in Bonn. On the other hand, the Soviet Zone split off to form the German Democratic Republic. West Germany was a capitalist state and member of the NATO, East Germany was Communist and a member of the Warsaw Pact.   Many years later and shortly after the death of Chernenko, who was until his death in 1985 leader of the Soviet Union, Gorbachev was voted unanimously into power and became the eighth leader of the Soviet Union. What followed was a time of drastic political change. A time marked by a freer press, the withdrawal of combat troops from Afghanistan, and the urge to free the world of nuclear weapons by the end of the 20th century. Even a desire for more democracy. [1]   Losing the political backing of the Soviet Union, which was also distracted by its many domestic issues, the German Democratic Republic started to crumble apart up until its end in 1990. The subsequent economic restructuring of the East resulted in significant costs, mainly paid for by West Germany. These payments comprised an average amount of around 100 billion euros per year [2]— an amount which was grossly underestimated in 1990, a time at which the German government still thought that no tax increases were necessary to finance the reunification. [3]   On the 30th of January 1991, the German government announced the so-called Solidarity Surcharges, a tax increase which was levied to cover the huge costs of the reunification and other international involvements of Germany. In 2021 this tax will soon be part of history for around 90 percent of the people. [4]   Thirty years after the reunification, the question might be asked whether the government’s investment reached its goal to equalize the economic situation of East and West. This blog-post uses GDP per Capita data for all 16 German states over time to analysis and visualize the trend in this metric over those 30 years.   Why the East started out so much lower   The separation between the West and East could not have been more fundamental and severe. Both parts of Germany were voluntarily or involuntarily heavily influenced by the United States and the Soviet Union, including their respective economic systems.   The political landscape of the Federal Republic of Germany (West Germany) was divided between proponents of the reunification of East and West and proponents of further integration of the FRG with the other countries of the west. That both the reunification and the west integration could not be realized at the same time resulted in the dependency of the FRG on the United States. At the time, the United States was in the midst of the cold war and had a large interest of containing the Soviet Union’s communism as much as possible (also referred to as Containment politics). If West Germany wanted to end the occupation regime of the western forces, it had to take a standpoint aligned to western interest. Konrad Adenauer, who was Chancellor at that time and in fact the first chancellor Germany ever had, therefore put high emphasis on the better alignment with the west [6]. The adaptations which followed included many cultural aspects but also the form of economy, namely a market economy.   In contrast to the West, East Germany could not count on large subsidies from its carrying superpower, the Soviet Union, quite to the contrary actually Not only did the Soviet Union not invest any amount in the economy of East Germany, it took out large amounts to make up for reparation costs [7]. As all member states of the Warsaw pact, East Germany had a command economy, or Planwirtschaft in German.   After elaborating on the historical context, it is now important to outline the large differences between a market economy and a command economy. A market economy builds on two fundamental aspects: private ownership, and voluntary exchanges and contracts. Which product is produced in which quantity is decided by the company itself, which uses estimated consumer preferences to decide production amounts. The equilibrium prices of goods are based on demand and supply, a process described by Adam Smith as the invisible hand [5]. It is to be said that there is not country which has a pure market economy. This is because oftentimes economic policies still regulate parts of the market to strengthen consumer rights. The economic model of the Federal Republic of Germany, which was also kept after the reunification is referred to as a social market economy. The German Ministry for Economic Affairs describers this type of economy as follows:    “the social market economy is the foundation of our liberal, open and democratic society. The main idea behind the social market economy is to protect the freedom of the economy and functioning competition, and at the same to foster prosperity and social security in our country.” Source: https://www.deutschland.de/en/topic/business/social-market-economy-in-germany-growth-and-prosperity    In a command economy, on the other hand, the government does all the planning of how many units of which product are produced via owning the means of production. Not only does this sound like an insanely difficult task, it also proved to be basically impossible in practice.   A high level of corruption, low level of efficiency and productivity, and the focus on industrial goods (around 90%) in a time when these goods where less in demand and more cheaply produced in Asia countries (Japan) led to the financial collapse of the command economy in the Soviet Union. By how much exactly the economy in the east was lagging the western economy, is visualized in the graph below.      The notable case of the state of Berlin   Even though Berlin is counted as an Eastern state within our analysis, it has to be said that this is not an obvious decision and might well be disputed. Berlin was, equally to the rest of Germany, geographically split by the Allies into a western and an eastern part. Therefore Berlin, even though located deep in the east of Germany, was partly under the authority of the western government.      The sudden alienation between east and west Berlin in 1948 took western forces by surprise, resulting in no electricity in many parts of west Berlin. Soon after west Berlin was cutoff from its geographical surroundings [11].   In order to supply the people in West Berlin with food and other supplies, the renowned Berlin airlift was put in place. Planes arrived every few minutes to West Berlin airports carrying all supplies needed by the population.      The tension between the East and West peaked in 1961, when East Germany began to build a wall which divided the eastern Berlin from western Berlin. This division ruled out the possibility of around 50.000 people from East Berlin to pursue their work in West Berlin, which subsequently greatly hurt the economic power of West Berlin.   During this time of geographic isolation, West Berlin could count on the help of Federal Republic of Germany, which also benefited from the Marshall plan. The goal of the Marshall plan was to improve the economic situation of western Europe and condemn the influence of the Soviet Union in Western Europe.   Given the help of Federal Republic of Germany to West Berlin, it makes Berlin as a whole difficult to compare to other states in the former East which did not have that level of economic support. It is therefore important to remember when looking at the upcoming analysis that Berlin’s performance is not completely representative for the economic performance of East Germany, even though it is counted as an eastern state.      Data and Methodology   The GDP per capita figures are taken from the website https://www.deutschlandinzahlen.de, which is a subsidiary of the German Economic Institute.   Given that all data is stated in nominal terms, which means that there are not inflation adjusted, we also imported German inflation data from Quandl, a world-renowned provider of financial and economic data.   In order to adjust for inflation we first divided the Consumer-Price-Index (CPI) by its level in 2019, before then dividing all nominal GDP per Capita figures by the result of the former calculation.   If we would for example have a CPI level of 70 in 1995 and a level of 120 in 2019, we would first calculate 70/120 = 0.5833. That would imply that the 10 Euros in 2019 have the same purchasing power than 5.83 in 1995. For a more elaborate explanation, we suggest reading this article of ours.   Overall Trend   Before moving into the cross-sectional analysis, where we look at figures for each state individually, we start with an overall analysis of the economic situation across the entire country. From the chart below we can see Box-plots of all German states in every year.   A box-plot is a tool from descriptive statistics for graphically depicting groups of numerical data by their quartiles. They are capable of showing five pieces of information, namely the minimum, maximum, the median (50th percentile), as well as the first quartile (25th percentile) and third quantile (75 percentile). Box-plots represent a great to get a feeling for the underlying distribution of the data.      The chart above contains several insights worth to pointing out. To begin with, we see that the Interquartile range (IQR), which describes the length of each box, decreases drastically in first couple of years. That means that the GDP per Capita for the States are brought more into line and closer to one another. After 1995, it seems though that there is not significant change visible.   Another notable observation is the positive development of the entire boxplot. Across all years, with the sole exception of the direct aftermath of the financial crisis in 2009, it looks like the box-plot as a whole is shifted upwards. This would indicate a systematic increase of GDP per Capita for all states in Germany. In order to test whether that effect also carries statistical significance, a linear regression is conducted. The results of this regression are shown below.      The significant positive coefficient of time (year_num) indicates that on average the GDP per Capita is rising by 318.29 Euros per year. This effect is shown to be significant, with a p-value below of the 5% hurdle. It is to be noted that we are looking at real GDP per Capita, and that these numbers are not simply a result of inflation.   Over Time and State   After seeing that we find an overall positive trend in GDP for all states, it would be now interesting to see how that development looks specifically each state in more detail. The heat-map below (tutorial for this chart can be found here) shows the development of GDP per Capita over the years.   In 1991 the dark blue in East Germany indicates the comparatively bad economic shape the East was in back then. Over the years, the stark color difference fades slowly but surely away, indicating the lower inequality between states.      In order to get a better feeling for magnitude, we also look at the dynamic bar-chart below. In order to better compare the Eastern and Western states, we colored all Western states in turquoise and the Eastern states in red.   The chart displays the economic dominance of the West. Next to Berlin, which cannot completely be referred to as a Eastern State, all Eastern states take the last places for all years.      Inequality Measure Over Time   In order to directly measure inequality, we apply different statistical measures, namely the Lorenz Curve and the Gini Coefficient.   The Lorenz curve is a graphical representation of a distribution and illustrates the magnitude of disparity within it. Oftentimes used to exemplify wealth or income inequality, it is used here to show the deviance to a world where every state would have the same GDP per Capita. Instead of displaying total wealth of a nation on the y-axis, how it is oftentimes done with a Lorenz curve, this presentation shows the cumulative GDP per Capita on the y-axis. Of course, the resulting summation does not carry any interpretable meaning, but it also does not have to have one. The sole purpose is to illustrate the level of inequlity over time.   Directly related to the Lorenz curve is the Gini-Coefficient which can be seen as a quantification of the Lorenz curve. More specifically, it measures the ratio between the diagonal and the Lorenz curve (blue-shaded area) to the entire area below the diagonal. The Gini-Coefficient will be therefore a number between 0 and 1, with higher numbers representing a higher inequality.   The chart below nicely shows how the blue-shaded area, and with it the Gini coefficient slowly decays over time. A clear sign of how the inequality in Germany reduces over time.      Awakening from a bad dream   The question might be asked why the East had and still has such a difficult time catching up with the West. One might think that thirty years of subsidy payments would help and that the East would benefit of the many sharp minds in all Germany. The sober truth looks different unfortunately and shows how difficult it is for an economy to redo a trend that was initiated through the division of Germany.   There are several reasons as to why the economic situations in the east and west were so varied. When the wall came down and the country reunified, the East was not able to control the movement of people anymore and lost a significant amount of their brightest and most productive people, people that are needed to make a nation economically prosperous. To assign a number to that phenomena: between 1991 and 2018, the East (without Berlin) saw a decrease in population of around two million people [10]. In contrast, West Germany plus Berlin gained more than five million.   The other reason, which might be very well the even bigger one, is the process of how companies in the East were privatized. The concept of private ownership had not previously existed in East Germany during the GDR, which was a communist state. Therefore, an agency referred to as Trust Agency (Treuhand in german) was put in charge of privatizing or closing of about 12,000 or more companies state-owned companies [8]. At the end of its work, every second company in the East was now in possession of the West [9]. These companies were responsible for around two thirds of the revenues of the East, which were now missing.   It is obvious that the aforementioned two reasons resemble a classical chicken and egg problem. With all productive workers leaving the East, it proves difficult to rebuild their economic wealth, yet without any well-run companies and high salaries, productive workers will continue to leave the East.   A solution does not come easy, with many politicians trying to appeal to prosperous and investment-willing companies to increase their present in East Germany [10].   Final Note   Looking back on the developments of the last thirty years, the numbers speak a clear language. The relative disparity became significantly lower over time. Looking at the chart below, we can see that in 2019 Eastern Germany had an average GDP per Capita level which represents 75% of the corresponding Western figure. It will probably take a lot longer before the disparity between the East and West completely vanishes. The eastern states were simply unlucky to have fallen into a zone which would turn out to be so damaging economically. It is now up to Germany to show unity and to reunify East and West also economically.      [1] McCauley, Martin (1998). Gorbachev. Profiles in Power. London and New York: Longman. ISBN 978–0582215979. [2] Deutsche Einheit — 100 Milliarden Euro fließen pro Jahr in den Osten. In: Welt Online. 21. August 2009, abgerufen am 31. Oktober 2009. [3] Werner Weidenfeld, Karl-Rudolf Korte (Hrsg.): Handbuch zur deutschen Einheit, 1949–1989–1999. Campus Verlag, 1999, S. 369. [4] https://www.bundesregierung.de/breg-de/aktuelles/solidaritaetszuschlag-1662388 [5]What is Invisible Hand? Definition of Invisible Hand, Invisible Hand …economictimes.indiatimes.com › definition › invisible-hand [6]https://www.geschichte-abitur.de/deutsche-teilung/westdeutschland [7] Berghoff, Hartmut; Balbier, Uta Andrea (2013). The East German economy, 1945–2010: falling behind or catching up?. Cambridge University Press. ISBN 978–1–107–03013–8. [8] https://www.bpb.de/geschichte/deutsche-einheit/zahlen-und-fakten-zur-deutschen-einheit/211280/das-vermoegen-der-ddr-und-die-privatisierung-durch-die-treuhand [9] https://www.sueddeutsche.de/wirtschaft/wiedervereinigung-florierende-ostfirmen-gingen-meist-an-westler-1.5034223 [10] https://www.welt.de/print/welt_kompakt/article195179199/Warum-der-Osten-nicht-mehr-aufholt.html [11] Ladd, Brian (1997). The Ghosts of Berlin: Confronting German History in the Urban Landscape. Chicago: University of Chicago Press. pp. 178–179. ISBN 978–0226467627.  ","categories": ["Data Journalism","R"],
        "tags": [],
        "url": "/data%20journalism/r/United-for-30-Years-Catching-up-to-West-Germany/",
        "teaser": null
      },{
        "title": "Human vs. Machine — Reinforcement Learning in the Context of Snake",
        "excerpt":"This blogpost elaborates on how to implement a reinforcement algorithm, which not only masters the game “Snake”, it even outperforms any human in a game with two players in one playing field.   Reinforcement Learning: Basics of Q Tables   For the Snake to play the game itself, the Snake has to answer at every step the very same question: What to do when faced with conditions such as a certain position of its body, position of the snack and position of the walls which would end the game if touched by the Snake. The information used to describe the situation or state of the Snake is referred to as “state variables”. A visual example is made to illustrate the problem at hand in a different context. Imagine a fictional character who is dressed in one of the following four ways:      The way the character is dressed should be taken as a given and represents the state variables. Furthermore, the fictional character now has four possible destinations to go to next. The destinations look like this:      The question at hand is: at which of these four places would the fictional character experience the most pleasure, given its clothes? Answering this question might seem intuitive. Perhaps this experience of wearing too warm clothing (or even winter clothing) in a warm environment has already been made in the past, and the lesson from the discomfort experienced as a result has been learned. Furthermore, this combination of state variable and action has also already been evaluated. In the situation of winter clothes and the beach, the evaluated outcome was most definitely negative. This evaluation of an action given its state variable that we have experienced and learned from in our own pasts is exactly the point of reinforcement learning. Using past experiences to indicate whether a certain action is a good or bad decision is exactly how Q-Tables work in the context of reinforcement learning. It is crucial to notice that a certain action given the state was only possible because the situation of wearing winter clothes (state variables) and going to the beach (action) were already experienced.      Using logic of how it should theoretically feel to wear warm clothes at the beach without having prior experience is an inference and would require some sort of forecasting model, which is not used in the basic setting of Q-Tables. This implies that when a situation occurs where the fictional character is dressed in a way that it had never been before, meaning it is not known how pleasurable it would be to go to one of the four possible locations dressed in that manner. The image illustrates this last argument, meaning that it has to visit all four different locations at least once to make an evaluation.   Q-Tables applied in the game Snake   The difference between the situation of the fictional character and the Snake is how the state is described and what kind of actions are at hand. The image below shows a normal scenario of a Snake game. The Snake’s body is shown as purple, the boundaries of the playing field are black, and the objective (the snack) is shown as green. The Snake has now four options what to do: it could go left, right, up or down. In the following situation it might seem intuitive for the Snake to go right and simply eat the snack. Why is that intuitive? Because this action was taken in the past and received as a positive experience. What about going left and backing up into itself? This combination of state and action was experienced as something negative.   As visible on the right side, the state at hand is described by two kind of variable categories. The “Danger Variables” give an indication whether the Snake is of risk of dying when going into a certain direction. As discussed before, going left and backing up into itself would kill the Snake. The other variable category is related to the snack the Snake would like to eat. This snack lies in a 90-degree position away from the Snake’s head and is only one field away to be eaten.      A examplatory position of the snake and the calculation of the Q-Value   Foundations of the Bellman Equation   In order to understand the magic of Reinforcement learning, it is necessary to discuss the Bellman Equation which is stated on the right side of the graphic. This equation calculates the Q-Values for every move the Snake can take. In order to understand what a Q-Value is, it is necessary to introduce the concept of rewards and penalties. Within Reinforcement Learning the algorithm receives feedback immediately for every action it takes. In the case the algorithm did something favourable (going to the beach when dressed in summer clothing), then a positive reward of is noted. Following the same logic, a negative reward is given for something unfavourable (going out into the snow with summer clothing) and a neutral reward is given for action which did not lead to a success or failure of the algorithm (going to church in summer clothing). For scaling purposes, normally a standardized reward structure is taken. This means for our Snake example: In case the Snake eats the snack, the algorithm receives a reward of plus one, if it dies it receives a reward of negative one and if it simply makes a move without dying nor eating a snack, it receives a reward of zero. This reward is represented by the “r” within the Bellman equation.   Moreover, it is important for the algorithm to apply some sort of forward thinking. As for many games or tasks, it is not only important to receive the next immediate reward, but also to consider the longer-term goal after that. This consideration of the importance of the future is denoted and incorporated by the “γ”. This gamma coefficient should be carefully chosen. If it is too low, then the agent will not care about the future and only try to receive the reward of the current state as soon as possible, sacrificing a favourable position for the long-term reward to come. If the discount rate is chosen too high, then the current goal is neglected since it carries a relatively low importance compared to the future. The max Operator within the Bellman Equation can be interpreted as the calculated upcoming rewards when a certain action is taken, meaning that if the Snake decides to go left then it knows that it will go e.g. right afterwards to receive the snack. Hence, not only the “left movement” carries importance, but also the upcoming “right movement”.   Q Table Performance   When the Q-Table is completely filled out, it is nothing other than a lookup table for the algorithm to decide what to do when a certain state occurs.      The Q-Table serves as a simple lookup table which is iteratively filled as soon as a certain state is shown to the algorithm   The next question would then be, how does the algorithm fill the table? The answer is simple, it just starts collecting the data itself by moving at random. By moving at random the algorithm collects its own data and iteratively gets better and learns how to react in every observed state. Since the algorithm is constantly learning, it is more and more encouraged to apply actions it learned in the past. Below two graphics are visible. On the left, the untrained algorithm is shown. It is clearly visible that there is no clear pattern in its movements and it strikingly fails to succeed in the game. On the right, on the other hand, a trained algorithm is shown. This difference is that this algorithm has learned to make its own decisions after approximately 50.000 iterations.         On the top an untrained algorithm is shown. It moves via random actions taken. On the bottom an algorithm which was trained with approx. 500k iterations is shown. It performs fairly well already.   Every time the Snake dies, the Snake is less advised to use a random algorithm for decision making purposes, but rather to make the decision given its past acquired knowledge. This approach is called Epsilon Decay, but there are other ways to steer the exploration and exploitation problem faced. A summary of the most well-known approaches is given below:   Epsilon Decay: The algorithm is taking a random action with probability epsilon. This probability then decays with a predefined rate before the next action is taken   Upper Confidence Bound: The action with the highest uncertainty is taken. When an action is taken often enough, and the outcome of that action is less uncertain, another action is taken   Epsilon Greedy: A random movement is taken with a pre-defined probability, in comparison to the Epsilon Decay approach, this probability never changes, meaning that even a trained model still takes random action.   When training the algorithm, the graphic that everybody would like to see is how the average reward develops over time. An increase in this chart would imply that there is a positive correlation between the numbers of games the algorithm saw and how well the algorithm succeeds. The chart below shows exactly the desired pattern: Over time the Snake algorithm increases its average body length. Since creating a longer Snake is the goal of the game in the first place, the algorithms trains!      Moving average (300 units) of the body length of the Snake over 50k training steps   Introducing a second player   Lastly, the game is made to appeal more to the end user. For that purpose, a second, human-controlled player (snake) is inserted in the very same playing field where the algorithm is already playing. Furthermore, the visual appearance of the now two Snakes is enhanced. This enhancement required only some simple editing in MS Paint.   No changes had to be made to the machine-controlled Snake. Whenever it faces the human controlled Snake, it will simply treat it as it treats any danger proximities- it avoids them! Because of the incredibly fast reaction time of the algorithm, no human stands a chance (even after playing 50 times, no one came close to beating it).   Everybody is encouraged to try to beat the machine. The rules are simple: you have one minute to score a higher Snacks-eaten/Deaths ratio than the machine. The entire code is found here on Github.     ","categories": ["Reinforcement Learning","Python"],
        "tags": [],
        "url": "/reinforcement%20learning/python/Human-vs.-Machine-Reinforcement-Learning-in-the-Context-of-Snake/",
        "teaser": null
      },{
        "title": "How the People Really Voted",
        "excerpt":" Why geographically correct maps show elections results inaccurately &lt;/em   Github Repository   The presidential election of 2020 was said by many to be the most important election ever held. The voting turnout was higher than ever before, and the arguably most controversial president in history, Donald Trump, lost to his democratic opponent Joe Biden.   The unprecedented amount of mail-in ballots took its toll on the patience of the nation and the world. On election day, the 3rd of November, it was anything but clear who won the election. The polls before election turned out not to be very representative of true sentiments, a similar flaw as was seen in 2016. Donald Trump’s performance and his backing were grossly underestimated.   On Saturday the 7th of November, the United States of America finally had a new president-elect, assuming of course that the Trump campaign is not able to successfully prove voter fraud in multiple states; a claim which at this point in time seems unfounded and not backed by hard evidence.      Throughout the long wait before the announcement of the president-elect, all major news sources showed a map of the United States in which every states was colored either in blue for the Democrats or red for the Republicans. This blogpost elaborates on why this map is a misleading representation of the relative importance of each state in determining who wins the presidency, and might convey the wrong feeling as to which candidate had the strength of numbers behind him in this election.   The problems of the actual map   The question might arise why in the United States each state is colored in the first place. The reason for that the different states are coloured in the first place is because of the Electoral College, which is a group of presidential electors whose sole purpose it is to elect the president. Every state is assigned a certain number of electors proportional to their population. Therefore, whichever candidate gets the majority within a state, gets all electors from one state (with the exception of Nebraska and Maine, where electors can be split between different candidates).      Below we can see the map of the United States already coloured by the party which won in the 2020 election. Even though the map is geographically correct, the large amount of red might convey the feeling that the Republican party was the predominant force. Considering the system of the electoral college, which was briefly discussed above, tells us that the number of electoral votes and not the size of the state is the decisive factor.   In order to give a better feeling how the people voted, or which state’s decision carried bigger importance, a geographically correct map is not the right tool to use. A significantly better method would be to use a cartogram.   What is a Cartogram   During the long period of waiting for results, all major news-sources used a country map of the United States, with states colored either in blue, red, or grey to indicate whether a state was called for the Democratic Party, the Republican Party or was not called yet, respectively. This map, despite being geographically accurate, does not really reflect the vote of the people.   That is because in the US there are many small states with a relatively large population, like New York or Massachussets. At the same time we also have large states which are home to only a few residents like Wyoming or North Dakota. All of this information remains hidden when looking on the map of the United States.   In order to give a better idea of how the people of the United States really voted in this election, the better choice would be to use a cartogram. A cartogram is a visualization based on maps, where features of the map like land mass size are distorted based on values of a variable.   To better explain the workings of such a map, we use the example from the R Graph gallery. Herein, the countries within the African continent are distorted based on their respective population size. Since Nigeria has the largest population of any country in Africa, it appears much larger than normal, as does Egypt. Other countries like Libya with very small populations appear smaller than their normal size.      Looking at this map gives us a much better idea of the relative population by country. The same concept is now applied to better visualize how the people in each state voted.   A cartogram for the United States election   When applying the methodology of the cartograms outlined above now to the map of the United States we get the following image.      Even the map looks very different from the geographically correct map we are accustomed to seeing, it is still possible to identify the states. Especially remarkable are the changes in size of the states on the East and West Coasts. California, an already larger state, has here more than doubled in size. This result may have been expected for those who know that California is home to about 40 million Americans. Even more drastic are the changes in the East Coast states, where relatively small states like New Jersey and Massachusetts now represent one of the biggest states there are in the US.   The cartogram above results in a graphical representation of the United States as we have not seen it before   Using electoral votes instead of actual population   It is important to note though that because of the electoral college, the cartogram which is based on the actual population by state might not be the optimal visualization to use. That is because electoral college delegates are not only distributed amongst the states entirely based on how many people live in each state — even the states with the smallest populations still get the same minimum number of 3 electors. This means that they receive relatively more electors per person in their state than states with larger populations.   The cartogram below shows, in contrast to the cartograms before, how the cartogram would look like if not the population but the number of electoral votes is used to determine the distortion of each state. We can see that states like California and basically the entire east coast still looks much bigger than the original map, even though the distortion magnitude decreased.      Whether it be by population, or by electoral votes, when looking at the colored map of the United States during election time, it is important to note that a purely geographical representation underestimates the importance of high-population, small-land-mass states.   The electoral system of the United States is drastically different to that of other countries. Therefore, a simple glance at a geographically correct map should always be taken with a grain of salt. To see the truth, one has to dig a bit deeper.  ","categories": ["Data Journalism","Web Scraping","R"],
        "tags": [],
        "url": "/data%20journalism/web%20scraping/r/How-the-People-Really-Voted/",
        "teaser": null
      },{
        "title": "Classifier Evaluation Methods - A Hands-On Explanation",
        "excerpt":" Accuracy/ Recall/ Precision/ Confusion Matrix/ ROC Curve/ AUC    Github Repository   In pretty much 50% of all Data Science interviews around the world, the interviewee is asked to build and assess a binary classification model. This means classifying a certain observation to be either positive (normally denoted as the number 1) or negative (denoted as 0), given a bunch of features. A common mistake that interviewees make is to spend too much time building and tuning an overly-sophisticated model and not enough time elaborating on which classification evaluation metric is appropriate for the problem. That habit is even enforced through Kaggle, or Kaggle-like Data Challenges, in which the classifier evaluation metric is not carefully chosen by the participant, but is already set by the host.   This blogpost will therefore shed more light on the different classification evaluation methods commonly used, how they are derived, when to use them, and, arguably most importantly, how to implement them in Python.   The toy dataset we use for this post is obtained from the DrivenData challenge called: Richter’s Predictor: Modeling Earthquake Damage. WE are already familiar with the dataset for this project, given that we participated in this challenge which can be read up on here. As the name of the project suggests, this challenge involves predicting earthquake damages, specifically damage from the Gorkha earthquake which occurred in April 2015 and killed over 9,000 people. It represents the worst natural disaster to strike Nepal since the 1934 Nepal-Bihar earthquake.   The prediction task for this project is to forecast how badly an individual house is damaged, given the information about its location, secondary usage, and the materials used to build the house in the first place. The damage grade of each house is stated as an integer variable between one and three.   Preliminaries/ Importing the data/ Feature engineering   We begin by importing the relevant packages and setting all the path variables in order to better access the data. Afterwards we import the features and the label which can be downloaded from the DrivenData website. All of that is done by the following lines of code.   # Packages import pandas as pd import numpy as np import pickle import matplotlib.pyplot as plt import seaborn as sns from tqdm import tqdm  from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import (accuracy_score,                              confusion_matrix,                              plot_confusion_matrix,                              recall_score,                              precision_score,                              auc, roc_curve)  # Paths MAIN_PATH = r\"/Users/paulmora/Documents/projects/roc_curve\" RAW_PATH = r\"{}/00 Raw\".format(MAIN_PATH) CODE_PATH = r\"{}/01 Code\".format(MAIN_PATH) DATA_PATH = r\"{}/02 Data\".format(MAIN_PATH) OUTPUT_PATH = r\"{}/03 Output\".format(MAIN_PATH)  # Loading the data total_labels = pd.read_csv(r\"{}/train_labels.csv\".format(RAW_PATH)) total_values = pd.read_csv(r\"{}/train_values.csv\".format(RAW_PATH))   In contrast to an usual data challenge, we do not need to import any test values, since this blogpost is elaborating on the evaluation of classification model and not on the classification model itself. Next up is some initial data cleaning and feature engineering. Given that we are not interested in this post in explaining optimizing the actual model-performance, we will keep the data-processing work to a minimum.   total_values.info()   The above mentioned line of code shows us what kind of work needs to be done, as it is showing us all column names and the information what datatype the information is stored as.      We can see that most variables are already specified in integer-form and therefore do not need any form of altering. The exception of that rule is the column building_id which is an identifier of the particular building and represent non-informative information and can therefore be dropped.   All columns which are represented as a categorical variable (dtype: “object”) will be transformed to dummy variables via one-hot-encoding. This could potentially result in a dangerously sparse dataset, but again, the model performance is not our focus.   It is now time to briefly look at the target variable before merging it with the features and to build our classification models. For that we consider the following line of code as well as the result given beneath it.   total_labels.loc[:, \"damage_grade\"].value_counts()      Given that we would like to focus on binary classification, we see from the output above that some changes in the target variable are necessary since we see that we have more than two outcomes. We therefore drop the damage category 3 from the data. Afterwards we subtract 1 from the target variable in order to create a target variable which is either 0 or 1. This step will also alter the meaning of the problem. Whereas before the target variable was telling us how badly a house was damaged, now the target variable can be regarded as an indication whether a house experienced high damage or whether it did not.   The following lines of code handle the feature engineering and the altering of the target variable. Furthermore, we create a countplot using the seaborn visualization package in order to see the balance of the target variable.   total_df = pd.concat([total_labels, total_values], axis=1) not_3_bool = total_df.loc[:, \"damage_grade\"] != 3 subset_df = total_df.loc[not_3_bool, :] subset_df.loc[:, \"damage_grade\"] = subset_df.loc[:, \"damage_grade\"] - 1 subset_df.rename(columns={\"damage_grade\": \"high_damage\"}, inplace=True) subset_df.drop(columns=[\"building_id\"], inplace=True) subset_dummy_df = pd.get_dummies(subset_df)  plt.rcParams.update({\"font.size\": 20}) fig, axs = plt.subplots(figsize=(10, 10)) sns.countplot(subset_df.loc[:, \"high_damage\"], ax=axs) axs.tick_params(axis=\"both\", which=\"major\") axs.set_xlabel(\"High Grade\") axs.set_ylabel(\"Count\") path = (r\"{}/int.png\".format(OUTPUT_PATH)) fig.savefig(path, bbox_inches='tight')      From the chart above we can see that the majority of cases show a highly damaged house and that the data is therefore imbalanced. An imbalanced dataset describes a situation where we have a significantly inequality in the number of appearances of one or multiple classes with the target variable. For the moment we will leave this imbalance as it is, but remember it for later when talking about the model performance.  Model Training   The three models chosen for our prediction model are a Logistic Regression, Gradient Boosting Classifier and Stochastic Gradient Descent Classifier. There is no particular reason to choose these three models and any other would do as well.   We then split the data into train and test in order to avoid data-leakage which would result in artificial superior performance of the model given that it was trained partly with test data. Afterwards the three models are initiated and a random state is set in order to be able to reproduce our results later. The following lines code cover the aforementioned as well as fit the three models on the data.   train, test = train_test_split(subset_dummy_df, test_size=0.3,                                random_state=28, shuffle=False) train.to_pickle(\"{}/train.pickle\".format(DATA_PATH)) test.to_pickle(\"{}/test.pickle\".format(DATA_PATH))  X_train = train.drop(columns=\"high_damage\") y_train = train.loc[:, \"high_damage\"] X_test = test.drop(columns=\"high_damage\") y_test = test.loc[:, \"high_damage\"]  logreg = LogisticRegression(max_iter=1000, random_state=28) logreg.fit(X_train, y_train)  gbt = GradientBoostingClassifier(random_state=28) gbt.fit(X_train, y_train)  sgdreg = SGDClassifier(fit_intercept=True) sgdreg.fit(X_train, y_train)   Confusion Matrix   So far we cleaned the data, created some basic features and trained three models. It is now time to finally create some predictions and evaluate these. In the case of a binary prediction we can end up with four different possibilities between the true label and our prediction, namely:      The true value is 1 and the prediction is 1 (True Positive)   The true value is 1 and the prediction is 0 (False Negative)   The true value is 0 and the prediction is 1 (False Positive)   The true value is 0 and the prediction is 0 (True Negative)   As it can be already seen from the description in brackets behind each case, there also exist a name for each of these cases. In order to have all of that information in one big picture, we look at something called the confusion matrix, which is implemented through the following code.   # Predictions logreg_pred = logreg.predict(X_test) gbt_pred = gbt.predict(X_test) sgdreg_pred = sgdreg.predict(X_test)  # Confusion matrix raw_conf_logreg = confusion_matrix(y_test, logreg_pred) raw_conf_gbt = confusion_matrix(y_test, gbt_pred) raw_conf_sgd = confusion_matrix(y_test, gbt_pred)  fig, axs = plt.subplots(ncols=3, figsize=(30, 10)) axs = axs.ravel() for i, (title, model) in enumerate(zip([\"Logistic Model\",                                         \"GradientBoosting Model\",                                         \"Stochastic Gradient Descent\"],                                        [logreg, gbt, sgdreg])):     plot_confusion_matrix(model, X_test, y_test, ax=axs[i])     axs[i].set_title(title, fontsize=24)     axs[i].set_ylabel(\"True Label\")     axs[i].set_xlabel(\"Predicted Label\") path = (r\"{}/confusion_matrices.png\".format(OUTPUT_PATH)) fig.savefig(path, bbox_inches='tight')      The confusion matrices above tell us how many of each of the four aforementioned cases we have for each prediction model. In the upper left corner of each graph we see number of cases of True Negatives, i.e. where the true label and the predicted label is zero. The bottom right shows the True Positives, meaning all cases where the True and Predicted value is equal to one. The other two, False Positives (upper right) and False Negatives (bottom left) are misclassification of the model.   When comparing the Gradient Boosting’s and Logistic Model’s confusion matricies, we see that the number of True Positives and the number of True Negatives are higher for the Gradient Boosting Model. This finding makes it clear that the Gradient Boosting model is much better than the Logistic Regression Model.   Figuring out whether the Gradient Boosting or the Stochastic Gradient Descent Model is better, on the other hand, is not so straightforward. That is because the number of True Negatives might be lower for the Stochastic Gradient Descent Model, but it has a higher amount of True Positives when compared to the Gradient Boosting Model.   Through the confusion matrix we could tell that we always would prefer the Gradient Boosting model over the Logistic Model, but we cannot infer which model to use when comparing the Stochastic Gradient Descent Model against the Gradient Boosting Model. For that we need to know more about our underlying problem by looking at further evaluation methods.   The Flaws of using Accuracy   The most common evaluation method used for classification models is arguably the Accuracy score. This score tells us out how often we were right out of all predictions. Accuracy is a very intuitive approach, but it can result in misleading results on imbalanced datasets.      The misleading nature of accuracy as an evaluation method becomes evident when considering an example of a classification model which detects every second whether a house is on fire. We then consider a stupid classifier which always says the house is not on fire. Given that a house is most of the time not in a burning state, the classifier’s prediction will be correct basically all the time.   However, that one night when someone forgot to blow out the candle, the classification model will have False Negative errors — with deadly consequences. Fatal mistakes like this one are not reflected in the Accuracy score, given that in every moment where the house was not burning, it predicted the label correctly.   This great performance of this stupid classifier shows the danger of evaluating a model only by its Accuracy score, when the overall dataset is imbalanced or when we are more concerned with a certain type or errors.   Precision and Recall   Two other popular classification evaluation methods are called Precision and Recall. When to use which method depends on the use case and specifically to which error we are more prone to. To illustrate that, we consider two cases:           Breast Cancer Classification: Within this scenario, it is preferable to predict that a woman has breast cancer even though she does not have any (False Positive). That is because predicting the other way around (False Negative) would leave a woman in the believe she is safe when she is not.            Spam Mail Classification: Here, it would be okay for us if our spam detector classifies a fishy email as non-spam (False Negative), but it would be annoying to find out that an important business email/bill was hiding in the spam folder for the last two weeks because it was incorrectly classified as spam (False Positive).       The two cases above show us that the choice of the evaluation metric depends crucially on what is important to us. In the first case we are much more concerned about False Negatives, meaning that leaving a woman with cancer believing she is well is much worse than telling a woman to undergo further checks even if she does not have to. When the cost of of False Negatives are high, we use Recall, which has the following definition:      In the second example we are more concerned about False Positive errors, namely saying an email is spam even though it was a harmless email from your work. Not seeing this email is much more damaging to the user than seeing one additional spam email. When er are concerned about False Positives, we use Precision, which is defined the following way:      In our example, we are predicting whether a house is highly damaged. We furthermore assume that in the case the classifcation model says that the house is not damaged, the house-owner rents the house out to others. In this case we are much more concerned about False Negative errors. This is because a False Negative mistake would lead the house owner to believe that the house has low damage even though it is highly damaged, endangering the lives of the people living in it. In the case of a False Positive, the house owner would falsely think that the house is severly damaged and order reparations of the house when none are needed — aconsiderably better outcome compared to death. Below we can see the different values of all aforementioned evaluation methods for all three models plus the generating code. From the highest bar within the Recall category, we can see that for our purposes the Stochastic Gradient Boosting model is the model of choice.      True Positive Rate/ False Positive Rate Closely related to Precision and Recall is the concept of True Positive Rate (TPR) and False Positive Rate (FPR), which are defined in the following way:      The observant reader might have the feeling that they have seen the formula for the TPR already. That potential feeling is correct, given that the True Positive Rate is a synonym for the Recall evaluation method we encountered before. Given that we already covered the concept of Recall/ TPR, we only elaborate on the intuition of the False Positive Rate.   Also known as the probability of False Alarm, the False Positive Rate is defined as the probability of rejecting a null-hypothesis which is in fact true. In the area of inferential statistics, this type of error is also referred to as a Type I error (in contrast to a Type II error which describes accepting a null-hypothesis when it is in fact incorrect).   Below we can see a matrix of how TPR and FPR fit in the bigger picture. It is obvious that in a perfect world we would like to have a high TPR but a low FPR. That is because we would like to classify everything that is positive as positive as well as nothing that is negative as positive.   eval_df = pd.DataFrame(columns=[\"Method\", \"Model\", \"Score\"])  i = 0 for model, pred in zip([\"Logistic Model\",                         \"GradientBoosting Model\",                         \"Stochastic Gradient Descent\"],                        [logreg_pred, gbt_pred, sgdreg_pred]):     for meth, score in zip([\"Accuracy\", \"Precision\", \"Recall\"],                            [accuracy_score, precision_score, recall_score]):          eval_df.loc[i, \"Method\"] = meth         eval_df.loc[i, \"Model\"] = model         eval_df.loc[i, \"Score\"] = score(y_test, pred)         i += 1  fig, axs = plt.subplots(figsize=(10, 10)) sns.barplot(x=\"Method\", y=\"Score\", hue=\"Model\",             data=eval_df, ax=axs) axs.legend(loc=\"lower center\") path = (r\"{}/acc_prec_rec.png\".format(OUTPUT_PATH)) fig.savefig(path, bbox_inches='tight')      We also notice that there is some sort of trade-off between these two rates. Consider an example where we have seven true values and three negative values. If our algorithm classifies all ten as true we would get a TPR and FPR of 1. If on the other hand we classify all ten observations as false we would get a value of zero for both rates.   Thinking back to the example of spam-mails and breast cancer, we know that there are cases where we are prone towards False Positive mistakes than towards False Negatives. Combining that desire with the mentioned trade-off between TPR and FPR above leads us to the next topic: specific targeting for certain error types.   Targeting for Error Types   If we are particularly concerned about a certain error type, many classification models allow us to increase the sensitivity towards one or the other error. In order to explain how that works, it is important to note that probabilistic classification algorithms do not assign each observation directly to either 0 or 1, but rather tocalculate a probability with which an observation belongs to either class. The default rule is then if the probability that an observation belongs to class 1 is above 50%, then the observation is assigned to class 1. It is exactly this threshold value of 50%, which we will denote as c, that we will tweak.   If, for example, the probability of an observation belonging to class one is 60%, or 0.6, it is assigned to class one, since this is above the threshold of 0.5. This decision rule, even though simple, is also illustrated in the graphic below.      If we now tweak parameter c to be either higher or lower than 0.5, we would also alter the TPR and FPR.      This is easiest explained and understood when considering an example. Below we find the probability to belong to class one for five observation, as well as their true label. We can see that when the cutoff level is at its default level of 0.5. With that level of c, we find one False Negative and 0 False Positives.      If we now alter the cutoff threshold to 0.2 we can see from the graphic below that we find one False Positive and zero False Negative.      We notice: When changing the threshold value c, it is possible to obtain a different TPR and FPR, which therefore allows us to target for a certain error type.  Concept of ROC Curve and AUC   Now that we’ve seen how we can target and tweak our model towards a certain type of error through adjusting the parameter c, the question might arise how we would know what the optimal value of c is for our problem. This is where the Receiver Operating Characteristc (or short ROC curve) comes into play. The ROC curve plots all combinatins of TPR and FPR at every meaningful threshold level c, as shown in the GIF below.      The off-the-shelf ROC curve implementation from sklearn does not take a random amount of different cutoffs, but loops over the probabilities of every observation for efficiency reasons. This will become clearer later when we implement the code for deriving the ROC curve ourselves.   Related to the concept of the ROC curve is the corresponding value under the curve, called simply area under the curve (or in short: AUC). This metric attempts to summarize the goodness-of-fit of the ROC curve in a single number. As the name implies, this is done by measuring the area under the ROC curve.   Given that the ideal curve hugs the upper lefthand corner as closely as possible — since that would mean that our classifier is able to identify all true positives while avoiding false positives — we know that the ideal model would have an AUC of 1. On the flipside, if your model was no better at predicting than a random guess, your TPR and FPR would increase in parallel to one another, corresponding with an AUC of 0.5.   When applying this concept to our house damage data, we note that not all classification methods provide the option to obtain a probability estimate. The Stochastic Gradient Descent (SGD) classification for example does not allow for probability score when using its default loss function (“hinge”). The reason for that is, that this loss function turns the SGD classifier into a Support Vector Machine, which is a non-probabilistic model.   The following code is therefore showing how to calculate and plot the ROC curve for the Gradient Boosting and Logistic Regression.   pred_proba_reg = logreg.predict_proba(X_test) pred_proba_gbt = gbt.predict_proba(X_test)  fig, axs = plt.subplots(figsize=(10, 10)) for model, pred in zip([\"Logistic Model\",                         \"GradientBoosting Model\"],                        [pred_proba_reg,                         pred_proba_gbt]):     fpr, tpr, _ = roc_curve(y_test, pred[:, 1])     auc_reg = auc(fpr, tpr)     plt.plot(fpr, tpr, label=\"{} AUC:{:.2f}\".format(model, auc_reg)) plt.legend() axs.set_xlabel(\"False Positive Rate\") axs.set_ylabel(\"True Positive Rate\") path = (r\"{}/automatic_auc_roc_curve.png\".format(OUTPUT_PATH)) fig.savefig(path, bbox_inches='tight')      Given that we already know that the GradientBoosting model was superior to the Logistic Regression in terms of the number of True Positives and True Negatives, it was predictable that the GradientBoosting model will also dominate in the ROC curve space. This can be seen by noticing that the red line (GradientBoosting Model) is above the blue line (Logistic Regression) for every single threshold value c.   Manual Implementation of the ROC Curve   Lastly, it might be interesting to see how the ROC curve is implemented when doing it by hand. The code below does not intend to be the most efficient code for implement the ROC curve (the source code from sklearn is the place to look for that), but to be very easy to understand.   As can be seen in line 11, we sort the data-frame, which consists of the true labels and the probability that an observation is equal to one, by their probability value. This is done in order to afterwards loop over these probabilities and use them as the cutoff value. Afterwards, we simply have to count how many True Positive and False Positives we find for all observations which have a higher probability than the cutoff level c and divide them respectively by the total number of positives and negative labels.    def roc_manual(y_true, pred_prob):      # Get the data ready     pred_prob_series = pd.Series(pred_prob[:, 1])     pred_prob_series.name = \"pred\"     df = pd.DataFrame(pd.concat([y_true.reset_index(drop=True),                                  pred_prob_series], axis=1))      # Sorting probabilities in order to loop in an ascending manner     sorted_df = df.sort_values(by=\"pred\")      # Calculate denominators     true_num_pos = sum(y_true == 1)     true_num_neg = sum(y_true == 0)      # Create list container for results     list_tpr, list_fpr = [], []      for prob in tqdm(sorted_df.loc[:, \"pred\"]):          # Create a boolean to mask only the values which qualify to be positive         bool_classified_pos = sorted_df.loc[:, \"pred\"] &gt; prob          # Total number of positives and negative values         tp = sum(sorted_df.loc[bool_classified_pos, \"high_damage\"] == 1)         fp = sum(sorted_df.loc[bool_classified_pos, \"high_damage\"] == 0)          # Calculate the TPR and FPR         tpr = tp / true_num_pos         fpr = fp / true_num_neg          list_tpr.append(tpr)         list_fpr.append(fpr)     return list_tpr, list_fpr   gbt_list_tpr, gbt_list_fpr = roc_manual(y_test, pred_proba_gbt) lgt_list_tpr, lgt_list_fpr = roc_manual(y_test, pred_proba_reg)  # Manual AUC and plotting manual_auc_reg = abs(np.trapz(lgt_list_tpr, lgt_list_fpr)) manual_auc_gbt = abs(np.trapz(gbt_list_tpr, gbt_list_fpr))  fig, axs = plt.subplots(figsize=(10, 10)) plt.plot(lgt_list_fpr, lgt_list_tpr, color=\"orange\",          label=\"Logistic Regression AUC:{:.2f}\".format(manual_auc_reg)) plt.plot(gbt_list_fpr, gbt_list_tpr, color=\"purple\",          label=\"GradientBoosting AUC:{:.2f}\".format(manual_auc_gbt)) plt.legend() axs.set_xlabel(\"False Positive Rate\") axs.set_ylabel(\"True Positive Rate\") path = (r\"{}/manual_auc_roc_curve.png\".format(OUTPUT_PATH)) fig.savefig(path, bbox_inches='tight')      From the chart above we see that our manual implementation went correctly and that it perfectly matches the result of the sklearn-algorithm.   Summary   In this post, we’ve seen how the go-to evaluation metric for classificaiton models, accuracy, can fail to show goodness-of-fit for certain cases, such as cases of imbalanced classes. We then explored additional metrics that give more insight into model perfomance on imbalanced data, such as precision and recall. Finally, we dervied our own ROC curve and showed how it can be used to determine which model is best for the prediction task. These additional metrics extend your toolkit for understanding your models — helping you ultimately choose the right model for the job.   ","categories": ["Classification","Tutorial","Python"],
        "tags": [],
        "url": "/classification/tutorial/python/Classifier-Evaluation-Methods-A-Hands-On-explanation/",
        "teaser": null
      }]
